[{"categories":["健康"],"content":"每日辟谣|多睡一分钟≠给身体多充一分钟电 转载自科普中国2022-09-05 适度赖床，可以让身体有一个从沉睡到清醒的过渡，但时间最多不宜超过一刻钟，而在这一刻钟里，也应尽量让自己慢慢清醒，而不是再度入睡。 现在很多年轻人都有赖床的习惯。他们有一个共同的特点就是，早上定好多个间隔五分钟的闹钟，或者即使提前醒来，也必须赖在床上等闹钟响，不到最后一秒绝不起床，也因为这种早晨在床上苦苦挣扎的过程，这群人被亲切地称为“起床困难户”。“赖床一族”认为，多睡一分钟或多躺一分钟就可以给身体多充一分钟电，那么，事实真的是这样吗？ 图片来源：视觉中国 适度赖床 有益身心健康 赖床一分钟当然是可以的。适度赖床，可以让身体有一个从沉睡到清醒的过渡，因为睡眠期间血液循环较慢，醒来后需要稍停片刻，可以伸个懒腰，轻轻活动一下四肢，让血液循环慢慢恢复到正常水平，防止瞬间起床造成脑供血不足以及血压波动而出现头晕的症状。 “赖床”为清醒 而不是再度入睡 需要注意的是，我们所说的“适度赖床”是时间最多不宜超过一刻钟，而在这一刻钟里，应尽量让自己慢慢清醒，而不是再度入睡。因为赖床时，人体处于浅表睡眠状态，且伴有多梦，属于低质量睡眠，这种状态持续时间越长，越会使人感到疲惫，人体长期处于半梦半醒状态下会造成身体无法顺利清醒，影响工作或学习，甚至造成晚上入睡困难。 许多赖床族都会在醒来之后还要再睡上半小时左右，这是不可取的。因为，人体的睡眠周期每90分钟循环一次。如果早上自然醒来，继续赖床，就会重新进入一个90分钟的睡眠循环，所以赖床30分钟或40分钟醒来后，反而会让人变得无精打采，甚至还会头晕恶心。 图片来源：视觉中国 长时间赖床危害多 1.造成大脑供血不足 我们在睡眠过程中，大脑皮质是处于抑制状态的，如果赖床，就会造成大脑皮质过长时间的抑制，从而导致大脑的供血不足，于是赖床醒来之后，就会感到头昏脑涨，没有精神。 2.破坏人体正常的生物钟规律 人体各个器官的活动都有一定的昼夜规律。保持正常的规律才会使我们在白天精力充沛，晚上睡眠安稳。如果经常赖床，就会打乱人体这种正常的生物钟规律。想利用假期来补充睡眠，可以在假期的前两天多睡一两个小时，然后在假期的后两天慢慢恢复正常的作息。 3.影响泌尿系统健康 赖床会导致尿液在体内长时间滞留，于是尿液当中的有毒物质就会损害我们的身体健康。另外，长时间赖床会减缓人体的血液循环，导致营养在体内不能很好地传送，于是肌肉和关节中代谢产生的物质也不能被排出体外。 4.导致消化不良 赖床的人经常会不吃早餐，于是打乱了人体正常的生理规律，对肠胃功能也产生了不利影响。赖床还会影响人体的排泄功能，也容易造成便秘。 因此，当早上醒来时，还是不要过度赖床，可以将闹钟的铃声设置为舒缓的乐声，音量以逐渐增强为佳，逐步唤醒身体。需要注意的是，切不可将闹钟铃声设置得很大，使处于睡眠中的人突然被惊醒，这样会导致心跳加速，血压瞬间升高，长期下去很可能引发高血压及心脏病。 ","date":"2022-09-05","objectID":"/posts/sleep/:0:0","tags":[],"title":"你的睡眠科学吗？","uri":"/posts/sleep/"},{"categories":[],"content":"前言 HTML5是一款向后兼容的语言，它基本上不需要做版本控制，因为用老版本写的文档在此标准下仍旧可以运行，这与XHTML的严苛形成了强烈的对比。比起遵守XHTML标准而被迫修改自己以前写的大量代码，程序员们自然更青睐HTML5语言。 ","date":"2022-09-04","objectID":"/posts/html5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/:1:0","tags":["HTML5"],"title":"HTML5读书笔记(1)","uri":"/posts/html5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"categories":[],"content":"HTML5的三个主要原理 了解语言背后的设计思想有助于更好地理解语言本身。 ","date":"2022-09-04","objectID":"/posts/html5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/:2:0","tags":["HTML5"],"title":"HTML5读书笔记(1)","uri":"/posts/html5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"categories":[],"content":"不破坏Web 标准不应该引入导致已有的网页无法工作的改变。 标准不应过分地改变规则，即不能让明明可以完美使用的网页因此立马作废。这就是XHTML2犯的错误。 HTML5怎么处理废弃元素 因为HTML5支持所有的HTML，所以也支持许多认为是废弃的功能，比如被人厌恶的和等特效元素。HTML5当然要禁止这些过时的元素，其结果就是它们好多年没有出现在官方规范里了。然而，现代的浏览器仍然悄无声息地支持着这些元素，那该如何解决这个问题呢？HTML5规范分为了两个独立的部分： 面向Web开发人员，要求他们摒弃过去的陋习和废弃的元素 针对浏览器开发商，它们需要支持HTML中的一切，以做到向后兼容 ","date":"2022-09-04","objectID":"/posts/html5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/:2:1","tags":["HTML5"],"title":"HTML5读书笔记(1)","uri":"/posts/html5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"categories":[],"content":"修补牛蹄子路 牛蹄子路(cowpath)指的是高低不平但是使用频率很高的路，借此比喻那种“走起来不舒服，但某种程度上却是最实际的解决之道”。 HTML5把标准化这些非官方**(但广泛应用)**的技术作为一个目标。因为对于一般的网站设计人员来说，切换到新技术可能会超出他们的能力范围。另一方面，使用旧浏览器的客户无法因新技术而受益。 ","date":"2022-09-04","objectID":"/posts/html5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/:2:2","tags":["HTML5"],"title":"HTML5读书笔记(1)","uri":"/posts/html5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"categories":[],"content":"实用至上 改变应该以实用为目的。必要的功能就在HTML标准中加入官方的支持，比如视频播放之类的，而不专注那些华而不实的功能。 ","date":"2022-09-04","objectID":"/posts/html5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/:2:3","tags":["HTML5"],"title":"HTML5读书笔记(1)","uri":"/posts/html5%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"categories":[],"content":"最近养成了写(复制粘贴)博客的习惯，又因为需要运用github远程仓库管理的相关操作，经常要git commit -am \"new\" git push 进行这些重复操作，所以就想到直接自定义一个命令吧！ 首先进入终端 cmd+space搜索termial 进入终端后输入cd ~ + Enter 进入根目录 执行open .zshrc, 这是一个用于自定义、个性化配置的文件。如果没有，请在根目录执行touch .zshrc 以创建该文件 在新文件中配置自定义命令：alias 命令名=\"原本的命令\" 例： source ~/.bash_profile # \u003e\u003e\u003e conda initialize \u003e\u003e\u003e # !! Contents within this block are managed by 'conda init' !! __conda_setup=\"$('/Users/husky/opt/anaconda3/bin/conda' 'shell.zsh' 'hook' 2\u003e /dev/null)\" if [ $? -eq 0 ]; then eval \"$__conda_setup\" else if [ -f \"/Users/husky/opt/anaconda3/etc/profile.d/conda.sh\" ]; then . \"/Users/husky/opt/anaconda3/etc/profile.d/conda.sh\" else export PATH=\"/Users/husky/opt/anaconda3/bin:$PATH\" fi fi unset __conda_setup # \u003c\u003c\u003c conda initialize \u003c\u003c\u003c alias gp=\"git commit -am'new'\u0026\u0026 git push\" # 这里！ 这样我就能快乐地用两个字母来代替这么长串的命令啦～ 注意！ 记得改写完后保存文件cmd + s然后重启终端哦～ 如果出现you have new mail就说明成功了。 雨临Lewis的博客 \"不想当写手的码农不是好咸鱼_(xз」∠)_\" ","date":"2022-09-02","objectID":"/posts/mac%E7%BB%88%E7%AB%AF%E8%87%AA%E5%AE%9A%E4%B9%89%E5%91%BD%E4%BB%A4/:0:0","tags":["Shell","MacOs"],"title":"Mac终端自定义命令","uri":"/posts/mac%E7%BB%88%E7%AB%AF%E8%87%AA%E5%AE%9A%E4%B9%89%E5%91%BD%E4%BB%A4/"},{"categories":["娱乐"],"content":"漫画动漫 mox 台湾香港的漫画网站，大陆下载较慢 提供动漫漫画一条龙服务，但广告较多 动漫网站 ","date":"2022-09-01","objectID":"/posts/creation/:1:0","tags":[],"title":"Recreation","uri":"/posts/creation/"},{"categories":["娱乐"],"content":"影视综艺 看网飞 ","date":"2022-09-01","objectID":"/posts/creation/:2:0","tags":[],"title":"Recreation","uri":"/posts/creation/"},{"categories":["娱乐"],"content":"游戏","date":"2022-09-01","objectID":"/posts/creation/:3:0","tags":[],"title":"Recreation","uri":"/posts/creation/"},{"categories":["数学建模"],"content":"回归(Regression) 指研究一组随机变量 $(Y_1,Y_2,\\dots,Y_i)$和另一组$(X_1，X_2，…，X_k)$变量之间关系的统计分析方法，又称多重回归分析。通常$(Y_1,Y_2,\\dots,Y_i)$是因变量，$(X_1，X_2，…，X_k) $是自变量 。 ","date":"2022-09-01","objectID":"/posts/%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B1/:0:0","tags":[],"title":"预测模型(1)","uri":"/posts/%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B1/"},{"categories":["数学建模"],"content":"线性回归 线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。 在线性回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。 如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。 并不是所有的数据集都可以用一元线性回归来建模。现实世界中的问题往往更复杂，变量几乎不可能非常理想化地符合线性模型的要求。因此使用线性回归，需要遵守下面几个假设： 线性回归是一个回归问题。 要预测的变量 y 与自变量 x 的关系是线性的。 各项误差服从正太分布，均值为0，与 x 同方差。 变量 x 的分布要有变异性, 不能让值都一样 多元线性回归中不同特征之间应该相互独立，避免线性相关。 回归和分类、定性变量和定量变量 与回归相对的是分类问题（classification），分类问题要预测的变量y输出集合是有限的，预测值只能是有限集合内的一个。当要预测的变量y输出集合是无限且连续，我们称之为回归。比如，天气预报预测明天是否下雨，是一个二分类问题；预测明天的降雨量多少，就是一个回归问题。 定性变量是统计学的概念，又名分类变量 ，观测的个体只能归属于几种互不相容类别中的一种时，一般是用非数字来表达其类别，这样的观测数据称为定性变量。 定量变量 也就是通常所说的连续量，如长度、重量、产量、人口、速度和温度等，它们是由测量或计数、统计所得到的量，这些变量具有数值特征，称为定量变量。 ","date":"2022-09-01","objectID":"/posts/%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B1/:1:0","tags":[],"title":"预测模型(1)","uri":"/posts/%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B1/"},{"categories":["数学建模"],"content":"一元线性回归 一元线性回归的基本公式就是早早学过的直线表达式 $$ y=mx+b \\tag{1} $$ 我们的任务就是选取 $m,b$ 两个参数使直线和给定的数据对 $(x,y)$ 拟合的最好 线性回归示意图 那么评价衡量这个模型的拟合的好坏程度呢？ 用损失函数(Loss Function)/ 代价函数(Cost Function)。其值越大，表示该模型拟合效果越差。一般用如下记号表示损失函数： $$ L(\\hat y,y) $$ 对于线性回归，常用的一个损失函数为： $$ L(\\hat y,y)=\\frac{1}{N} \\sum_{1}^{N}(\\hat y_{i}-y_{i})^2 \\tag{2} $$ 结合公式(1)，进一步得到： $$ m^*,b^*=arg\\underset{m,b}min \\frac{1}{N} \\sum_{1}^{N}(\\hat y_{i}-y_{i})^2 $$ argmin(expression) 的意思是寻找使得表达式值最小的参数 其实，这就是最小二乘法的数学表示，“二乘\"表示平方，“最小\"表示损失最小。 那么求解时，只要对m和b求偏导数： 然后让两个偏导数分别等于0，即可得到所求参数的值。 ","date":"2022-09-01","objectID":"/posts/%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B1/:1:1","tags":[],"title":"预测模型(1)","uri":"/posts/%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B1/"},{"categories":["数学建模"],"content":"多元线性回归 现实中，我们想要得到的某个预测结果往往由多个因素决定。假如共有D个维度的影响因素，机器学习中也将这D个影响因素称为特征(feature)。 与一元情况类似，每个样本都有一个需要预测的y值，但是变量却是一个D维的向量X。原来的参数m也变为了一个D维的向量w。 如此一来，样本i的预测值 $y_i$则可以表示为： $$ y_i=b+\\sum_{d=1}^{D}w_dx_{i,d} $$ 为了方便起见，可以将b归纳到向量w中。让 $ b = W_{D+1}$然后将其添加到特征向量w 的最后一位，并将D维特征拓展为D+1维。 $$ \\mathbf{w}=\\left[\\begin{array}{c}w_1 \\newline w_2 \\newline \\vdots \\newline w_D \\newline w_{D+1}\\end{array}\\right] $$ $$ \\mathbf{X}=\\left[\\begin{array}{c}x_1 \\newline x_2 \\newline \\vdots \\newline x_N\\end{array}\\right]=\\left(\\begin{array}{ccccc}x_{1,1} \u0026 x_{1,2} \u0026 \\cdots \u0026 x_{1, D} \u0026 1 \\newline x_{2,1} \u0026 x_{2,2} \u0026 \\cdots \u0026 x_{2, D} \u0026 1 \\newline \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \u0026 \\vdots \\newline x_{N, 1} \u0026 x_{N, 2} \u0026 \\cdots \u0026 x_{N, D} \u0026 1\\end{array}\\right) $$ $$ \\mathbf{y}=\\left[\\begin{array}{c}y_1 \\newline y_2 \\newline \\vdots \\newline y_N\\end{array}\\right] $$ 由此可以进一步推出： $$ \\mathbf{y}=\\mathbf{X}\\mathbf{w} $$ 多元线性回归的损失函数如下 $$ L(w)=(\\mathbf{X} \\mathbf{w}-\\mathbf{y})^{\\top}(\\mathbf{X} \\mathbf{w}-\\mathbf{y})=|\\mathbf{X} \\mathbf{w}-\\mathbf{y}|_2^2 \\tag{a} $$ 公式最右边的记号叫做 $L_2$ 范数的平方 L2范数是指向量中各个元素的平方和再开平方。 范数 L1范数是指向量中各个元素绝对值之和。 等式a对w求导得到： $$ \\frac{\\partial}{\\partial \\mathbf{w}} L(\\mathbf{w})=2 \\mathbf{X}^{\\top}(\\mathbf{X} \\mathbf{w}-\\mathbf{y})=0 $$ 进一步可得： $$ \\mathbf{X}^{\\top} \\mathbf{X} \\mathbf{w}=\\mathbf{X}^{\\top} \\mathbf{y} \\Rightarrow\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{X} \\mathbf{w}=\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y} $$ $$ \\mathbf{w}^*=\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y} \\tag{normal equation} $$ 由此我们知道，多元线性回归的最优解其实就是在解这个称为Normal Equation的矩阵方程。 ","date":"2022-09-01","objectID":"/posts/%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B1/:1:2","tags":[],"title":"预测模型(1)","uri":"/posts/%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B1/"},{"categories":["数学建模"],"content":"多重共线性 定义 在多元线性回归模型经典假设中，其重要假定之一是回归模型的解释变量之间不存在线性关系，也就是说，解释变量 $X1，X2，……，Xk$ 中的任何一个都不能是其他解释变量的线性组合。 如果线性回归模型中某一个解释变量与其他解释变量间存在线性关系，就称线性回归模型中存在多重共线性。 如果 $$ c_1 X_{1 i}+c_2 X_{2 i}+\\ldots+c_k X_{k i}=0 \\newline i=1,2, \\ldots, n $$ 其中 $c_i$ 不全为0，则某个解释变量可以由其他解释变量的线性组合表示，则称为解释变量间存在完全共线性。 如果存在 $$ \\begin{gathered} c_1 X_{1 i}+c_2 X_{2 i}+\\ldots+c_k X_{k i}+v_i=0 \\ \\mathrm{i}=1,2, \\ldots, \\mathrm{n} \\end{gathered} $$ 其中 $c_i$ 不全为 $0, v_i$ 为随机误差项, 则称为一般共线性(近似共线性)或者交互相关(intercorrelated) 严重的多重共线性可能会产生问题，因为它可以增大回归系数的方差，使它们变得不稳定。以下是不稳定系数导致的一些后果： 即使预测变量和响应之间存在显著关系，系数也可能看起来并不显著。 高度相关的预测变量的系数在样本之间差异很大。 从模型中去除任何高度相关的项都将大幅影响其他高度相关项的估计系数。高度相关项的系数甚至会包含错误的符号。 判别 有多种方法可以检测多重共线性，较常使用的是回归分析中的VIF值，VIF值越大，多重共线性越严重。一般认为VIF大于10时（严格是5），代表模型存在严重的共线性问题。 有时候也会以容差值作为标准，$容差值=1/VIF$，所以容差值大于0.1则说明没有共线性(严格是大于0.2)。 VIF和容差值有逻辑对应关系，两个指标任选其一即可。 方差膨胀系数 方差膨胀系数(variance inflation factor，VIF)是衡量多元线性回归模型 中复 (多重)共线性严重程度的一种度量。 它表示回归系数估计量的方差与假设自变量间不线性相关时方差相比的比值。 VIF检验的具体步骤如下： 设原方程为：$Y=\\beta_1+\\beta_2 X_2+\\beta_3 X_3+\\ldots+\\beta_k X_k+\\mathbf{u}$ 我们需要计算k个VIF，每个 $X_i$ 一个VIF，具体步骤又分以下三步： (1) Xi 对原方程中其它全部解释变量进行OLS(ordinary least squares, 普通最小二乘回归法)回归, 例如, 若 $\\mathbf{i}=1$, 则回归下面的方程: $$ X_1=\\alpha_1+\\alpha_2 X_2+\\alpha_3 X_3+\\ldots+\\alpha_k X_k+v $$ (2) 计算的方差膨胀因子(VIF): 其中 $\\mathbf{R}_{\\mathrm{i}}{ }^2$ 是第一步辅助回归的决定系数。 (3) 分析多重共线性的程度, 一般认为VIF大于10时（严格是5），代表模型存在严重的共线性问题。 ","date":"2022-09-01","objectID":"/posts/%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B1/:1:3","tags":[],"title":"预测模型(1)","uri":"/posts/%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B1/"},{"categories":["自动化办公"],"content":"xlwings比起xlrd、xlwt和xlutils，xlwings可豪华多了，它具备以下特点： xlwings能够非常方便的读写Excel文件中的数据，并且能够进行单元格格式的修改 可以和matplotlib以及pandas无缝连接，支持读写numpy、pandas数据类型，将matplotlib可视化图表导入到excel中。 可以调用Excel文件中VBA写好的程序，也可以让VBA调用用Python写的程序。 开源免费，一直在更新 官网地址：https://www.xlwings.org/ 官方文档：https://docs.xlwings.org/en/stable/api.html ","date":"2022-08-31","objectID":"/posts/xlwings/:0:0","tags":[],"title":"Xlwings","uri":"/posts/xlwings/"},{"categories":["自动化办公"],"content":"4.1 pip安装xlwings pip install xlwings ","date":"2022-08-31","objectID":"/posts/xlwings/:0:1","tags":[],"title":"Xlwings","uri":"/posts/xlwings/"},{"categories":["自动化办公"],"content":"4.2 基本操作 引入库 import xlwings as xw 打开Excel程序，默认设置：程序可见，只打开不新建工作薄 app = xw.App(visible=True,add_book=False) #新建工作簿 (如果不接下一条代码的话，Excel只会一闪而过，卖个萌就走了） wb = app.books.add() 打开已有工作簿（支持绝对路径和相对路径） wb = app.books.open('example.xlsx') #练习的时候建议直接用下面这条 #wb = xw.Book('example.xlsx') #这样的话就不会频繁打开新的Excel 保存工作簿 wb.save('example.xlsx') 退出工作簿（可省略） wb.close() 退出Excel app.quit() 三个例子：（1）打开已存在的Excel文档 # 导入xlwings模块 import xlwings as xw # 打开Excel程序，默认设置：程序可见，只打开不新建工作薄，屏幕更新关闭 app=xw.App(visible=True,add_book=False) app.display_alerts=False app.screen_updating=False # 文件位置：filepath，打开test文档，然后保存，关闭，结束程序 filepath=r'g:\\Python Scripts\\test.xlsx' wb=app.books.open(filepath) wb.save() wb.close() app.quit() （2）新建Excel文档，命名为test.xlsx，并保存在D盘 import xlwings as xw app=xw.App(visible=True,add_book=False) wb=app.books.add() wb.save(r'd:\\test.xlsx') wb.close() app.quit() （3）在单元格输入值新建test.xlsx，在sheet1的第一个单元格输入 “人生” ，然后保存关闭，退出Excel程序。 import xlwings as xw app=xw.App(visible=True,add_book=False) wb=app.books.add() # wb就是新建的工作簿(workbook)，下面则对wb的sheet1的A1单元格赋值 wb.sheets['sheet1'].range('A1').value='人生' wb.save(r'd:\\test.xlsx') wb.close() app.quit() 打开已保存的test.xlsx，在sheet2的第二个单元格输入“苦短”，然后保存关闭，退出Excel程序 import xlwings as xw app=xw.App(visible=True,add_book=False) wb=app.books.open(r'd:\\test.xlsx') # wb就是新建的工作簿(workbook)，下面则对wb的sheet1的A1单元格赋值 wb.sheets['sheet1'].range('A1').value='苦短' wb.save() wb.close() app.quit() 掌握以上代码，已经完全可以把Excel当作一个txt文本进行数据储存了，也可以读取Excel文件的数据，进行计算后，并将结果保存在Excel中。 ","date":"2022-08-31","objectID":"/posts/xlwings/:0:2","tags":[],"title":"Xlwings","uri":"/posts/xlwings/"},{"categories":["自动化办公"],"content":"4.3 引用工作薄、工作表和单元格 （1）按名字引用工作簿，注意工作簿应该首先被打开 wb=xw.books['工作簿的名字‘] （2）引用活动的工作薄 wb=xw.books.active （3）引用工作簿中的sheet sht=xw.books['工作簿的名字‘].sheets['sheet的名字'] # 或者 wb=xw.books['工作簿的名字'] sht=wb.sheets[sheet的名字] （4）引用活动sheet sht=xw.sheets.active （5）引用A1单元格 rng=xw.books['工作簿的名字‘].sheets['sheet的名字'] # 或者 sht=xw.books['工作簿的名字‘].sheets['sheet的名字'] rng=sht.range('A1') （6）引用活动sheet上的单元格 # 注意Range首字母大写 rng=xw.Range('A1') #其中需要注意的是单元格的完全引用路径是： # 第一个Excel程序的第一个工作薄的第一张sheet的第一个单元格 xw.apps[0].books[0].sheets[0].range('A1') 迅速引用单元格的方式是 sht=xw.books['名字'].sheets['名字'] # A1单元格 rng=sht[’A1'] # A1:B5单元格 rng=sht['A1:B5'] # 在第i+1行，第j+1列的单元格 # B1单元格 rng=sht[0,1] # A1:J10 rng=sht[:10,:10] #PS： 对于单元格也可以用表示行列的tuple进行引用 # A1单元格的引用 xw.Range(1,1) #A1:C3单元格的引用 xw.Range((1,1),(3,3)) 引用单元格： rng = sht.range('a1') #rng = sht['a1'] #rng = sht[0,0] 第一行的第一列即a1,相当于pandas的切片 引用区域： rng = sht.range('a1:a5') #rng = sht['a1:a5'] #rng = sht[:5,0] ","date":"2022-08-31","objectID":"/posts/xlwings/:0:3","tags":[],"title":"Xlwings","uri":"/posts/xlwings/"},{"categories":["自动化办公"],"content":"4.4 写入\u0026读取数据 1.写入数据（1）选择起始单元格A1,写入字符串‘Hello’ sht.range('a1').value = 'Hello' （2）写入列表 # 行存储：将列表[1,2,3]储存在A1：C1中 sht.range('A1').value=[1,2,3] # 列存储：将列表[1,2,3]储存在A1:A3中 sht.range('A1').options(transpose=True).value=[1,2,3] # 将2x2表格，即二维数组，储存在A1:B2中，如第一行1，2，第二行3，4 sht.range('A1').options(expand='table')=[[1,2],[3,4]] 默认按行插入：A1:D1分别写入1,2,3,4 sht.range('a1').value = [1,2,3,4] 等同于 sht.range('a1:d1').value = [1,2,3,4] 按列插入：A2:A5分别写入5,6,7,8 你可能会想： sht.range('a2:a5').value = [5,6,7,8] 但是你会发现xlwings还是会按行处理的，上面一行等同于： sht.range('a2').value = [5,6,7,8] 正确语法: sht.range('a2').options(transpose=True).value = [5,6,7,8] 既然默认的是按行写入，我们就把它倒过来嘛（transpose），单词要打对，如果你打错单词，它不会报错，而会按默认的行来写入（别问我怎么知道的） 多行输入就要用二维列表了： sht.range('a6').expand('table').value = [['a','b','c'],['d','e','f'],['g','h','i']] 2.读取数据（1）读取单个值 # 将A1的值，读取到a变量中 a=sht.range('A1').value （2）将值读取到列表中 #将A1到A2的值，读取到a列表中 a=sht.range('A1:A2').value # 将第一行和第二行的数据按二维数组的方式读取 a=sht.range('A1:B2').value 选取一列的数据 先计算单元格的行数(前提是连续的单元格) rng = sht.range('a1').expand('table') nrows = rng.rows.count 接着就可以按准确范围读取了 a = sht.range(f'a1:a{nrows}').value 选取一行的数据 ncols = rng.columns.count #用切片 fst_col = sht[0,:ncols].value ","date":"2022-08-31","objectID":"/posts/xlwings/:0:4","tags":[],"title":"Xlwings","uri":"/posts/xlwings/"},{"categories":["自动化办公"],"content":"4.5 常用函数和方法 1.Book工作薄常用的api wb=xw.books[‘工作簿名称'] wb.activate() 激活为当前工作簿 wb.fullname 返回工作簿的绝对路径 wb.name 返回工作簿的名称 wb.save(path=None) 保存工作簿，默认路径为工作簿原路径，若未保存则为脚本所在的路径 wb. close() 关闭工作簿 代码示例： # 引用Excel程序中，当前的工作簿 wb=xw.books.acitve # 返回工作簿的绝对路径 x=wb.fullname # 返回工作簿的名称 x=wb.name # 保存工作簿，默认路径为工作簿原路径，若未保存则为脚本所在的路径 x=wb.save(path=None) # 关闭工作簿 x=wb.close() 2.sheet常用的api # 引用某指定sheet sht=xw.books['工作簿名称'].sheets['sheet的名称'] # 激活sheet为活动工作表 sht.activate() # 清除sheet的内容和格式 sht.clear() # 清除sheet的内容 sht.contents() # 获取sheet的名称 sht.name # 删除sheet sht.delete 3.range常用的api # 引用当前活动工作表的单元格 rng=xw.Range('A1') # 加入超链接 # rng.add_hyperlink(r'www.baidu.com','百度',‘提示：点击即链接到百度') # 取得当前range的地址 rng.address rng.get_address() # 清除range的内容 rng.clear_contents() # 清除格式和内容 rng.clear() # 取得range的背景色,以元组形式返回RGB值 rng.color # 设置range的颜色 rng.color=(255,255,255) # 清除range的背景色 rng.color=None # 获得range的第一列列标 rng.column # 返回range中单元格的数据 rng.count # 返回current_region rng.current_region # 返回ctrl + 方向 rng.end('down') # 获取公式或者输入公式 rng.formula='=SUM(B1:B5)' # 数组公式 rng.formula_array # 获得单元格的绝对地址 rng.get_address(row_absolute=True, column_absolute=True,include_sheetname=False, external=False) # 获得列宽 rng.column_width # 返回range的总宽度 rng.width # 获得range的超链接 rng.hyperlink # 获得range中右下角最后一个单元格 rng.last_cell # range平移 rng.offset(row_offset=0,column_offset=0) #range进行resize改变range的大小 rng.resize(row_size=None,column_size=None) # range的第一行行标 rng.row # 行的高度，所有行一样高返回行高，不一样返回None rng.row_height # 返回range的总高度 rng.height # 返回range的行数和列数 rng.shape # 返回range所在的sheet rng.sheet #返回range的所有行 rng.rows # range的第一行 rng.rows[0] # range的总行数 rng.rows.count # 返回range的所有列 rng.columns # 返回range的第一列 rng.columns[0] # 返回range的列数 rng.columns.count # 所有range的大小自适应 rng.autofit() # 所有列宽度自适应 rng.columns.autofit() # 所有行宽度自适应 rng.rows.autofit() 4.books 工作簿集合的api # 新建工作簿 xw.books.add() # 引用当前活动工作簿 xw.books.active 4.sheets 工作表的集合 # 新建工作表 xw.sheets.add(name=None,before=None,after=None) # 引用当前活动sheet xw.sheets.active ","date":"2022-08-31","objectID":"/posts/xlwings/:0:5","tags":[],"title":"Xlwings","uri":"/posts/xlwings/"},{"categories":["自动化办公"],"content":"4.6 数据结构 1.一维数据python的列表，可以和Excel中的行列进行数据交换，python中的一维列表，在Excel中默认为一行数据。 import xlwings as xw sht=xw.sheets.active # 将1，2，3分别写入了A1，B1，C1单元格中 sht.range('A1').value=[1,2,3] # 将A1，B1，C1单元格的值存入list1列表中 list1=sht.range('A1:C1').value # 将1，2，3分别写入了A1，A2，A3单元格中 sht.range('A1').options(transpose=True).value=[1,2,3] # 将A1，A2，A3单元格中值存入list1列表中 list1=sht.range('A1:A3').value 2.二维数据python的二维列表，可以转换为Excel中的行列。二维列表，即列表中的元素还是列表。在Excel中，二维列表中的列表元素，代表Excel表格中的一列。例如： # 将a1,a2,a3输入第一列，b1,b2,b3输入第二列 list1=[[‘a1’,'a2','a3'],['b1','b2','b3']] sht.range('A1').value=list1 # 将A1：B3的值赋给二维列表list1 list1=sht.range('A1:B3').value 3.Excel中区域的选取表格 # 选取第一列 rng=sht. range('A1').expand('down') rng.value=['a1','a2','a3'] # 选取第一行 rng=sht.range('A1').expand('right') rng=['a1','b1'] # 选取表格 rng.sht.range('A1').expand('table') rng.value=[[‘a1’,'a2','a3'],['b1','b2','b3']] ","date":"2022-08-31","objectID":"/posts/xlwings/:0:6","tags":[],"title":"Xlwings","uri":"/posts/xlwings/"},{"categories":["自动化办公"],"content":"4.7 xlwings生成图表 生成图表的方法，具体方法也可参见：Python 操作 Excel 库 xlwings 常用操作详解！ import xlwings as xw app = xw.App() wb = app.books.active sht = wb.sheets.active chart = sht.charts.add(100, 10) # 100, 10 为图表放置的位置坐标。以像素为单位。 chart.set_source_data(sht.range('A1').expand()) # 参数为表格中的数据区域。 # chart.chart_type = i # 用来设置图表类型，具体参数件下面详细说明。 chart.api[1].ChartTitle.Text = i # 用来设置图表的标题。 示例代码： import xlwings as xw app = xw.App() wb = app.books.active sht = wb.sheets.active # 生成图表的数据 sht.range('A1').value = [['时间', '数量'], ['1日', 2], ['2日', 1], ['3日', 3] , ['4日', 4], ['5日', 5], ['6日', 6]] \"\"\"图表类型参数，被注释的那几个，无法生成对应的图表\"\"\" dic = { '3d_area': -4098, '3d_area_stacked': 78, '3d_area_stacked_100': 79, '3d_bar_clustered': 60, '3d_bar_stacked': 61, '3d_bar_stacked_100': 62, '3d_column': -4100, '3d_column_clustered': 54, '3d_column_stacked': 55, '3d_column_stacked_100': 56, '3d_line': -4101, '3d_pie': -4102, '3d_pie_exploded': 70, 'area': 1, 'area_stacked': 76, 'area_stacked_100': 77, 'bar_clustered': 57, 'bar_of_pie': 71, 'bar_stacked': 58, 'bar_stacked_100': 59, 'bubble': 15, 'bubble_3d_effect': 87, 'column_clustered': 51, 'column_stacked': 52, 'column_stacked_100': 53, 'cone_bar_clustered': 102, 'cone_bar_stacked': 103, 'cone_bar_stacked_100': 104, 'cone_col': 105, 'cone_col_clustered': 99, 'cone_col_stacked': 100, 'cone_col_stacked_100': 101, 'cylinder_bar_clustered': 95, 'cylinder_bar_stacked': 96, 'cylinder_bar_stacked_100': 97, 'cylinder_col': 98, 'cylinder_col_clustered': 92, 'cylinder_col_stacked': 93, 'cylinder_col_stacked_100': 94, 'doughnut': -4120, 'doughnut_exploded': 80, 'line': 4, 'line_markers': 65, 'line_markers_stacked': 66, 'line_markers_stacked_100': 67, 'line_stacked': 63, 'line_stacked_100': 64, 'pie': 5, 'pie_exploded': 69, 'pie_of_pie': 68, 'pyramid_bar_clustered': 109, 'pyramid_bar_stacked': 110, 'pyramid_bar_stacked_100': 111, 'pyramid_col': 112, 'pyramid_col_clustered': 106, 'pyramid_col_stacked': 107, 'pyramid_col_stacked_100': 108, 'radar': -4151, 'radar_filled': 82, 'radar_markers': 81, # 'stock_hlc': 88, # 'stock_ohlc': 89, # 'stock_vhlc': 90, # 'stock_vohlc': 91, # 'surface': 83, # 'surface_top_view': 85, # 'surface_top_view_wireframe': 86, # 'surface_wireframe': 84, 'xy_scatter': -4169, 'xy_scatter_lines': 74, 'xy_scatter_lines_no_markers': 75, 'xy_scatter_smooth': 72, 'xy_scatter_smooth_no_markers': 73 } w = 385 h = 241 n = 0 x = 100 y = 10 for i in dic.keys(): xx = x + n % 3*w # 用来生成图表放置的x坐标。 yy = y + n//3*h # 用来生成图表放置的y坐标。 chart = sht.charts.add(xx, yy) chart.set_source_data(sht.range('A1').expand()) chart.chart_type = i chart.api[1].ChartTitle.Text = i n += 1 wb.save('chart_图表') wb.close() app.quit() 效果如下： 4.8 实战训练 1. xlwings 新建 Excel 文档 程序示例： # 3.4.2 xlwings 新建 Excle 文档 def fun3_4_2(): \"\"\" visible Ture：可见excel False：不可见excel add_book True:打开excel并且新建工作簿 False：不新建工作簿 \"\"\" app = xw.App(visible=True, add_book=False) # 新建工作簿 (如果不接下一条代码的话，Excel只会一闪而过，卖个萌就走了） wb = app.books.add() # 保存工作簿 wb.save('example.xlsx') # 退出工作簿 wb.close() # 退出Excel app.quit() 执行程序后文件夹增加了“example.xlsx”: 此时表格是空的： 2. xlwings 打开已存在的 Excel 文档现有表格长这样： 运行程序： # 3.4.3 xlwings 打开已存在的Excel文件 def fun3_4_3(): # 新建Excle 默认设置：程序可见，只打开不新建工作薄，屏幕更新关闭 app = xw.App(visible=True, add_book=False) app.display_alerts = False app.screen_updating = False # 打开已存在的Excel文件 wb=app.books.open('./3_4 xlwings 修改操作练习.xlsx') # 保存工作簿 wb.save('example_2.xlsx') # 退出工作簿 wb.close() # 退出Excel app.quit() 生成新的表格： 内容如下： 3. xlwings 读写 Excel程序示例： # 3.4.4 xlwings读写 Excel def fun3_4_4(): # 新建Excle 默认设置：程序可见，只打开不新建工作薄，屏幕更新关闭 app = xw.App(visible=True, add_book=False) app.display_alerts = False app.screen_updating = False # 打开已存在的Excel文件 wb=app.books.open('./3_4 xlwings 修改操作练习.xlsx') # 获取sheet对象 print(wb.sheets) sheet = wb.sheets[0] # sheet = wb.sheets[\"sheet1\"] # 读取Excel信息 cellB1_value = sheet.range('B1').value print(\"单元格B1内容为：\",cellB1_value) # 清除单元格内容和格式 sheet.range('A1').clear() # 写入单元格 sheet.range('A1').value = \"xlwings写入\" # 保存工作簿 wb.save('example_3.xlsx') # 退出工作簿 wb.close() # 退出Excel app.quit() 执行效果： ","date":"2022-08-31","objectID":"/posts/xlwings/:0:7","tags":[],"title":"Xlwings","uri":"/posts/xlwings/"},{"categories":[],"content":"准备 电锯人漫画进入页面网址：https://www.mangabz.com/577bz/ 母链：https://www.mangabz.com ","date":"2022-08-30","objectID":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/:0:1","tags":["爬虫","Python"],"title":"爬取电锯人漫画","uri":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/"},{"categories":[],"content":"第一步：拿到所有章节的入口链接 xpath：/html/body/div[5]/div[2]/a[1] ","date":"2022-08-30","objectID":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/:0:2","tags":["爬虫","Python"],"title":"爬取电锯人漫画","uri":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/"},{"categories":[],"content":"第二步：从章节进入后爬取该章节的图片源地址 漫画的图片地址： https://image.mangabz.com/1/577/43329/1_8283.jpg?cid=43329\u0026key=2b0ac9cf0be35f46d3d743634ff07ab7\u0026uk= https://image.mangabz.com/1/577/43329/2_7777.jpg?cid=43329\u0026key=2b0ac9cf0be35f46d3d743634ff07ab7\u0026uk= 相较而言没有什么规律。且如果直接由requests.get()方法拉取漫画网的响应(response)的话，html文件中并没有我们想要的漫画图片源地址。 原来，该网站经过了js的渲染, 页面上的图片源地址需要执行隐藏在html中的javascript脚本才能获取！ 防爬机制有很多，这只是其中一种！点这里看更多防爬机制的介绍 有两种解决办法： 用Selenium库利用Ajax 接口模拟浏览器爬取(我还不会) 用execjs库，结合html文件中提供的js参数，运行js脚本得到网址 这里选择用第二种方法 ","date":"2022-08-30","objectID":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/:0:3","tags":["爬虫","Python"],"title":"爬取电锯人漫画","uri":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/"},{"categories":[],"content":"第三步：根据图片源地址下载图片 这个无需多言，主要是要设置好下载的路径以及注意with open(filename, mode = \"wb\")图片是二进制！ ","date":"2022-08-30","objectID":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/:0:4","tags":["爬虫","Python"],"title":"爬取电锯人漫画","uri":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/"},{"categories":[],"content":"问题 由于网站上资源的错误，有几章存在缺失，好在我把所有的链接内容全部下载下来。卷中残缺的章节可以根据页数从全部下载内容中找到对应图片并补全 爬虫过程中会出现工作暂停、卡死不动的情况，我的解决办法是根据目标文件夹的创建情况定位正在下载的章节，然后从这个章节重新开始。可能有更好的实现实时刷新而不是重启程序的方法？ ","date":"2022-08-30","objectID":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/:0:5","tags":["爬虫","Python"],"title":"爬取电锯人漫画","uri":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/"},{"categories":[],"content":"源代码 from time import sleep from lxml import etree import pandas as pd import os # 创建目录 import tqdm # 进度条显示 import requests import re import string import random import execjs #解析javascript代码，破解图片源地址 import urllib.parse # 参数设定 user_agent = \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7\" start_url = 'https://apps.apple.com/cn/genre/ios-%E6%97%85%E8%A1%8C/id6003' # 起初界面 # 防爬，所以增加了refer！ headers = { \"referer\": \"https://www.mangabz.com/577bz/\", \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\", \"sec-ch-ua-mobile\": \"?0\", \"sec-ch-ua-platform\": \"macOS\", \"sec-fetch-dest\": \"document\", \"sec-fetch-mode\": \"navigate\", \"sec-fetch-site\": \"same-origin\", \"upgrade-insecure-requests\": \"1\" } base_url = \"https://www.mangabz.com/577bz\" refer_url = \"https://www.mangabz.com\" parent_dir = \"/Users/husky/Desktop/电锯人\" # 用于储存 response = requests.get(base_url,headers = headers) html_str = response.content tree = etree.HTML(html_str) #获取章节的入口码 chapters_code = tree.xpath(\"/html/body/div[5]/div[2]//a/@href\") chapters_url = [refer_url + code for code in list(chapters_code)][::-1] # 翻转顺序，从第一章开始 # print(chapters_url) # 获取某一章节的图片源地址 imgs = [] class Mangabz: def __init__(self, url): self.url = url self.session = requests.Session() self.headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\", \"Referer\": self.url, \"Cookie\": \"image_time_cookie=17115|637270678077155170|2\", } # 通过正则表达式获取java相关参数 def get_chapter_argv(self): res = requests.get(self.url, headers=self.headers, timeout=10) mangabz_cid = re.findall(\"MANGABZ_CID=(.*?);\", res.text)[0] mangabz_mid = re.findall(\"MANGABZ_MID=(.*?);\", res.text)[0] page_total = re.findall(\"MANGABZ_IMAGE_COUNT=(.*?);\", res.text)[0] #这里可以找到page的总页数 mangabz_viewsign_dt = re.findall(\"MANGABZ_VIEWSIGN_DT=\\\"(.*?)\\\";\", res.text)[0] mangabz_viewsign = re.findall(\"MANGABZ_VIEWSIGN=\\\"(.*?)\\\";\", res.text)[0] return (mangabz_cid, mangabz_mid, mangabz_viewsign_dt, mangabz_viewsign, page_total) def get_images_js(self, page, mangabz_cid, mangabz_mid, mangabz_viewsign_dt, mangabz_viewsign): url = self.url + \"chapterimage.ashx?\" + \"cid=%s\u0026page=%s\u0026key=\u0026_cid=%s\u0026_mid=%s\u0026_dt=%s\u0026_sign=%s\" % (mangabz_cid, page, mangabz_cid, mangabz_mid, urllib.parse.quote(mangabz_viewsign_dt), mangabz_viewsign) res = self.session.get(url, headers=self.headers, timeout=10) self.headers[\"Referer\"] = res.url # 自动获取refer return res.text def run(self): mangabz_cid, mangabz_mid, mangabz_viewsign_dt, mangabz_viewsign, page_total = self.get_chapter_argv() for i in tqdm.tqdm(range(int(page_total)),desc=\"正在拉取该章节下的所有图片链接\",position = 2,colour='green',leave=\"False\"): i += 1 js_str = self.get_images_js(i, mangabz_cid, mangabz_mid, mangabz_viewsign_dt, mangabz_viewsign) imagesList = execjs.eval(js_str) imgs.append(imagesList[0]) # print(imagesList[0]) # if __name__ == '__main__': # mangabz = Mangabz(\"https://www.mangabz.com/m43328/\") # mangabz.run() # 定义图片下载函数 def download_img(img_url,chapter_num,page_num,download_path =''): response = requests.get(img_url, headers = mangabz.headers) data = response.content if(not download_path): with open(parent_dir + \"/Chapter{0}/page{1}.jpg\".format(chapter_num,page_num), mode = \"wb+\") as f: f.write(data) else: with open( download_path + \"/page{}.jpg\".format(page_num), mode = \"wb+\") as f: f.write(data) for i in tqdm.tqdm(range(111,112),desc= \"进入Chapter{}\".format(i + 1),position= 0,colour='blue'): # 每一章节 # 创建以章节为单位的文件夹 dir = \"Chapter{}\".format(i + 1) # 下标需要 + 1才是章节数 path = os.path.join(parent_dir,dir) if(not os.path.exists(path)): os.makedirs(path) # 获取该章节的所有漫画链接 mangabz = Mangabz(chapters_url[i]) mangabz.run() # 下载 for n in tqdm.tqdm(range(len(imgs)),desc=\"正在下载该章节内的所有漫画\", position = 2, colour= 'red',leave=\"False\"): download_img(imgs[n],i + 1,n + 1) # 清空imgs列表，用于下次下载 imgs = [] # 任务完成 print(\"d","date":"2022-08-30","objectID":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/:0:6","tags":["爬虫","Python"],"title":"爬取电锯人漫画","uri":"/posts/%E7%88%AC%E5%8F%96%E7%94%B5%E9%94%AF%E4%BA%BA%E6%BC%AB%E7%94%BB/"},{"categories":["自学笔记"],"content":"XPath 常用规则 表达式 描述 nodename 1 // 从当前节点选取子孙节点 / 从当前节点选取子节点 . 选取当前结点 … 选取当前节点的父节点 @ 选取属性 ","date":"2022-08-29","objectID":"/posts/requests-xpath/:0:1","tags":["爬虫"],"title":"Requests 和 Xpath","uri":"/posts/requests-xpath/"},{"categories":["自学笔记"],"content":"获取文本 # 第一种 # 获取文本所在节点后直接获取文本 from lxml import etree html = etree.parse('./test.html', etree.HTMLParser()) result = html.xpath('//li[@class=\"item-0\"]/a/text()') print(result) # 第二种 # 会获取到补全代码时换行产生的特殊字符，推荐使用第一种方法，可以保证获取的结果是整洁的。 from lxml import etree html = etree.parse('./test.html', etree.HTMLParser()) result = html.xpath('//li[@class=\"item-0\"]//text()') print(result) ","date":"2022-08-29","objectID":"/posts/requests-xpath/:0:2","tags":["爬虫"],"title":"Requests 和 Xpath","uri":"/posts/requests-xpath/"},{"categories":["自学笔记"],"content":"参考链接：https://www.runoob.com/xpath/xpath-syntax.html","date":"2022-08-29","objectID":"/posts/requests-xpath/:0:3","tags":["爬虫"],"title":"Requests 和 Xpath","uri":"/posts/requests-xpath/"},{"categories":["自学笔记"],"content":"任务描述 我们可以在如下页面： https://apps.apple.com/cn/genre/ios-%E6%97%85%E8%A1%8C/id6003 (链接到外部网站。)链接到外部网站。 看到Apple iOS 应用商店（AppStore）中的所有应用（app）的相关页面，例如对于“春秋航空”其页面是： https://apps.apple.com/cn/app/%E6%98%A5%E7%A7%8B%E8%88%AA%E7%A9%BA-%E4%BD%8E%E4%BB%B7%E6%9C%BA%E7%A5%A8%E9%A2%84%E8%AE%A2/id460033035 (链接到外部网站。)链接到外部网站。 我们可以从每个app对应的页面上获取到很多信息，例如对于“春秋航空”，我们可以得到下面一些信息： app对应的ID：460033035 app名称：春秋航空-低价机票预订 app类别：旅游 app开发者：春秋航空 Spring Airlines app供应商：Spring Airlines Co.,Ltd app评分：4.5 • 6,880 个评分 app最新版本： 7.1.5 发布日期：2022年7月1日 app价格：免费 我们写一个爬虫： 1 获取 https://apps.apple.com/cn/ (链接到外部网站。)链接到外部网站。 下面所有app的信息页面 2 分析每个页面，为每个app提取上述类别的信息 ","date":"2022-08-29","objectID":"/posts/%E4%BB%8Eappstore%E4%B8%AD%E8%8E%B7%E5%8F%96app%E4%BF%A1%E6%81%AF/:0:1","tags":["爬虫","Python"],"title":"从Appstore上爬取app信息","uri":"/posts/%E4%BB%8Eappstore%E4%B8%AD%E8%8E%B7%E5%8F%96app%E4%BF%A1%E6%81%AF/"},{"categories":["自学笔记"],"content":"代码如下 from time import sleep from lxml import etree import pandas as pd import os import tqdm # 进度条显示 import requests import re import string start_url = 'https://apps.apple.com/cn/genre/ios-%E6%97%85%E8%A1%8C/id6003' # 起初界面 header = {'user-agent':\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"} # 防爬 response = requests.get(start_url,headers = header) html_str = response.content tree = etree.HTML(html_str) app_urls = tree.xpath('//*[@id=\"selectedcontent\"]/div[1]/ul//a/@href') # 获取母页面 dict = {} # 用于储存数据 dict[\"app_id\"] = [] dict[\"app_name\"] = [] dict[\"app_category\"] = [] dict[\"app_developer\"] = [] dict[\"app_supplier\"] = [] dict[\"app_score\"] = [] dict[\"app_version\"] = [] dict[\"app_release_date\"] = [] dict[\"price\"] = [] # 获取信息 def get_info(app_url): response = requests.get(app_url,headers = header) html_str = response.content # response.encoding = \"utf-8\" tree = etree.HTML(html_str) # 记录 app_id app_id = tree.xpath('/html/head/meta[10]/@content') dict[\"app_id\"].append(app_id[0]) # 记录 app_name app_name = tree.xpath('//h1[@class=\"product-header__title app-header__title\"]/text()') list(app_name) app_name[0] = str(app_name[0]) app_name[0] = app_name[0].strip() app_name[0] = app_name[0].strip(\"\\u202c\") app_name[0] = app_name[0][:-2] + app_name[0][-1] # 去除 \\u202a 和 \\u202c 这两个奇怪的字符 dict[\"app_name\"].append(app_name[0]) # 记录 app_category app_category = tree.xpath('//dl[@class=\"information-list information-list--app medium- columns l-row\"]/div[3]/dd/a/text()') dict[\"app_category\"].append(app_category[0].strip()) # 记录 app_developer app_developer = tree.xpath('//h2/a[@dir]/text()') dict[\"app_developer\"].append(app_developer[0].strip()) # 记录 app_supplier app_supplier = tree.xpath('//dd[@class = \"information-list__item__definition\"]/text()') dict[\"app_supplier\"].append(app_supplier[0].strip()) #记录 app_score app_score = tree.xpath('//figure[@class = \"we-star-rating\"]/figcaption/text()') #处理可能没有评分的情况 if(app_score): dict[\"app_score\"].append(app_score[0]) else: dict[\"app_score\"].append(\"Nan\") # 记录 app_version app_version = tree.xpath('//p[@class =\"l-column small-6 medium-12 whats- new__latest__version\"]/text()') dict[\"app_version\"].append(app_version[0][2:]) # 记录 app_release_date app_release_date = tree.xpath('//time[@aria-label]/text()') dict[\"app_release_date\"].append(app_release_date[0].strip()) # 记录 price # 不同的网页price的Full Xpath有不同，所以要自己写 price = tree.xpath('//div/dt[text()=\"价格\"]/following-sibling::dd/text()') dict[\"price\"].append(price[0]) # loop for url in tqdm.tqdm(app_urls): #tqdm 提供进度条 get_info(url) data = pd.DataFrame(dict) # 用于最后导出数据 data.to_csv(\"App Information.csv\") # 工作完成 print(\"done\") ","date":"2022-08-29","objectID":"/posts/%E4%BB%8Eappstore%E4%B8%AD%E8%8E%B7%E5%8F%96app%E4%BF%A1%E6%81%AF/:0:2","tags":["爬虫","Python"],"title":"从Appstore上爬取app信息","uri":"/posts/%E4%BB%8Eappstore%E4%B8%AD%E8%8E%B7%E5%8F%96app%E4%BF%A1%E6%81%AF/"},{"categories":["自学笔记"],"content":"疑问记录 如果直接从chrome上复制元素的Xpath爬取的数据总为空，必须要减少路径长度才行，原因未知。 对于etree的element对象，从网上爬取到中文字符有可能出现奇怪的编码，比如\\u202a和\\u202c，最后我消除的办法是先将对象转换为str类型然后用字符串处理消除。 ","date":"2022-08-29","objectID":"/posts/%E4%BB%8Eappstore%E4%B8%AD%E8%8E%B7%E5%8F%96app%E4%BF%A1%E6%81%AF/:0:3","tags":["爬虫","Python"],"title":"从Appstore上爬取app信息","uri":"/posts/%E4%BB%8Eappstore%E4%B8%AD%E8%8E%B7%E5%8F%96app%E4%BF%A1%E6%81%AF/"},{"categories":["自学笔记"],"content":"tqdm介绍 python处理进度条可视化的工具 ","date":"2022-08-29","objectID":"/posts/tqdm/:0:1","tags":["爬虫","Tools"],"title":"Tqdm","uri":"/posts/tqdm/"},{"categories":["自学笔记"],"content":"基本语法 # 方法1 for i in tqdm.tqdm(可迭代对象): pass # 方法2 for idx, i in enumerate(tqdm.tqdm(可迭代对象)): pass ","date":"2022-08-29","objectID":"/posts/tqdm/:0:2","tags":["爬虫","Tools"],"title":"Tqdm","uri":"/posts/tqdm/"},{"categories":["自学笔记"],"content":"参数 iterable: 可迭代的对象, 在⼿动更新时不需要进⾏设置 desc: 字符串, 左边进度条描述⽂字 total: 总的项⽬数 leave: bool值, 迭代完成后是否保留进度条 file: 输出指向位置, 默认是终端, ⼀般不需要设置 ncols: 调整进度条宽度, 默认是根据环境⾃动调节长度, 如果设置为0, 就没有进度条, 只有输出的信息 unit: 描述处理项⽬的⽂字, 默认是it, 例如: 100 it/s, 处理照⽚的话设置为img ,则为 100 img/s unit_scale: ⾃动根据国际标准进⾏项⽬处理速度单位的换算, 例如 100000 it/s » 100k it/s colour: 进度条颜色 import tqdm import time d = {'loss':0.2,'learn':0.8} \"\"\" desc设置名称 ncols设置进度条长度 -\u003e 建议设置在100以内 postfix以字典形式传入详细信息 \"\"\" for i in tqdm.tqdm(range(50),desc='名称',ncols=100,postfix=d): time.sleep(0.1) pass 资料来源：https://blog.csdn.net/weixin_44878336/article/details/124894210 官网: https://pypi.org/project/tqdm/ ","date":"2022-08-29","objectID":"/posts/tqdm/:0:3","tags":["爬虫","Tools"],"title":"Tqdm","uri":"/posts/tqdm/"},{"categories":["自学笔记"],"content":"文件默认权限：umask umask 是指目前用户在建立文件或目录时候的权限默认值 umask -S(Symbolic) 0022 u=rwx,g=rx,o=rx 若用户建立为文件，则默认没有可执行（x）权限，即只有rw，也就是最大666 -rw-rw-rw- 若用户建立为目录，则默认为777 drwxrwxrwx 注意，umask指的是该默认值需要减掉的权限 修改umask： umask 002 ","date":"2022-08-23","objectID":"/posts/%E6%96%87%E4%BB%B6%E4%B8%8E%E7%9B%AE%E5%BD%95%E7%9A%84%E9%BB%98%E8%AE%A4%E6%9D%83%E9%99%90%E5%92%8C%E9%9A%90%E8%97%8F%E6%9D%83%E9%99%90/:0:1","tags":["Linux","Shell"],"title":"Linux文件与目录的默认权限与隐藏权限","uri":"/posts/%E6%96%87%E4%BB%B6%E4%B8%8E%E7%9B%AE%E5%BD%95%E7%9A%84%E9%BB%98%E8%AE%A4%E6%9D%83%E9%99%90%E5%92%8C%E9%9A%90%E8%97%8F%E6%9D%83%E9%99%90/"},{"categories":["自学笔记"],"content":"文件隐藏属性 chattr 配置文件隐藏属性 chattr [] 文件或目录名 -a 设置a后，这个文件只能增加数据，而不能删除也不能修改数据，只有root可以设置此属性 -i 让一个文件不能被改名、删除、设置链接也无法写入新增数据。只有root可以设置此属性 lsattr显示文件隐藏属性 lsattr [-adR] 文件或目录 -a 将隐藏文件的属性显示出来 -d 如果接的是目录，只列出目录本身的属性而不是目录内的文件名 -R 连同子目录的数据也一并列出来 ","date":"2022-08-23","objectID":"/posts/%E6%96%87%E4%BB%B6%E4%B8%8E%E7%9B%AE%E5%BD%95%E7%9A%84%E9%BB%98%E8%AE%A4%E6%9D%83%E9%99%90%E5%92%8C%E9%9A%90%E8%97%8F%E6%9D%83%E9%99%90/:0:2","tags":["Linux","Shell"],"title":"Linux文件与目录的默认权限与隐藏权限","uri":"/posts/%E6%96%87%E4%BB%B6%E4%B8%8E%E7%9B%AE%E5%BD%95%E7%9A%84%E9%BB%98%E8%AE%A4%E6%9D%83%E9%99%90%E5%92%8C%E9%9A%90%E8%97%8F%E6%9D%83%E9%99%90/"},{"categories":["自学笔记"],"content":"用户与用户组 Linux中，任何一个文件都具有用户、所属群组、其他人三种身份的个别权限，以及root权限 一个主机，可以有很多用户组 用户组中有很多用户 不同用户组之间有权限阻挡 一个用户可以拥有几个用户组的查阅权限 总结：用户组划定共享空间，用户划定私人空间 三个重要文件 /etc/passwd 记录系统账号、一般用户和root的相关信息 /etc/shadow 记录个人密码 /etc/group 记录Linux中所有组名 ","date":"2022-08-23","objectID":"/posts/linux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/:0:1","tags":["Linux","Shell"],"title":"Linux文件权限","uri":"/posts/linux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/"},{"categories":["自学笔记"],"content":"Linux文件权限的概念 文件属性 ls -al 显示出所有文件的权限和属性，包括以 . 开头的隐藏文件 文件类型权限 链接数 文件拥有者 文件所属用户组 文件大小 (Bytes) 文件最后被修改时间 文件名 -rw-r–r– 1 root root 1864 May 4 18:01 initial-setup-ks.cfg 文件类型权限 共10个字符 第一个字符表示这个文件是目录、文件或是链接文件 d 目录 - 文件 | 链接文件 b 设备文件里面的可供存储的周边设备 c 设备文件里面的串行端口设备，如键盘、鼠标 接下来的字符以三个为一组，且均为[rwx]三个参数的组合 r 可读 w 可写 x 可执行 第一组为文件拥有者可具备的权限 第二组为加入用户组之账号的权限 第三组为没有加入本用户组的其他账号的权限 另外，目录与文件的权限意义并不相同, 对于目录而言, 仅有r而没有x意味着不能进入该目录。 日期 如果修改时间距今太久，则只会显示年份 ls -l --full-time 可以显示完整的时间格式 如何修改文件属性与权限 chgrp 修改文件所属用户组 chgrp [-R] dirname/filename -R 进行递归修改，即连同子目录下的所有文件全部更新 要被修改的组名必须在 /etc/group中！ chown 修改文件拥有者 chown [-R] 账号名称 文件或目录 chown [-R] 账号名称:用户组名称 文件或目录 chown 可以仅仅修改用户组， 如 chown .group file chmod 修改文件权限,两种修改方式 数字： chmod [-R] xyz 文件或目录 xyz为数字类型的权限属性，为以下数值相加 r: 4 w: 2 x: 1 每种身份数字需要累加, 如： owner = rwx = 4 + 2 + 1 = 7 group = rwx = 4 + 2 + 1 = 7 others= --- = 0 + 0 + 0 = 0 ​ chmod 777 打开所有权限 ​ chmod 755 变为可执行权限且不让其他人修改此文件 符号类型 方法 身份 操作 权限 文件或目录名 chmod u、g、o、a +（加入）、-（移除）、=（设置） r、w、x filename or directoryname ​ chmod u=rwx,go=rx .bashrc 中间u=rwx,go=rx 是连接在一起的，中间并没有任何空格 ","date":"2022-08-23","objectID":"/posts/linux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/:0:2","tags":["Linux","Shell"],"title":"Linux文件权限","uri":"/posts/linux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/"},{"categories":["自学笔记"],"content":"目录文件的权限意义 权限对文件的重要性 文件一般包括文本文件、数据库文件、二进制可执行文件 r 可读取此文件的实际内容，如读取文字内容 w 可以编辑、新增或是修改该文件的内容 (但不包括删除) x 该文件具有可以被系统执行的权限 权限对目录的重要性 目录的主要内容在于记录文件名列表，文件名与目录有强烈的关联 r 具有读取目录结构列表的权限 w 具有改动目录结构列表的权限 建立新的文件与目录 删除已经存在的文件与目录 (不论该文件的权限是什么) 将已存在的文件或目录重命名 移动该目录里的文件和目录位置 x 可以进入该目录使之成为工作目录 Linux文件种类与扩展名 任何设备在linux下都是文件，连数据沟通的接口都有专门的文件负责 文件种类 常规文件- 纯文本文件（ASCII） 人类可以直接读到的数据 二进制文件（binary），如Linux中的可执行文件（scripts，脚本文件不算） 数据文件（data） 目录d 链接文件l 设备与设备文件 区块设备文件：硬盘软盘 b 字符设备文件：键盘鼠标 c 数据接口文件（sockets）s 数据输送文件（FIFO，pipe）p Linux文件扩展名 ​ 首先，Linux中一个文件可否执行和扩展名无关，只与权限中的x有关。 ​ 但是，可否执行成功和可否执行又是两个概念。 常用拓展名： *.sh ：脚本或批处理文件（scripts） *.Z、*.tar、*.tar.gz、*.zip、*.tgz ： 经过打包压缩的文件 *.html、*.php ：网页相关文件 Linux文件名长度限制 单一文件或目录名长度最大为255字节，约128个汉字 Linux文件名限制 避免此类符号 * ` ？ \u003e \u003c ; \u0026 ! [] | \\ ' \" () {} · 开头的文件是隐藏文件 此外，尽量避免用+ -作为文件名的开头 ​ ","date":"2022-08-23","objectID":"/posts/linux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/:0:3","tags":["Linux","Shell"],"title":"Linux文件权限","uri":"/posts/linux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/"},{"categories":["自学笔记"],"content":"相对路径和绝对路径 相对路径 不是由根目录\\写起，移植性好 绝对路径 一定由根目录\\写起，正确性好 ","date":"2022-08-23","objectID":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/:0:1","tags":["Linux","Shell"],"title":"Linux文件目录常见操作","uri":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/"},{"categories":["自学笔记"],"content":"目录的相关操作 . 代表此层目录 .. 代表上一层目录 - 代表前一个工作目录 ~ 代表目前使用者所在的家目录 ~account 代表account这个使用者的家目录（account为账号名称） cd cd/cd~ 回到家目录 pwd pwd 显示目前所在目录 pwd -P 不会显示链接文件 的路径，而是显示正确的完整路径 mkdir mkdir [-mp] -m 设置文件的权限，不适用默认权限 -p递归创建 rmdir rmdir [-p] 目录名称 -p 连同上层的空目录一起删除 只能删除空目录！否则必须使用rm -r 目录名称 ","date":"2022-08-23","objectID":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/:0:2","tags":["Linux","Shell"],"title":"Linux文件目录常见操作","uri":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/"},{"categories":["自学笔记"],"content":"关于执行文件路径的变量$PATH 执行echo $PATH 来看有哪些目录被定义 PATH=\"${PATH}:/root\" 将/root文件目录添加到PATH中，从而使该目录下的文件可见 不同用户默认的PATH不同 PATH是可以被修改的 使用绝对路径或相对路径直接指定某个命令的文件名来执行，会比查找PATH来的正确和安全 本目录(.)最好不要放到PATH中，以防出现安全问题 ","date":"2022-08-23","objectID":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/:0:3","tags":["Linux","Shell"],"title":"Linux文件目录常见操作","uri":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/"},{"categories":["自学笔记"],"content":"文件与目录的查看ls ls -a 全部文件，包括隐藏文件 ls -d 仅列出目录 ls -l 详细信息显示 ","date":"2022-08-23","objectID":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/:0:4","tags":["Linux","Shell"],"title":"Linux文件目录常见操作","uri":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/"},{"categories":["自学笔记"],"content":"复制删除与移动 cp cp [-adfilprsu] 源文件（source） 目标文件（desitnation） cp [options] source1 source2 source3 ... directory -a : 等价于-dr --preserve=all -d : 若源文件为链接文件的属性，则复制链接文件属性而非文件本身；不加参数的情况下，cp复制的是原始文件 -i : 若目标文件已经存在时，在覆盖时会先询问操作的进行，按下 n 或 y 以二次确认 -l : 进行硬链接的链接文件建立，而非复制文件本身 -p : 连同文件的属性（权限、用户、时间）一起复制过去，而非使用默认属性（备份常用） -r : 递归复制，用于目录的复制操作,不加这个参数目录是无法复制的 -s : 复制成为符号连接文件（symbolic link），也就是“快捷方式”文件 -u : destination比source旧才更新destination，或destination不存在的情况下才复制 --preserve=all 除了-p的权限相关参数外，还加入了SElinux的属性，links、xattr等也复制 如果文件有两个以上，则destination一定要是目录才行 如果权限不够，不能随意修改文件的拥有者和用户组的情况下，即使用ls -a 也只能复制相关权限与时间的属性，而无法复制拥有者和用户组 rm rm [-fir] 文件或目录 -f force强制，忽略不存在的文件，不会出现警告信息 -i 交互模式，在删除前会询问使用者是否操作 -r 递归删除，最常用于目录的删除，这是非常危险的选项 rm -i bashrc* 移除以bashrc开头的文件名，*是通配符，表示0到无穷多个字符 mv mv [-fiu] source destination -f force强制，如果目标文件已经存在，不会询问而是直接覆盖 -i 若目标文件已经存在，就会询问是否覆盖 -u 若目标文件已存在，且source比较新，才会更新（update） 如果文件有两个以上，则destination一定要是目录才行 范例：复制一文件，建立一个目录，将文件移动到目录中 cd /tmp cp ~/.bashrc bashrc mkdir mvtest mv mvtest mvtest2 将dir mvtest更名为mvtest2 ","date":"2022-08-23","objectID":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/:0:5","tags":["Linux","Shell"],"title":"Linux文件目录常见操作","uri":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/"},{"categories":["自学笔记"],"content":"获取路径的文件名与目录名称 basename /etc/sysconfig/network network 取得最后的文件名 dirname /etc/sysconfig/network /etc/sysconfig 取得所在目录名 ","date":"2022-08-23","objectID":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/:0:6","tags":["Linux","Shell"],"title":"Linux文件目录常见操作","uri":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/"},{"categories":["自学笔记"],"content":"文件内容查看 cat 由第一行开始显示内容 tac 从最后一行开始显示 nl 显示的时候输出行号 more 可以一页一页地显示文件内容 less 除了可以一页一页查看，还可以倒着翻页 od 查看非文本文件 ","date":"2022-08-23","objectID":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/:0:7","tags":["Linux","Shell"],"title":"Linux文件目录常见操作","uri":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/"},{"categories":["自学笔记"],"content":"修改文件时间或创建新文件 touch 修改时间（modification time，mtime）内容数据变更时，就会更新 状态时间（status time，ctime）权限与属性的改变会更新这个时间 读取时间（access time，atime）文件内容被读取时会更新这个时间 默认情况下ls列出的是该文件的mtime touch [-acdmt] 文件 -a 仅自定义access time -c 仅修改文件的时间，若文件不存在则不建立新文件 -d 后面可以接欲自定义的日期而不用目前的日期，也可以使用--date=“日期或时间” -m 仅修改mtime -t 后面可以接欲自定义的日期而不用目前的日期，格式为[YYYYMMDDhhmm] ","date":"2022-08-23","objectID":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/:0:8","tags":["Linux","Shell"],"title":"Linux文件目录常见操作","uri":"/posts/linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C/"},{"categories":["健康"],"content":"饮食 ","date":"2022-08-23","objectID":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/:0:0","tags":["Exercise"],"title":"健身笔记","uri":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/"},{"categories":["健康"],"content":"黄金时间： ​ 运动后30分钟内摄入蛋白质最佳 热身 ","date":"2022-08-23","objectID":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/:1:0","tags":["Exercise"],"title":"健身笔记","uri":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/"},{"categories":["健康"],"content":"静态拉伸 ​ 静态拉伸就是把自己的身体摆放到关节活动度极限位置附近，然后保持这个动作持续10-30秒左右，感受到肌纤维被拉伸的感觉即可。例如侧压腿这个动作就属于静态伸展。其实我们生活中常说的“拉筋”、“压腿”等动作都属于静态拉伸。 研究表明，运动前做静态拉伸是有害的，应该在运动后进行，目的是拉伸和放松肌肉 ","date":"2022-08-23","objectID":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/:2:0","tags":["Exercise"],"title":"健身笔记","uri":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/"},{"categories":["健康"],"content":"动态拉伸： ​ 动态拉伸指的是用自身的身体用力去做一些伸展的动作，也就是大家通常说的“热身运动”。例如做一些简单的开合跳、高抬腿跑、踢臀跑等动作，这些都是常见的正是锻炼前的拉伸动作。 肩绕环 碰脚尖 侧弯拉伸腹部 前后甩小腿 手臂向后伸，拉伸胸肌 大腿向外绕环 腹部 ","date":"2022-08-23","objectID":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/:3:0","tags":["Exercise"],"title":"健身笔记","uri":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/"},{"categories":["健康"],"content":"龙旗 找一个有倾斜度的仰卧起坐辅助长椅，双手抓住原本用来固定脚部的横杆，头位于横杆之下。全身肌肉紧张，从肩膀开始，到背阔肌、手臂、上半身核心肌肉和双腿。 双脚向上，直至身体几乎跟地面垂直，肩胛骨仍留在长凳上。保持身体成一直线，然后慢慢放下双腿，直至它们仅仅刚刚离开长凳，然后再次将双腿抬起至与地面垂直。 小秘诀：如果一开始做不到完整动作，可先尝试将腿抬至与地面成45°的位置，要增加难度可在腿抬到顶点时稍作停留，或者在抬腿过程中多次停留在不同位置，都可挑战肌肉稳定性。 https://baike.baidu.com/item/ 龙旗/16534074?fr=aladdin 腿部 ","date":"2022-08-23","objectID":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/:4:0","tags":["Exercise"],"title":"健身笔记","uri":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/"},{"categories":["健康"],"content":"腿后肌群 ","date":"2022-08-23","objectID":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/:5:0","tags":["Exercise"],"title":"健身笔记","uri":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/"},{"categories":["健康"],"content":"加速肌 与跑步有关 ","date":"2022-08-23","objectID":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/:5:1","tags":["Exercise"],"title":"健身笔记","uri":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/"},{"categories":["健康"],"content":"减速肌 控制人体后退 饮食 空腹暴饮暴食会让血糖迅速升高，进而导致胰岛素迅速升高，不仅向肌肉细胞传输能量，而且向脂肪细胞传输能量，导致肥胖 每周空出一天给自己放开来吃，可以让身体解除因热量摄入不足而进入的低能耗模式，从而增加减肥的效果（欺骗日） 一些喝蛋白粉的关键点： 1、凉水/温水冲泡蛋白粉 2、锻炼后30分钟饮用效果最佳 3、蛋白粉、肌酸、维生素、肌酸、蜂蜜可以一起用，不冲突 4、如果有消化不良或腹泻，不要早上喝，不要空腹喝蛋白粉，不要一次吃太多 5、蛋白粉喝了“上火”，原因是维生素B缺乏，多喝水 6、发胖的原因不是蛋白质，而是脂肪和碳水 ","date":"2022-08-23","objectID":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/:5:2","tags":["Exercise"],"title":"健身笔记","uri":"/posts/%E5%81%A5%E8%BA%AB%E7%AC%94%E8%AE%B0/"},{"categories":["健康"],"content":"神经募集能力 瘦子增肌会经历三个阶段 阶段一：力量增加很快，一个月从5kg到10kg 阶段二：力量增速变慢，两三个月才从10kg到15kg；但肌肉轮廓明显起来了 阶段三：力量和肌肉的增长都变得缓慢，增长关系比较规律 为什么第一阶段力量增速远快于肌肉增速？ 原因是神经募集能力得到了提升。其实，你的肌肉本来就有举起10kg的能力，但是由于基本没做过这个动作，大脑发给肌肉的指令并不完全，导致你只能举起5kg的重量。或者说，大脑控制肌肉的能力是可以用训练提升的。 但是神经募集能力终究是有上限的，不可能超过百分百。想要进一步提升力量，那么就要了解功能性肌肥大这一概念。 ","date":"2022-08-23","objectID":"/posts/%E8%82%8C%E8%82%89%E4%B8%8E%E5%8A%9B%E9%87%8F%E7%9A%84%E5%85%B3%E7%B3%BB/:0:1","tags":["Exercise"],"title":"肌肉与力量的关系","uri":"/posts/%E8%82%8C%E8%82%89%E4%B8%8E%E5%8A%9B%E9%87%8F%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["健康"],"content":"功能性肌肥大 肌肉是由很多肌纤维组成的，肌纤维的力量加起来就是整块肌肉的力量。而每根肌纤维还可以拆分为很多根肌原纤维——肌肉收缩的基本单位。 肌原纤维的增加就称为功能性肌肥大。 ","date":"2022-08-23","objectID":"/posts/%E8%82%8C%E8%82%89%E4%B8%8E%E5%8A%9B%E9%87%8F%E7%9A%84%E5%85%B3%E7%B3%BB/:0:2","tags":["Exercise"],"title":"肌肉与力量的关系","uri":"/posts/%E8%82%8C%E8%82%89%E4%B8%8E%E5%8A%9B%E9%87%8F%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["健康"],"content":"非功能性肌肥大 但是肌原纤维的增加是很有限的，可能练了很久都没发现自己的肌肉变的很大。其实，大肌肉主要是由肌质的贡献。 肌质可以说是肌原纤维的营养液，充满在肌原纤维之间。 肌质是没有收缩能力的，肌质再多，也没法直接提升绝对力量，顶多多做一两组。 所以，肌质带来的肌肉变大，就叫做非功能肌肥大 ","date":"2022-08-23","objectID":"/posts/%E8%82%8C%E8%82%89%E4%B8%8E%E5%8A%9B%E9%87%8F%E7%9A%84%E5%85%B3%E7%B3%BB/:0:3","tags":["Exercise"],"title":"肌肉与力量的关系","uri":"/posts/%E8%82%8C%E8%82%89%E4%B8%8E%E5%8A%9B%E9%87%8F%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["健康"],"content":"力量和维度 RM 最大重复次数 大重量 \u003c= 5 更有利于提升肌肉力量，但对维度提升不大 中等重量 8～12 利于肌质的增长，提升肌肉维度 中等重量 \u003e= 15 肌肉中毛细血管增加，耐力变好，但对力量和维度帮助不大了 ","date":"2022-08-23","objectID":"/posts/%E8%82%8C%E8%82%89%E4%B8%8E%E5%8A%9B%E9%87%8F%E7%9A%84%E5%85%B3%E7%B3%BB/:0:4","tags":["Exercise"],"title":"肌肉与力量的关系","uri":"/posts/%E8%82%8C%E8%82%89%E4%B8%8E%E5%8A%9B%E9%87%8F%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"categories":["自学笔记"],"content":"转载自python之禅 正则表达式处理文本有如疾风扫秋叶，绝大部分编程语言都内置支持正则表达式，它应用在诸如表单验证、文本提取、替换等场景。爬虫系统更是离不开正则表达式，用好正则表达式往往能收到事半功倍的效果。 介绍正则表达式前，先来看一个问题，下面这段文本来自豆瓣的某个网页链接，我对内容进行了缩减。问：如何提取文本中所有邮箱地址呢？ html = \"\"\" \u003cstyle\u003e .qrcode-app{ display: block; background: url(/pics/qrcode_app4@2x.png) no-repeat; } \u003c/style\u003e \u003cdiv class=\"reply-doc content\"\u003e \u003cp class=\"\"\u003e34613453@qq.com，谢谢了\u003c/p\u003e \u003cp class=\"\"\u003e30604259@qq.com麻烦楼主\u003c/p\u003e \u003c/div\u003e \u003cp class=\"\"\u003e490010464@163.com\u003cbr/\u003e谢谢\u003c/p\u003e \"\"\" 如果你还没接触过正则表达式，我想对此会是一筹莫展，不用正则，似乎想不到一种更好的方式来处理，不过，我们暂且放下这个问题，待学习完正则表达式之后再来考虑如何解决。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:0","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"字符串的表现形式 Python 字符串有几种表现形式，以u开头的字符串称为Unicode字符串，它不在本文讨论范围内，此外，你应该还看到过这两种写法： \u003e\u003e\u003e foo = \"hello\" \u003e\u003e\u003e bar = r\"hello\" 前者是常规字符串，后者 r 开头的是原始字符串，两者有什么区别？因为在上面的例子中，它们都是由普通文本字符组成的串，在这里没什么区别，下面可以证明 \u003e\u003e\u003e foo is bar True \u003e\u003e\u003e foo == bar True 但是，如果字符串中包括有特殊字符，会是什么情况呢？再来看一个例子： \u003e\u003e\u003e foo = \"\\n\" \u003e\u003e\u003e bar = r\"\\n\" \u003e\u003e\u003e foo, len(foo) ('\\n', 1) \u003e\u003e\u003e bar, len(bar) ('\\\\n', 2) \u003e\u003e\u003e foo == bar False \u003e\u003e\u003e \"\\n\" 是一个转义字符，它在 ASCII 中表示换行符。而 r\"\\n\" 是一个原始字符串，原始字符串不对特殊字符进行转义，它就是你看到的字面意思，由 \" \\\" 和 “n” 两个字符组成的字符串。 定义原始字符串可以用小写r或者大写R开头，比如 r\"\\b\" 或者 R\"\\b\" 都是允许的。在 Python 中，正则表达式一般用原始字符串的形式来定义，为什么呢？ 举例来说，对于字符 \"\\b\" 来说，它在 ASCII 中是有特殊意义的，表示退格键，而在正则表达式中，它是一个特殊的元字符，用于匹配一个单词的边界，为了能让正则编译器正确地表达它的意义就需要用原始字符串，当然也可以使用反斜杠 \" \\\" 对常规定义的字符串进行转义 \u003e\u003e\u003e foo = \"\\\\b\" \u003e\u003e\u003e bar = r\"\\b\" \u003e\u003e\u003e foo == bar True ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:1","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"正则基本介绍 正则表达式由普通文本字符和特殊字符（元字符）两种字符组成。元字符在正则表达式中具有特殊意义，它让正则表达式具有更丰富的表达能力。例如，正则表达式 r\"a.d\"中 ，字符'a'和 'd' 是普通字符，. 是元字符，. 可以指代任意字符，它能匹配 ‘a1d’、‘a2d’、‘acd’ ，它的匹配流程是： Python 内置模块 re 是专门用于处理正则表达式的模块。 \u003e\u003e\u003e rex = r\"a.d\" # 正则表达式文本 \u003e\u003e\u003e original_str = \"and\" # 原始文本 \u003e\u003e\u003e pattern = re.compile(rex) # 正则表达式对象 \u003e\u003e\u003e m = pattern.match(original_str) # 匹配对象 \u003e\u003e\u003e m \u003c_sre.SRE_Match object at 0x101c85b28\u003e # 等价于 \u003e\u003e\u003e re.match(r\"a.d\", \"and\") \u003c_sre.SRE_Match object at 0x10a15dcc8\u003e 如果原文本字符串与正则表达式匹配，那么就会返回一个 Match 对象，当不匹配时，match 方法返回的 None，通过判断m是否为None可进行表单验证。 接下来，我们需要学习更多元字符。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:2","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"基本元字符 .：匹配除换行符以外的任意一个字符，例如：“a.c” 可以完全匹配 “abc”，也可以匹配 “abcef” 中的 “abc” \\： 转义字符，使特殊字符具有本来的意义，例如： “1\\.“2 可以匹配 “1.2” [...]：匹配方括号中的任意一个字符，例如：a[bcd]e 可以匹配 abe、ace、ade，它还支持范围操作，比如：a到z可表示为 “a-z”，0到9可表示为 “0-9”，注意，在 “[]” 中的特殊字符不再有特殊意义，就是它字面的意义，例如：[.*]就是匹配 . 或者 * [^...]，字符集取反，表示只要不是括号中出现的字符都可以匹配，例如：“a[^bcd]e” 可匹配 aee、afe等 \u003e\u003e\u003e re.match(r\"a.c\", \"abc\").group() 'abc' \u003e\u003e\u003e re.match(r\"a.c\", \"abcef\").group() 'abc' \u003e\u003e\u003e re.match(r\"1\\.2\", \"1.2\").group() '1.2' \u003e\u003e\u003e re.match(r\"a[0-9]b\", \"a2b\").group() 'a2b' \u003e\u003e\u003e re.match(r\"a[0-9]b\", \"a5b11\").group() 'a5b' \u003e\u003e\u003e re.match(r\"a[.*?]b\", \"a.b\").group() 'a.b' \u003e\u003e\u003e re.match(r\"abc[^\\w]\", \"abc!123\").group() 'abc! group 方法返回原字符串(abcef)中与正则表达式相匹配的那部分子字符串(abc)，前提是要匹配成功 match 方法才会返回 Match 对象，进而才有group方法。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:3","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"预设元字符 \\w 匹配任意一个单词字符，包括数字和下划线，它等价于 [A-Za-z0-9_]，例如 a\\wc 可以匹配 abc、acc \\W 匹配任意一个非单词字符，与 \\w 操作相反，它等价于 [^A-Za-z0-9_]，例如： a\\Wc 可匹配 a!c \\s 匹配任意一个空白字符，空格、回车等都是空白字符，例如：a\\sc 可以配 a\\nc，这里的 \\n表示回车 \\S 匹配任意一个非空白字符 \\d 匹配任意一个数字，它等价于[0-9]，例如：a\\dc 可匹配 a1c、a2c … \\D 匹配任意一个非数字 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:4","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"边界匹配 边界匹配相关的符号专门用于修饰字符。 ^ 匹配字符的开头，在字符串的前面，例如：^abc 表示匹配 a开头，后面紧随bc的字符串，它可以匹配 abc $ 匹配字符的结尾，在字符串的末尾位置，例如： hello$ \u003e\u003e\u003e re.match(r\"^abc\",\"abc\").group() 'abc' \u003e\u003e\u003e re.match(r\"^abc$\",\"abc\").group() 'abc' ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:5","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"重复匹配 前面的元字符都是针对单个字符来匹配的，如果希望匹配的字符重复出现，比如匹配身份证号码，长度18位，那么就需要用到重复匹配的元字符 * 重复匹配零次或者更多次 ? 重复匹配零次或者一次 + 重复匹配1次或者多次 {n} 重复匹配n次 {n,} 重复匹配至少n次 {n, m} 重复匹配n到m次 # 简单匹配身份证号码，前面17位是数字，最后一位可以是数字或者字母X \u003e\u003e\u003e re.match(r\"\\d{17}[\\dX]\", \"42350119900101153X\").group() '42350119900101153X' # 匹配5到12的QQ号码 \u003e\u003e\u003e re.match(r\"\\d{5,12}$\", \"4235011990\").group() '4235011990' ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:6","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"逻辑分支 匹配一个固定电话号码，不同地区规则不一样，有的地方区号是3位，电话是8位，有的地方区号是4位，电话为7位，区号与号码之间用 - 隔开，如果应对这样的需求呢？这时你需要用到逻辑分支条件字符 |，它把表达式分为左右两部分，先尝试匹配左边部分，如果匹配成功就不再匹配后面部分了，这是逻辑 “或” 的关系 # abc|cde 可以匹配abc 或者 cde，但优先匹配abc \u003e\u003e\u003e re.match(r\"aa(abc|cde)\",\"aaabccde\").group() 'aaabc' 0\\d{2}-\\d{8}|0\\d{3}-\\d{7} 表达式以0开头，既可以匹配3位区号8位号码，也可以匹配4位区号7位号码 \u003e\u003e\u003e re.match(r\"0\\d{2}-\\d{8}|0\\d{3}-\\d{7}\", \"0755-4348767\").group() '0755-4348767' \u003e\u003e\u003e re.match(r\"0\\d{2}-\\d{8}|0\\d{3}-\\d{7}\", \"010-34827637\").group() '010-34827637' ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:7","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"分组 前面介绍的匹配规则都是针对单个字符而言的，如果想要重复匹配多个字符怎么办，答案是，**用子表达式（也叫分组）**来表示，分组用小括号”()“表示，例如 (abc){2} 表示匹配abc两次， 匹配一个IP地址时，可以使用 (\\d{1,3}\\.){3}\\d{1,3}，因为IP是由4组数组3个点组成的，前面3组数字和3个点可以作为一个分组重复3次，最后一部分是一个1到3个数字组成的字符串。如：192.168.0.1。 关于分组，group 方法可用于提取匹配的字符串分组，默认它会把整个表达式的匹配结果当做第0个分组，就是不带参数的 group() 或者是 group(0)，第一组括号中的分组用group(1)获取，以此类推 \u003e\u003e\u003e m = re.match(r\"(\\d+)(\\w+)\", \"123abc\") ＃分组０，匹配整个正则表达式 \u003e\u003e\u003e m.group() '123abc' #等价 \u003e\u003e\u003e m.group(0) '123abc' # 分组1，匹配第一对括号 \u003e\u003e\u003e m.group(1) '123' # 分组2，匹配第二对括号 \u003e\u003e\u003e m.group(2) 'abc' \u003e\u003e\u003e 通过分组，我们可以从字符串中提取出想要的信息。另外，分组还可以通过指定名字的方式获取。 # 第一个分组的名字是number # 第二个分组的名字是char \u003e\u003e\u003e m = re.match(r\"(?P\u003cnumber\u003e\\d+)(?P\u003cchar\u003e\\w+)\", \"123abc\") \u003e\u003e\u003e m.group(\"number\") '123' # 等价 \u003e\u003e\u003e m.group(1) '123' ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:8","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"贪婪与非贪婪 默认情况下，正则表达式重复匹配时，在使整个表达式能得到匹配的前提下尽可能匹配多的字符，我们称之为贪婪模式，是一种贪得无厌的模式。例如： r\"a.*b\" 表示匹配 a 开头 b 结尾，中间可以是任意多个字符的字符串，如果用它来匹配 aaabcb，那么它会匹配整个字符串。 \u003e\u003e\u003e re.match(r\"a.*b\", \"aaabcb\").group() 'aaabcb' 有时，我们希望尽可能少的匹配，怎么办？只需要在量词后面加一个问号” ？\"，在保证匹配的情况下尽可能少的匹配，比如刚才的例子，我们只希望匹配 aaab，那么只需要修改正则表达式为 r\"a.*?b\" \u003e\u003e\u003e re.match(r\"a.*?b\", \"aaabcb\").group() 'aaab' \u003e\u003e\u003e 非贪婪模式在爬虫应用中使用非常频繁。在网页上涉及img标签元素是相对路径的情况，我们需要把它替换成绝对路径 \u003e\u003e\u003e html = '\u003cimg src=\"/images/category.png\"\u003e\u003cimg src=\"/images/js_framework.png\"\u003e' # 非贪婪模式就匹配的两个img标签 # 你可以改成贪婪模式看看可以匹配几个 \u003e\u003e\u003e rex = r'\u003cimg.*?src=\"(.*?)\"\u003e' \u003e\u003e\u003e re.findall(rex, html) ['/images/category.png', '/images/js_framework.png'] \u003e\u003e\u003e \u003e\u003e\u003e def fun(match): ... img_tag = match.group() ... src = match.group(1) ... full_src = \"http://foofish.net\" + src ... new_img_tag = img_tag.replace(src, full_src) ... return new_img_tag ... \u003e\u003e\u003e re.sub(rex, fun, html) \u003cimg src=\"http://foofish.net/images/category.png\"\u003e\u003cimg src=\"http://foofish.net/images/js_framework.png\"\u003e sub 函数可以接受一个函数作为替换目标对象，函数返回值用来替换正则表达式匹配的部分，在这里，我把整个img标签定义为一个正则表达式 r’’，group() 返回的值是 \u003cimg src=\"/images/category.png\"\u003e，而 group(1) 的返回值是 /images/category.png，最后，我用 replace 方法把相对路径替换成绝对路径。 以上讲解了正则表达式的基本概念和语法以及re模块的基本使用方式，这节来详细说说 re 模块作为 Python 正则表达式引擎提供了哪些便利性操作。 \u003e\u003e\u003e import re 正则表达式的所有操作都是围绕着匹配对象(Match)进行的，只有表达式与字符串匹配才有可能进行后续操作。判断匹配与否有两个方法，分别是 re.match() 和 re.search()，两者有什么区别呢？ ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:9","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"re.match(pattern, string) match 方法从字符串的起始位置开始检查，如果刚好有一个子字符串与正则表达式相匹配，则返回一个Match对象，只要起始位置不匹配则退出，不再往后检查了，返回 None \u003e\u003e\u003e re.match(r\"b.r\", \"foobar\") # 不匹配 \u003e\u003e\u003e re.match(r\"b.r\", \"barfoo\") # 匹配 \u003c_sre.SRE_Match object at 0x102f05b28\u003e \u003e\u003e\u003e ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:10","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"re.search(pattern, string) search 方法虽然也是从起始位置开始检查，但是它在起始位置不匹配的时候会一直尝试往后检查，直到匹配为止，如果到字符串的末尾还没有匹配，则返回 None \u003e\u003e\u003e re.search(r\"b.r\", \"foobar\") # 匹配 \u003c_sre.SRE_Match object at 0x000000000254D578\u003e \u003e\u003e\u003e re.match(r\"b.r\", \"foobr\") # 不匹配 两者接收参数都是一样的，第一个参数是正则表达式，第二个是预匹配的字符串。另外，不管是search还是 match，一旦找到了匹配的子字符串，就立刻停止往后找，哪怕字符串中有多个可匹配的子字符串，例如 \u003e\u003e\u003e re.search(r\"f.o\", \"foobarfeobar\").group() 'foo' 两者的差异使得他们在应用场景上也不一样，如果是检查文本是否匹配某种模式，比如，检查字符串是不是有效的邮箱地址，则可以使用 match 来判断： \u003e\u003e\u003e rex = r\"[\\w]+@[\\w]+\\.[\\w]+$\" \u003e\u003e\u003e re.match(rex, \"123@qq.com\") # 匹配 \u003c_sre.SRE_Match object at 0x102f05bf8\u003e \u003e\u003e\u003e re.match(rex, \"the email is 123@qq.com\") # 不匹配 \u003e\u003e\u003e 尽管第二个字符串中包含有邮件地址，但字符串整体不能当作一个邮件地址来使用，在网页上填邮件地址时，显然第二种写法是无效的。 通常，search 方法可用于判断字符串中是否包含有与正则表达式相匹配的子字符串，还可以从中提出匹配的子字符串，例如： \u003e\u003e\u003e rex = r\"[\\w]+@[\\w]+\\.[\\w]+\" \u003e\u003e\u003e m = re.search(rex, \"the email is 123@qq.com .\") \u003e\u003e\u003e m is None False \u003e\u003e\u003e m.group() '123@qq.com' \u003e\u003e\u003e 细心的你可能已经发现了，上面例子与前面例子的正则表达式写法有细微区别，前者多一个元字符 $，它的目的是用于完全匹配字符串。因为不加 $，那么下面这种情况用match方法也匹配，显示这在表单验证时是无法满足要求的。 \u003e\u003e\u003e rex = r\"[\\w]+@[\\w]+\\.[\\w]+\" \u003e\u003e\u003e re.match(rex, \"123@qq.com is my email\") \u003c_sre.SRE_Match object at 0x10cadebf8\u003e \u003e\u003e\u003e 那么有没有可能不加$，就可以判断是否完全匹配字符串呢？在 Python3 中，re.fullmatch 就可以满足这样的需求。 \u003e\u003e\u003e rex = r\"[\\w]+@[\\w]+\\.[\\w]+\" \u003e\u003e\u003e re.fullmatch(rex, \"123@qq.com is my email\") # 不匹配 \u003e\u003e\u003e re.fullmatch(rex, \"123@qq.com\") # 匹配 \u003c_sre.SRE_Match object; span=(0, 10), match='123@qq.com'\u003e 虽然二者都可以通过 group() 提取出匹配的子字符串，但是，如果字符串中有多个匹配的子字符串时，两个方法都不行，因为它们都是在一旦匹配了第一个子字符串，就不再往后匹配了。 \u003e\u003e\u003e m = re.search(rex, \"email is 123@qq.com, anthor email is abc@gmail.com !\") \u003e\u003e\u003e m.group() '123@qq.com' 那么如何把文本中的所有匹配的邮件地址提取出来呢？re 模块为我们准备了 re.findall() 和 re.finditer() 这两个方法，它们会返回文本中所有与正则表达式相匹配的内容。前者返回的是一个列表(list)对象，后者返回的是一个迭代器(iterator)。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:11","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"re.findall(pattern, string) \u003e\u003e\u003e emails = re.findall(rex, \"email is 123@qq.com, anthor email is abc@gmail.com\") \u003e\u003e\u003e emails ['123@qq.com', 'abc@gmail.com'] findall 返回的对象是由匹配的子字符串组成的列表，它返回了所有匹配的邮件地址。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:12","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"re.finditer(pattern, string) \u003e\u003e\u003e emails = re.finditer(rex, \"email is 123@qq.com, anthor email is abc@gmail.com\") \u003e\u003e\u003e emails \u003ccallable-iterator object at 0x0000000002592390\u003e \u003e\u003e\u003e for e in emails: ... print(e.group()) ... 123@qq.com abc@gmail.com finditer 返回的对象是由 Match 对象组成的迭代器，因为里面的元素是Match对象，所以要获取里面的邮件地址还需要调用group方法来提取。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:13","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"re.split 我们都知道字符串有一个split方法，可根据某个子串分隔字符串，如： \u003e\u003e\u003e \"this is a string.\".split(\" \") ['this', 'is', 'a', 'string.'] 但该方法有一个缺陷，比如上面的字符串，根据空格分隔字符串时，字符串后面多一个点，如果用 re.split 就可以避免这种情况。 \u003e\u003e\u003e words = re.split(r\"\\W+\", \"this is a string.\") \u003e\u003e\u003e words ['this', 'is', 'a', 'string', ''] \u003e\u003e\u003e list(filter(lambda x: x, words)) # lambda x: x 的意思是 x 非空的就是True， 就在words里选出来 ['this', 'is', 'a', 'string'] \u003e\u003e\u003e re.split是一种更为高级的字符串分隔操作的方法。在这里，split根据非字母正则来分隔字符串，但凡是 string.split 没法处理的问题，可以考虑使用re模块下的split方法来处理。此外，正则表达式中如果有分组括号，那么返回结果又不一致，这个可以留给大家查阅文档，某些场景用得着。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:14","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"re.sub(pattern, repl, string) re.split是一种更为高级的字符串分隔操作的方法。在这里，split根据非字母正则来分隔字符串，但凡是 string.split 没法处理的问题，可以考虑使用re模块下的split方法来处理。此外，正则表达式中如果有分组括号，那么返回结果又不一致，这个可以留给大家查阅文档，某些场景用得着。 把所有邮箱地址替换成 admin@qq.com \u003e\u003e\u003e rex = r\"[\\w]+@[\\w]+\\.[\\w]+\" # 邮件地址正则 \u003e\u003e\u003e re.sub(rex, \"admin@qq.com\", \"234@qq.com, 456@qq.com \") 'admin@qq.com, admin@qq.com ' \u003e\u003e\u003e 另外一个例子，就是上次讲过的将 img 标签的 src 路径替换成绝对完整的URL地址 html = \"\"\" ... \u003cimg src=\"/images/category.png\"\u003e this is anthor words \u003cimg src=\"http://foofish.net/images/js_framework.png\"\u003e \"\"\" 如果用字符串的replace方法是没法实现了，这时需要用到正则表达式的 re.sub，正则表达式应用了非贪婪模式，使用了一个分组，用于提取 src 的路径。 rex = r'.*?\u003cimg src=\"(.*?)\".*?\u003e' 这里我们要把替换目标 repl 作为函数来处理。 def fun(m): img_tag = m.group() src = m.group(1) if not src.startswith(\"http:\"): full_src = \"http://foofish.net\" + src else: full_src = src new_img_tag = img_tag.replace(src, full_src) return new_img_tag 引擎会自动把所有匹配的结果应用到该函数中，函数的参数就是每一个匹配的Match对象，通过 group(1) 提取分组后判断是否为一个完整的URL路径，只有是不完整的我们才替换，否则还是按照原来的方式返回。 new_html = re.compile(rex).sub(fun, html) print(new_html) # 输出 ... \u003cimg src=\"http://foofish.net/images/category.png\"\u003e this is anthor words \u003cimg src=\"http://foofish.net/images/js_framework.png\"\u003e 如果还想知道替换次数是多少，那么可以使用 re.subn方法，这个方法具体使用可以参考文档，留着读者自己思考。 此外，以上方法都有一个默认的 flag 参数，该参数用于改变匹配的行为，常用的可选值有： re.I(IGNORECASE): 忽略大小写（括号内的单词为完整写法，两种方式都支持） re.M(MULTILINE): 多行模式，改变’^‘和’$‘的行为 re.S(DOTALL): 改变’.‘的行为，默认 . 只能匹配除换行之外的字符，加上它就可以匹配换行了 例如： \u003e\u003e\u003e re.match(r\"foo\", \"FoObar\", re.I) \u003c_sre.SRE_Match object; span=(0, 3), match='FoO'\u003e \u003e\u003e\u003e 以上介绍的都是 re 模块下面的方法，其实，这些只不过是一些简便方法，例如 re.match 方法 re.match(r'foo', 'foo bar') 等价于 pattern = re.compile(r'foo') pattern.match('foo bar') 那么，后者有什么好处呢？为了提高正则匹配的速度，它可以重复利用正则对象，如果一个正则表达式需要匹配多个字符串，那么就推荐后者，先编译在去匹配。更多使用方式可以参考文档 https://docs.python.org/3/library/re.html ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB4/:0:15","tags":["爬虫","python"],"title":"爬虫(四)正则表达式","uri":"/posts/%E7%88%AC%E8%99%AB4/"},{"categories":["自学笔记"],"content":"转载自python之禅 不同的网站返回的内容通常有多种不同的格式，一种是 json 格式，这类数据对开发者来说最友好。另一种 XML 格式的，还有一种最常见格式的是 HTML 文档。 BeautifulSoup 专注于 HTML 文档操作，名字来源于 Lewis Carroll 的一首同名诗歌。 BeautifulSoup 是一个用于解析 HTML 文档的 Python 库，通过 BeautifulSoup，你只需要用很少的代码就可以提取出 HTML 中任何感兴趣的内容，此外，它还有一定的 HTML 容错能力，对于一个格式不完整的HTML 文档，它也可以正确处理。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:0","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"安装 BeautifulSoup pip install beautifulsoup4 BeautifulSoup3 被官方放弃维护，你要下载最新的版本 BeautifulSoup4。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:1","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"HTML 标签 学习 BeautifulSoup4 前有必要先对 HTML 文档有一个基本认识，如下代码，HTML 是一个树形组织结构。 \u003chtml\u003e \u003chead\u003e \u003ctitle\u003ehello, world\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eBeautifulSoup\u003c/h1\u003e \u003cp\u003e如何使用BeautifulSoup\u003c/p\u003e \u003cbody\u003e \u003c/html\u003e 它由很多标签（Tag）组成，比如 html、head、title等等都是标签 一个标签对构成一个节点，比如 … 是一个根节点 节点之间存在某种关系，比如 h1 和 p 互为邻居，他们是相邻的兄弟（sibling）节点 h1 是 body 的直接子（children）节点，还是 html 的子孙（descendants）节点 body 是 p 的父（parent）节点，html 是 p 的祖辈（parents）节点 嵌套在标签之间的字符串是该节点下的一个特殊子节点，比如 “hello, world” 也是一个节点，只不过没名字。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:2","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"使用 BeautifulSoup 构建一个 BeautifulSoup 对象需要两个参数: 第一个参数是将要解析的 HTML 文本字符串 第二个参数告诉 BeautifulSoup 使用哪个解析器来解析 HTML。 解析器负责把 HTML 解析成相关的对象，而 BeautifulSoup 负责操作数据（增删改查）。”html.parser” 是Python内置的解析器，”lxml” 则是一个基于c语言开发的解析器，它的执行速度更快，不过它需要额外安装 通过 BeautifulSoup 对象就可以定位到 HTML 中的任何一个标签节点。 from bs4 import BeautifulSoup text = \"\"\" \u003chtml\u003e \u003chead\u003e \u003ctitle \u003ehello, world\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eBeautifulSoup\u003c/h1\u003e \u003cp class=\"bold\"\u003e如何使用BeautifulSoup\u003c/p\u003e \u003cp class=\"big\" id=\"key1\"\u003e 第二个p标签\u003c/p\u003e \u003ca href=\"http://foofish.net\"\u003epython\u003c/a\u003e \u003c/body\u003e \u003c/html\u003e \"\"\" soup = BeautifulSoup(text, \"html.parser\") # title 标签 \u003e\u003e\u003e soup.title \u003ctitle\u003ehello, world\u003c/title\u003e # p 标签 \u003e\u003e\u003e soup.p \u003cp class=\"bold\"\u003e\\u5982\\u4f55\\u4f7f\\u7528BeautifulSoup\u003c/p\u003e # p 标签的内容 \u003e\u003e\u003e soup.p.string u'\\u5982\\u4f55\\u4f7f\\u7528BeautifulSoup' BeatifulSoup 将 HTML 抽象成为 4 类主要的数据类型，分别是Tag , NavigableString , BeautifulSoup，Comment 。每个标签节点就是一个Tag对象，NavigableString 对象一般是包裹在Tag对象中的字符串，BeautifulSoup 对象代表整个 HTML 文档。例如： \u003e\u003e\u003e type(soup) \u003cclass 'bs4.BeautifulSoup'\u003e \u003e\u003e\u003e type(soup.h1) \u003cclass 'bs4.element.Tag'\u003e \u003e\u003e\u003e type(soup.p.string) \u003cclass 'bs4.element.NavigableString'\u003e ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:3","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"Tag 每个 Tag 都有一个名字，它对应 HTML 的标签名称。 \u003e\u003e\u003e soup.h1.name u'h1' \u003e\u003e\u003e soup.p.name u'p' 标签还可以有属性，属性的访问方式和字典是类似的，它返回一个列表对象 \u003e\u003e\u003e soup.p['class'] [u'bold'] ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:4","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"NavigableString 获取标签中的内容，直接使用.stirng即可获取，它是一个 NavigableString 对象，你可以显式地将它转换为 unicode 字符串。 \u003e\u003e\u003e soup.p.string u'\\u5982\\u4f55\\u4f7f\\u7528BeautifulSoup' \u003e\u003e\u003e type(soup.p.string) \u003cclass 'bs4.element.NavigableString'\u003e \u003e\u003e\u003e unicode_str = unicode(soup.p.string) \u003e\u003e\u003e unicode_str u'\\u5982\\u4f55\\u4f7f\\u7528BeautifulSoup' 基本概念介绍完，现在可以正式进入主题了，如何从 HTML 中找到我们关心的数据？BeautifulSoup 提供了两种方式，一种是遍历，另一种是搜索，通常两者结合来完成查找任务。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:5","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"遍历文档树 遍历文档树，顾名思义，就是是从根节点 html 标签开始遍历，直到找到目标元素为止，遍历的一个缺陷是，如果你要找的内容在文档的末尾，那么它要遍历整个文档才能找到它，速度上就慢了。因此还需要配合第二种方法。 通过遍历文档树的方式获取标签节点可以直接通过 .标签名的方式获取，例如： 获取 body 标签： \u003e\u003e\u003e soup.body \u003cbody\u003e\\n\u003ch1\u003eBeautifulSoup\u003c/h1\u003e\\n\u003cp class=\"bold\"\u003e\\u5982\\u4f55\\u4f7f\\u7528BeautifulSoup\u003c/p\u003e\\n\u003c/body\u003e 获取 p 标签 \u003e\u003e\u003e soup.body.p \u003cp class=\"bold\"\u003e\\u5982\\u4f55\\u4f7f\\u7528BeautifulSoup\u003c/p\u003e 获取 p 标签的内容 \u003e\u003e\u003e soup.body.p.string \\u5982\\u4f55\\u4f7f\\u7528BeautifulSoup 前面说了，内容也是一个节点，这里就可以用 .string 的方式得到。遍历文档树的另一个缺点是只能获取到与之匹配的第一个子节点，例如，如果有两个相邻的 p 标签时，第二个标签就没法通过 .p 的方式获取，这是需要借用 next_sibling 属性获取相邻且在后面的节点。此外，还有很多不怎么常用的属性，比如：.contents 获取所有子节点，.parent 获取父节点，更多的参考请查看官方文档 。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:6","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"搜索文档树 搜索文档树是通过指定标签名来搜索元素，另外还可以通过指定标签的属性值来精确定位某个节点元素，最常用的两个方法就是 find 和 find_all。这两个方法在 BeatifulSoup 和 Tag 对象上都可以被调用。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:7","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"find_all() find_all( name , attrs , recursive , text , **kwargs ) find_all 的返回值是一个 Tag 组成的列表，方法调用非常灵活，所有的参数都是可选的。 第一个参数 name 是标签节点的名字。 # 找到所有标签名为title的节点 \u003e\u003e\u003e soup.find_all(\"title\") [\u003ctitle\u003ehello, world\u003c/title\u003e] \u003e\u003e\u003e soup.find_all(\"p\") [\u003cp class=\"bold\"\u003e\\xc8\\xe7\\xba\\xce\\xca\\xb9\\xd3\\xc3BeautifulSoup\u003c/p\u003e, \u003cp class=\"big\"\u003e \\xb5\\xda\\xb6\\xfe\\xb8\\xf6p\\xb1\\xea\\xc7\\xa9\u003c/p\u003e] 第二个参数是标签的class属性值 # 找到所有class属性为big的p标签 \u003e\u003e\u003e soup.find_all(\"p\", \"big\") [\u003cp class=\"big\"\u003e \\xb5\\xda\\xb6\\xfe\\xb8\\xf6p\\xb1\\xea\\xc7\\xa9\u003c/p\u003e] 等效于 \u003e\u003e\u003e soup.find_all(\"p\", class_=\"big\") [\u003cp class=\"big\"\u003e \\xb5\\xda\\xb6\\xfe\\xb8\\xf6p\\xb1\\xea\\xc7\\xa9\u003c/p\u003e] 因为 class 是 Python 关键字，所以这里指定为 class_。 kwargs 是标签的属性名值对，例如：查找有href(指定超链接目标的URL)属性值为 “http://foofish.net ” 的标签 \u003e\u003e\u003e soup.find_all(href=\"http://foofish.net\") [\u003ca href=\"http://foofish.net\"\u003epython\u003c/a\u003e] 当然，它还支持正则表达式 \u003e\u003e\u003e import re \u003e\u003e\u003e soup.find_all(href=re.compile(\"^http\")) [\u003ca href=\"http://foofish.net\"\u003epython\u003c/a\u003e] 属性除了可以是具体的值、正则表达式之外，它还可以是一个布尔值（True/Flase），表示有属性或者没有该属性。 \u003e\u003e\u003e soup.find_all(id=\"key1\") [\u003cp class=\"big\" id=\"key1\"\u003e \\xb5\\xda\\xb6\\xfe\\xb8\\xf6p\\xb1\\xea\\xc7\\xa9\u003c/p\u003e] \u003e\u003e\u003e soup.find_all(id=True) [\u003cp class=\"big\" id=\"key1\"\u003e \\xb5\\xda\\xb6\\xfe\\xb8\\xf6p\\xb1\\xea\\xc7\\xa9\u003c/p\u003e] 遍历和搜索相结合查找，先定位到 body 标签，缩小搜索范围，再从 body 中找 a 标签。 \u003e\u003e\u003e body_tag = soup.body \u003e\u003e\u003e body_tag.find_all(\"a\") [\u003ca href=\"http://foofish.net\"\u003epython\u003c/a\u003e] ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:8","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"find() find 方法跟 find_all 类似，唯一不同的地方是，它返回的单个 Tag 对象而非列表，如果没找到匹配的节点则返回 None。如果匹配多个 Tag，只返回第0个。 \u003e\u003e\u003e body_tag.find(\"a\") \u003ca href=\"http://foofish.net\"\u003epython\u003c/a\u003e \u003e\u003e\u003e body_tag.find(\"p\") \u003cp class=\"bold\"\u003e\\xc8\\xe7\\xba\\xce\\xca\\xb9\\xd3\\xc3BeautifulSoup\u003c/p\u003e ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:9","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"get_text() 获取标签里面内容，除了可以使用 .string 之外，还可以使用 get_text 方法，不同的地方在于前者返回的一个 NavigableString 对象，后者返回的是 unicode 类型的字符串。 \u003e\u003e\u003e p1 = body_tag.find('p').get_text() \u003e\u003e\u003e type(p1) \u003ctype 'unicode'\u003e \u003e\u003e\u003e p1 u'\\xc8\\xe7\\xba\\xce\\xca\\xb9\\xd3\\xc3BeautifulSoup' \u003e\u003e\u003e p2 = body_tag.find(\"p\").string \u003e\u003e\u003e type(p2) \u003cclass 'bs4.element.NavigableString'\u003e \u003e\u003e\u003e p2 u'\\xc8\\xe7\\xba\\xce\\xca\\xb9\\xd3\\xc3BeautifulSoup' \u003e\u003e\u003e 实际场景中我们一般使用 get_text 方法获取标签中的内容。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:10","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"总结 BeatifulSoup 是一个用于操作 HTML 文档的 Python 库，初始化 BeatifulSoup 时，需要指定 HTML 文档字符串和具体的解析器。BeatifulSoup 有3类常用的数据类型，分别是 Tag、NavigableString、和 BeautifulSoup。查找 HTML元素有两种方式，分别是遍历文档树和搜索文档树，通常快速获取数据需要二者结合。 ","date":"2022-08-23","objectID":"/posts/%E7%88%AC%E8%99%AB3/:0:11","tags":["爬虫","python"],"title":"爬虫(三)BeautifulSoup","uri":"/posts/%E7%88%AC%E8%99%AB3/"},{"categories":["自学笔记"],"content":"经常写爬虫的都知道，有些页面在登录之前是被禁止抓取的，比如知乎的话题页面就要求用户登录才能访问，而 “登录” 离不开 HTTP 中的 Cookie 技术。 ","date":"2022-08-22","objectID":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/:0:0","tags":["爬虫","python"],"title":"登录与cookie","uri":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/"},{"categories":["自学笔记"],"content":"登录原理 Cookie 的原理非常简单，因为 HTTP 是一种无状态的协议，因此为了在无状态的 HTTP 协议之上维护会话（session）状态，让服务器知道当前是和哪个客户在打交道，Cookie 技术出现了 ，Cookie 相当于是服务端分配给客户端的一个标识。 浏览器第一次发起 HTTP 请求时，没有携带任何 Cookie 信息 服务器把 HTTP 响应，同时还有一个 Cookie 信息，一起返回给浏览器 浏览器第二次请求就把服务器返回的 Cookie 信息一起发送给服务器 服务器收到HTTP请求，发现请求头中有Cookie字段， 便知道之前就和这个用户打过交道了。 ","date":"2022-08-22","objectID":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/:0:1","tags":["爬虫","python"],"title":"登录与cookie","uri":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/"},{"categories":["自学笔记"],"content":"实战应用 用过知乎的都知道，只要提供用户名和密码以及验证码之后即可登录。当然，这只是我们眼中看到的现象。而背后隐藏的技术细节就需要借助浏览器来挖掘了。现在我们就用 Chrome 来查看当我们填完表单后，究竟发生了什么？ （如果已经登录的，先退出）首先进入知乎的登录页面 https://www.zhihu.com/#signin ，打开 Chrome 的开发者工具条（按 F12）先尝试输入一个错误的验证码观察浏览器是如何发送请求的。 从浏览器的请求可以发现几个关键的信息 登录的 URL 地址是 https://www.zhihu.com/login/email 登录需要提供的表单数据有4个：用户名（email）、密码（password）、验证码（captcha）、_xsrf。 获取验证码的URL地址是 https://www.zhihu.com/captcha.gif?r=1490690391695\u0026type=login _xsrf 是什么？如果你对 CSRF（跨站请求伪造）攻击非常熟悉的话，那么你一定知道它的作用，xsrf是一串伪随机数，它是用于防止跨站请求伪造的。它一般存在网页的 form 表单标签中，为了证实这一点，可以在页面上搜索 “xsrf”，果然，_xsrf在一个隐藏的 input 标签中。 摸清了浏览器登录时所需要的数据是如何获取之后，那么现在就可以开始写代码用 Python 模拟浏览器来登录了。登录时所依赖的两个第三方库是 requests 和 BeautifulSoup，先安装 pip install beautifulsoup4==4.5.3 pip install requests==2.13.0 http.cookiejar 模块可用于自动处理HTTP Cookie，LWPCookieJar 对象就是对 cookies 的封装，它支持把 cookies 保存到文件以及从文件中加载。 而 session 对象 提供了 Cookie 的持久化，连接池功能，可以通过 session 对象发送请求 首先从cookies.txt 文件中加载 cookie信息，因为首次运行还没有cookie，所有会出现 LoadError 异常。 from http import cookiejar session = requests.session() session.cookies = cookiejar.LWPCookieJar(filename='cookies.txt') try: session.cookies.load(ignore_discard=True) except LoadError: print(\"load cookies failed\") ","date":"2022-08-22","objectID":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/:0:2","tags":["爬虫","python"],"title":"登录与cookie","uri":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/"},{"categories":["自学笔记"],"content":"获取 xsrf 前面已经找到了 xsrf 所在的标签，，利用 BeatifulSoup 的 find 方法可以非常便捷的获取该值 def get_xsrf(): response = session.get(\"https://www.zhihu.com\", headers=headers) soup = BeautifulSoup(response.content, \"html.parser\") xsrf = soup.find('input', attrs={\"name\": \"_xsrf\"}).get(\"value\") return xsrf ","date":"2022-08-22","objectID":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/:0:3","tags":["爬虫","python"],"title":"登录与cookie","uri":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/"},{"categories":["自学笔记"],"content":"获取验证码 验证码是通过 /captcha.gif 接口返回的，这里我们把验证码图片下载保存到当前目录，由人工识别，当然你可以用第三方支持库来自动识别，比如 pytesser。 def get_captcha(): \"\"\" 把验证码图片保存到当前目录，手动识别验证码 :return: \"\"\" t = str(int(time.time() * 1000)) captcha_url = 'https://www.zhihu.com/captcha.gif?r=' + t + \"\u0026type=login\" r = session.get(captcha_url, headers=headers) with open('captcha.jpg', 'wb') as f: f.write(r.content) captcha = input(\"验证码：\") return captcha ","date":"2022-08-22","objectID":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/:0:4","tags":["爬虫","python"],"title":"登录与cookie","uri":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/"},{"categories":["自学笔记"],"content":"登录 一切参数准备就绪之后，就可以请求登录接口了。 def login(email, password): login_url = 'https://www.zhihu.com/login/email' data = { 'email': email, 'password': password, '_xsrf': get_xsrf(), \"captcha\": get_captcha(), 'remember_me': 'true'} response = session.post(login_url, data=data, headers=headers) login_code = response.json() print(login_code['msg']) for i in session.cookies: print(i) session.cookies.save() 请求成功后，session 会自动把 服务端的返回的cookie 信息填充到 session.cookies 对象中，下次请求时，客户端就可以自动携带这些cookie去访问那些需要登录的页面了。 源码：https://github.com/lzjun567/crawler_html2pdf/blob/master/zhihu/auto_login.py 参考资料： 关于 HTTP 协议的简单介绍，推荐公众号「Python之禅」中写的一篇文章一次完整的HTTP请求过程 https://docs.python.org/3/library/http.cookies.html http://docs.python-requests.org/en/master/user/advanced/#session-objects ","date":"2022-08-22","objectID":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/:0:5","tags":["爬虫","python"],"title":"登录与cookie","uri":"/posts/%E7%99%BB%E5%BD%95%E4%B8%8Ecookie/"},{"categories":["自学笔记"],"content":"转载自python之禅 urllib、urllib2、urllib3、httplib、httplib2 都是和 HTTP 相关的 Python 模块，看名字就觉得很反人类，更糟糕的是这些模块在 Python2 与 Python3 中有很大的差异，如果业务代码要同时兼容 2 和 3，写起来会让人崩溃。 好在，还有一个非常惊艳的 HTTP 库叫 requests，它是 GitHUb 关注数最多的 Python 项目之一，requests 的作者是 Kenneth Reitz 大神。 requests 实现了 HTTP 协议中绝大部分功能，它提供的功能包括 Keep-Alive、连接池、Cookie持久化、内容自动解压、HTTP代理、SSL认证、连接超时、Session等很多特性，最重要的是它同时兼容 python2 和 python3。requests 的安装可以直接使用 pip 方法：pip install requests ","date":"2022-08-22","objectID":"/posts/%E7%88%AC%E8%99%AB2/:0:0","tags":["爬虫","python"],"title":"爬虫(二)HTTP库","uri":"/posts/%E7%88%AC%E8%99%AB2/"},{"categories":["自学笔记"],"content":"发送请求 import requests response = requests.get(\"https://foofish.net\") ","date":"2022-08-22","objectID":"/posts/%E7%88%AC%E8%99%AB2/:0:1","tags":["爬虫","python"],"title":"爬虫(二)HTTP库","uri":"/posts/%E7%88%AC%E8%99%AB2/"},{"categories":["自学笔记"],"content":"相应内容 请求返回的值是一个Response 对象，Response 对象是对 HTTP 协议中服务端返回给浏览器的响应数据的封装，响应的中的主要元素包括: 状态码、原因短语、响应首部、响应体等等，这些属性都封装在Response 对象中。 # 状态码 \u003e\u003e\u003e response.status_code 200 # 原因短语 \u003e\u003e\u003e response.reason 'OK' # 响应首部 \u003e\u003e\u003e for name,value in response.headers.items(): ... print(\"%s:%s\" % (name, value)) ... Content-Encoding:gzip Server:nginx/1.10.2 Date:Thu, 06 Apr 2017 16:28:01 GMT # 响应内容 \u003e\u003e\u003e response.content '\u003chtml\u003e\u003cbody\u003e此处省略一万字...\u003c/body\u003e\u003c/html\u003e requests 除了支持 GET 请求外，还支持 HTTP 规范中的其它所有方法，包括 POST、PUT、DELTET、HEADT、OPTIONS方法。 ","date":"2022-08-22","objectID":"/posts/%E7%88%AC%E8%99%AB2/:0:2","tags":["爬虫","python"],"title":"爬虫(二)HTTP库","uri":"/posts/%E7%88%AC%E8%99%AB2/"},{"categories":["自学笔记"],"content":"查询参数 很多URL都带有很长一串参数，我们称这些参数为URL的查询参数，用\"?“附加在URL链接后面，多个参数之间用”\u0026“隔开，比如：http://fav.foofish.net/?p=4\u0026s=20 ，现在你可以用字典来构建查询参数： \u003e\u003e\u003e args = {\"p\": 4, \"s\": 20} \u003e\u003e\u003e response = requests.get(\"http://fav.foofish.net\", params = args) \u003e\u003e\u003e response.url 'http://fav.foofish.net/?p=4\u0026s=2' ","date":"2022-08-22","objectID":"/posts/%E7%88%AC%E8%99%AB2/:0:3","tags":["爬虫","python"],"title":"爬虫(二)HTTP库","uri":"/posts/%E7%88%AC%E8%99%AB2/"},{"categories":["自学笔记"],"content":"请求首部 requests 可以很简单地指定请求首部字段 Headers，比如有时要指定 User-Agent 伪装成浏览器发送请求，以此来蒙骗服务器。直接传递一个字典对象给参数 headers 即可。 r = requests.get(url, headers={'user-agent': 'Mozilla/5.0'}) ","date":"2022-08-22","objectID":"/posts/%E7%88%AC%E8%99%AB2/:0:4","tags":["爬虫","python"],"title":"爬虫(二)HTTP库","uri":"/posts/%E7%88%AC%E8%99%AB2/"},{"categories":["自学笔记"],"content":"请求体 requests 可以非常灵活地构建 POST 请求需要的数据，如果服务器要求发送的数据是表单数据，则可以指定关键字参数 data，如果要求传递 json 格式字符串参数，则可以使用json关键字参数，参数的值都可以字典的形式传过去。 作为表单数据传输给服务器 \u003e\u003e\u003e payload = {'key1': 'value1', 'key2': 'value2'} \u003e\u003e\u003e r = requests.post(\"http://httpbin.org/post\", data=payload) 作为 json 格式的字符串格式传输给服务器 \u003e\u003e\u003e import json \u003e\u003e\u003e url = 'http://httpbin.org/post' \u003e\u003e\u003e payload = {'some': 'data'} \u003e\u003e\u003e r = requests.post(url, json=payload) ","date":"2022-08-22","objectID":"/posts/%E7%88%AC%E8%99%AB2/:0:5","tags":["爬虫","python"],"title":"爬虫(二)HTTP库","uri":"/posts/%E7%88%AC%E8%99%AB2/"},{"categories":["自学笔记"],"content":"响应内容 HTTP返回的响应消息中很重要的一部分内容是响应体，响应体在 requests 中处理非常灵活，与响应体相关的属性有：content、text、json()。 content 是 byte 类型，适合直接将内容保存到文件系统或者传输到网络中 \u003e\u003e\u003e r = requests.get(\"https://pic1.zhimg.com/v2-2e92ebadb4a967829dcd7d05908ccab0_b.jpg\") \u003e\u003e\u003e type(r.content) \u003cclass 'bytes'\u003e # 另存为 test.jpg \u003e\u003e\u003e with open(\"test.jpg\", \"wb\") as f: ... f.write(r.content) text 是 str 类型，比如一个普通的 HTML 页面，需要对文本进一步分析时，使用 text。 \u003e\u003e\u003e r = requests.get(\"https://foofish.net/understand-http.html\") \u003e\u003e\u003e type(r.text) \u003cclass 'str'\u003e \u003e\u003e\u003e re.compile('xxx').findall(r.text) 如果使用第三方开放平台或者API接口爬取数据时，返回的内容是json格式的数据时，那么可以直接使用json()方法返回一个经过json.loads()处理后的对象。 \u003e\u003e\u003e r = requests.get('https://www.v2ex.com/api/topics/hot.json') \u003e\u003e\u003e r.json() [{'id': 352833, 'title': '在长沙，父母同住... ","date":"2022-08-22","objectID":"/posts/%E7%88%AC%E8%99%AB2/:0:6","tags":["爬虫","python"],"title":"爬虫(二)HTTP库","uri":"/posts/%E7%88%AC%E8%99%AB2/"},{"categories":["自学笔记"],"content":"代理设置 当爬虫频繁地对服务器进行抓取内容时，很容易被服务器屏蔽掉，因此要想继续顺利的进行爬取数据，使用代理是明智的选择。如果你想爬取墙外的数据，同样设置代理可以解决问题，requests 完美支持代理。这里我用的是本地 ShadowSocks 的代理，（socks协议的代理要这样安装 pip install requests[socks]） import requests proxies = { 'http': 'socks5://127.0.0.1:1080', 'https': 'socks5://127.0.0.1:1080', } requests.get('https://foofish.net', proxies=proxies, timeout=5) ","date":"2022-08-22","objectID":"/posts/%E7%88%AC%E8%99%AB2/:0:7","tags":["爬虫","python"],"title":"爬虫(二)HTTP库","uri":"/posts/%E7%88%AC%E8%99%AB2/"},{"categories":["自学笔记"],"content":"超时设置 requests 发送请求时，默认请求下线程一直阻塞，直到有响应返回才处理后面的逻辑。如果遇到服务器没有响应的情况时，问题就变得很严重了，它将导致整个应用程序一直处于阻塞状态而没法处理其他请求。 \u003e\u003e\u003e import requests \u003e\u003e\u003e r = requests.get(\"http://www.google.coma\") ...一直阻塞中 正确的方式的是给每个请求显示地指定一个超时时间。 \u003e\u003e\u003e r = requests.get(\"http://www.google.coma\", timeout=5) 5秒后报错 Traceback (most recent call last): socket.timeout: timed out ","date":"2022-08-22","objectID":"/posts/%E7%88%AC%E8%99%AB2/:0:8","tags":["爬虫","python"],"title":"爬虫(二)HTTP库","uri":"/posts/%E7%88%AC%E8%99%AB2/"},{"categories":["自学笔记"],"content":"Session HTTP协议是一中无状态的协议，为了维持客户端与服务器之间的通信状态，使用 Cookie 技术使之保持双方的通信状态。 有些网页是需要登录才能进行爬虫操作的，而登录的原理就是浏览器首次通过用户名密码登录之后，服务器给客户端发送一个随机的Cookie，下次浏览器请求其它页面时，就把刚才的 cookie 随着请求一起发送给服务器，这样服务器就知道该用户已经是登录用户。 import requests # 构建会话 session = requests.Session() #　登录url session.post(login_url, data={username, password}) #　登录后才能访问的url r = session.get(home_url) session.close() 构建一个session会话之后，客户端第一次发起请求登录账户，服务器自动把cookie信息保存在session对象中，发起第二次请求时requests 自动把session中的cookie信息发送给服务器，使之保持通信状态。 ","date":"2022-08-22","objectID":"/posts/%E7%88%AC%E8%99%AB2/:0:9","tags":["爬虫","python"],"title":"爬虫(二)HTTP库","uri":"/posts/%E7%88%AC%E8%99%AB2/"},{"categories":["自学笔记"],"content":"项目实战 最后是一个实战项目，如何用 requests 实现知乎自动登录并给用户发私信，我会在下一篇文章中进行讲解，关注公众号 ‘Python之禅’。 延伸阅读： Python实现知乎自动登录：https://foofish.net/python-auto-login-zhihu.html requests文档：http://docs.python-requests.org/en/master/ 如何阅读 requests 源码： https://www.slideshare.net/onceuponatimeforever/lets-read-code-pythonrequests-library?qid=9f3099df-4c9e-419a-ae62-b601f55b39f3\u0026v=\u0026b=\u0026from_search=3 ","date":"2022-08-22","objectID":"/posts/%E7%88%AC%E8%99%AB2/:0:10","tags":["爬虫","python"],"title":"爬虫(二)HTTP库","uri":"/posts/%E7%88%AC%E8%99%AB2/"},{"categories":["自学笔记"],"content":"转载自python之禅 HTTP协议是什么？ HTTP 协议是互联网应用中，客户端（浏览器）与服务器之间进行数据通信的一种协议。 协议中规定了客户端发送请求的格式，同时也约定了服务端返回的响应结果的格式。 HTTP协议本身无状态，并不记录客户端的历史请求记录。 graph LR 客户端--HTTP请求--\u003e服务器 服务器--HTTP相应--\u003e客户端 HTTP请求 HTTP 请求由3部分组成，分别是请求行、请求首部、请求体，首部和请求体是可选的，并不是每个请求都需要的。 ","date":"2022-08-21","objectID":"/posts/%E7%88%AC%E8%99%AB1/:0:0","tags":["爬虫","python"],"title":"爬虫(一)HTTP协议","uri":"/posts/%E7%88%AC%E8%99%AB1/"},{"categories":["自学笔记"],"content":"请求行 请求行是每个请求必不可少的部分，它由3部分组成，分别是请求方法（method)、请求URL（URI）、HTTP协议版本，以空格隔开。 HTTP协议中最常用的请求方法有：GET、POST、PUT、DELETE。GET 方法用于从服务器获取资源，90%的爬虫都是基于GET请求抓取数据。 请求 URL 是指资源所在服务器的路径地址，比如上图的例子表示客户端想获取 index.html 这个资源，它的路径在服务器 foofish.net 的根目录（/）下面。 ","date":"2022-08-21","objectID":"/posts/%E7%88%AC%E8%99%AB1/:1:0","tags":["爬虫","python"],"title":"爬虫(一)HTTP协议","uri":"/posts/%E7%88%AC%E8%99%AB1/"},{"categories":["自学笔记"],"content":"请求首部 请求行携带的信息有限，以至于客户端还有很多想向服务器要说的事情不得不放在请求首部（Header） 请求首部用于给服务器提供一些额外的信息，比如 User-Agent 用来表明客户端的身份，让服务器知道你是来自浏览器的请求还是爬虫，是来自 Chrome 浏览器还是 FireFox。 HTTP/1.1 规定了47种首部字段类型。HTTP首部字段的格式很像 Python 中的字典类型，由键值对组成，中间用冒号隔开。比如： User-Agent: Mozilla/5.0 因为客户端发送请求时，发送的数据（报文）是由字符串构成的，为了区分请求首部的结尾和请求体的开始，用一个空行来表示，遇到空行时，就表示这是首部的结尾，请求体的开始。 ","date":"2022-08-21","objectID":"/posts/%E7%88%AC%E8%99%AB1/:2:0","tags":["爬虫","python"],"title":"爬虫(一)HTTP协议","uri":"/posts/%E7%88%AC%E8%99%AB1/"},{"categories":["自学笔记"],"content":"请求体 ​ 请求体是客户端提交给服务器的真正内容，比如用户登录时的需要用的用户名和密码，比如文件上传的数据，比如注册用户信息时提交的表单信息。 现在我们用 Python 提供的最原始API socket 模块来模拟向服务器发起一个 HTTP 请求: with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: # 1. 与服务器建立连接 s.connect((\"www.seriot.ch\", 80)) # 2. 构建请求行，请求资源是 index.php request_line = b\"GET /index.php HTTP/1.1\" # 3. 构建请求首部，指定主机名 headers = b\"Host: seriot.ch\" # 4. 用空行标记请求首部的结束位置 blank_line = b\"\\r\\n\" # 请求行、首部、空行这3部分内容用换行符分隔，组成一个请求报文字符串 # 发送给服务器 message = b\"\\r\\n\".join([request_line, headers, blank_line]) s.send(message) # 服务器返回的响应内容稍后进行分析 response = s.recv(1024) print(response) HTTP 响应 服务端接收请求并处理后，返回响应内容给客户端，同样地，响应内容也必须遵循固定的格式浏览器才能正确解析。HTTP 响应也由3部分组成，分别是：响应行、响应首部、响应体，与 HTTP 的请求格式是相对应的。 ","date":"2022-08-21","objectID":"/posts/%E7%88%AC%E8%99%AB1/:3:0","tags":["爬虫","python"],"title":"爬虫(一)HTTP协议","uri":"/posts/%E7%88%AC%E8%99%AB1/"},{"categories":["自学笔记"],"content":"响应行 响应行同样也是3部分组成，由服务端支持的 HTTP 协议版本号、状态码、以及对状态码的简短原因描述组成。 状态码是响应行中很重要的一个字段。通过状态码，客户端可以知道服务器是否正常处理的请求。**如果状态码是200，说明客户端的请求处理成功，如果是500，说明服务器处理请求的时候出现了异常。404 表示请求的资源在服务器找不到。**除此之外，HTTP 协议还很定义了很多其他的状态码，不过它不是本文的讨论范围。 ","date":"2022-08-21","objectID":"/posts/%E7%88%AC%E8%99%AB1/:4:0","tags":["爬虫","python"],"title":"爬虫(一)HTTP协议","uri":"/posts/%E7%88%AC%E8%99%AB1/"},{"categories":["自学笔记"],"content":"响应首部 响应首部和请求首部类似，用于对响应内容的补充，在首部里面可以告知客户端响应体的数据类型是什么,响应内容返回的时间是什么时候，响应体是否压缩了，响应体最后一次修改的时间。 ","date":"2022-08-21","objectID":"/posts/%E7%88%AC%E8%99%AB1/:5:0","tags":["爬虫","python"],"title":"爬虫(一)HTTP协议","uri":"/posts/%E7%88%AC%E8%99%AB1/"},{"categories":["自学笔记"],"content":"响应体 响应体（body）是服务器返回的真正内容，它可以是一个HTML页面，或者是一张图片、一段视频等等。 我们继续沿用前面那个例子来看看服务器返回的响应结果是什么？因为我只接收了前1024个字节，所以有一部分响应内容是看不到的。 b'HTTP/1.1 200 OK\\r\\n Date: Tue, 04 Apr 2017 16:22:35 GMT\\r\\n Server: Apache\\r\\n Expires: Thu, 19 Nov 1981 08:52:00 GMT\\r\\n Set-Cookie: PHPSESSID=66bea0a1f7cb572584745f9ce6984b7e; path=/\\r\\n Transfer-Encoding: chunked\\r\\n Content-Type: text/html; charset=UTF-8\\r\\n\\r\\n118d\\r\\n \u003c!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\"\u003e\\n\\n \u003chtml xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\"\u003e\\n \u003chead\u003e\\n\\t \u003cmeta http-equiv=\"Content-Type\" content=\"text/html;charset=iso-8859-1\" /\u003e \\n\\t \u003cmeta http-equiv=\"content-language\" content=\"en\" /\u003e\\n\\t ... \u003c/html\u003e 从结果来看，它与协议中规范的格式是一样的，第一行是响应行，状态码是200，表明请求成功。第二部分是响应首部信息，由多个首部组成，有服务器返回响应的时间，Cookie信息等等。第三部分就是真正的响应体 HTML 文本。 ","date":"2022-08-21","objectID":"/posts/%E7%88%AC%E8%99%AB1/:6:0","tags":["爬虫","python"],"title":"爬虫(一)HTTP协议","uri":"/posts/%E7%88%AC%E8%99%AB1/"},{"categories":["自学笔记"],"content":"vi的使用 vi分为三种模式： 一般命令模式(command mode) 移动光标 删除字符 复制粘贴 不能编辑文件内容 编辑模式(insert mode) 编辑文件内容 命令行模式(command-line mode) 查找数据 读取、保存、批量替换 退出vi 显示行号 ｜-- i,o(插入),R(替换) -\u003e insert mode command mode \u003c ----- [ESC] ------------ | | -- :,/,? -\u003e command-line mode 编辑模式与一般命令模式不能相互切换！ :wq 编辑文件完毕后，输入以保存并退出vi环境 :wq! 强制写入文件，在拥有足够权限的情况下成立 :q! 强制退出不保存 :! command 暂时退出vi到命令行模式下执行command的显示结果 set nu 显示行号 set nonu 取消行号 ","date":"2022-08-06","objectID":"/posts/vim/:0:1","tags":["Vim","Shell"],"title":"Vim","uri":"/posts/vim/"},{"categories":["日记"],"content":"大一下暑假 ","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:0:0","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"回母校 群贤继起，势大声宏 8.16号 暑假时间所剩无几，我也终于闲下来了 快快乐乐冲母校咯！ 蔡老师可爱可亲一如既往，大家也分明都还是高中生的模样 无论何时何地，我们都能齐聚一堂 This's my capacity, and also my obligation. ","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:1:0","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"驾考 驾考的神终于拿到驾照了，浅浅记录下折磨的驾考生涯： 科二挂了两次，科三挂了两次，全部都是第三次过的。 以前有多痛 现在就有多爽呗 其实，多开几次也好，上路就会更放松自然。毕竟每次练车来回的路上我都有开嘛。 ","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:2:0","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"课程 《概率论》 b站宋浩 简简单单过了一遍，下学期上课笔记的活可以省一点了 时间可以用来刷题去 置信区间和枢轴变量在数学建模的时候看到过，各种分布也稍微了解了点 《复分析》b站崔贵珍 ","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:2:1","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"旅游 云南 5 日游*（7.25～7.30） 大理 洱海 大理古城 双廊古城 丽江 丽江古城 蓝月谷 云杉坪 昆明 江苏泰州社会实践 5 天* （8.1 ～ 8.5） ​ 认识了很多新朋友，星火计划团建 ","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:3:0","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"书籍 6 本 《长安的荔枝》 马伯庸力作！极力推荐～ 官场凶恶、人生无常、苦尽甘来… 《嫌疑犯X的献身》 《嫌疑人X的献身》是日本推理小说 作家东野圭吾 创作的长篇推理小说，也是“伽利略系列”的第三本小说。该作讲述一个数学天才为了帮助一对母女隐藏杀害前夫的罪行，和警方展开了一连串的斗智，制造整个骗局。 该作同时获得 直木奖 和本格推理小说大奖 ，同时摘得“这本小说了不起”、“本格推理小说Top 10”、“周刊文艺推理小说Top 10”三大推理小说排行榜年度总冠军。 天才的两面 《恶意》 小说开始以野野口修第一人称的口吻叙事，由邻居家的猫之死开篇，讲述畅销书作家日高在移居加拿大前一天被人杀死在家中的故事。负责侦破此案的加贺恭一郎很快就根据现场发现的烟头以及同为作家的死者好友野野口修的口供确定野野口修就是凶手，然而加贺发现野野口修的口供有几处故意疏漏的地方而且野野口修始终不肯透露作案动机，只求速死。通过野野口修留下的细微线索，加贺发现原来野野口修才是真正的“受害者”——日高发现好友与妻子初美有暧昧关系并以此为要挟强迫野野口修成为自己的影子写手，从而成为畅销书作家。而此时野野口修身患不治之症，他忍辱负重为了维护已经过世的初美的名誉隐瞒了杀害日高的动机。可事情绝没有那么简单 。 无限放大的恶意，嫉妒是人类的原罪 《24个比利》 《24个比利》是美国作家丹尼尔·凯斯 创作的长篇小说，是一部多重人格分裂 纪实的作品。 该书讲述了一个真实的故事，威廉·斯坦利·米利根（比利），是美国史上第一位犯下重罪，结果却获判无罪的嫌犯，因为他是一位多重人格分裂者。比利的多重人格达24个之多，他体内的人格可以互相交谈、下棋，互相控制对方的行为，所以比利接受治疗之前的生活是极其混乱的。 一个罪犯，一个人，一个天才 《女性主义有什么用？》 这是一本写给女人，也写给男人的女性主义入门读物。 40个问题直击女性生活痛点，从情感到工作，从个人到政治，全面涵盖性别议题的方方面面。 120位挑战父权制的女性主义先驱，给你理解的温暖，也给你理论的武装。 本书能令你觉醒，令你真正明白何为女性。 看完后把书当镜子照照，真觉得自己有点大男子主义，是个 善意的父权者 《半小时漫画世界名著》 用漫画的形式快速过名著！ 但丁的《神曲》好称欧洲《西游记》 《悲惨世界》原来有前后两个版本 “做那个敢于挑战风车的傻子”原来是真傻 《鲁滨逊漂流记》和《格列佛游记》原来是同一时代英国的“死对头” … 名著尽是社会现实赤裸裸的写照 ","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:4:0","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"纪录片 《七个世界，一个星球》 《七个世界 一个星球》(Seven Worlds, One Planet) 由BBC Studios自然历史部摄制，BBC美国台、企鹅影视、德国电视二台 、法国电视台 和中央广播电视总台 央视纪录频道联合制作呈现，并在英国时间2019年10月27日晚于英国BBC One频道震撼上线，中央广播电视总台央视纪录频道和腾讯视频于2019年10月28日21时进行首播 ","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:5:0","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"电影 16 部 《彗星来的那一夜》 《彗星来的那一夜》是由詹姆斯·沃德·布柯特自编自导的第一部长片，由艾米丽·芭尔多尼、莫瑞·史特林、雨果·阿姆斯特朗、伊丽莎白·格瑞斯、亚历克斯·马努吉安、劳伦·马赫、尼古拉斯·布兰登、劳伦·斯卡法莉娅主演。该电影于2013年9月19日在美国奇幻电影节（位于奥斯汀）首映。该片讲述了在一场大停电之后，一起聚餐的八个朋友的人际关系、甚至世界秩序都有了惊人的改变的故事。 吹爆女主的颜！ 《网络谜踪》 《网络谜踪》是由阿尼什·查甘蒂执导，约翰·赵、黛博拉·梅辛、米切尔·拉主演的悬疑剧情电影。该片以电脑桌面为载体，讲述了16岁少女突然人间蒸发，父亲通过破解女儿社交网站密码继而展开追查的故事。该片于2018年8月24日在北美上映，后于2018年12月14日在中国内地上映。 《返老还童》（本杰明·巴顿奇事） 《本杰明·巴顿奇事》是由美国华纳兄弟、派拉蒙影业联合出品的159分钟奇幻爱情影片。该片由大卫·芬奇执导，布拉德·皮特、凯特·布兰切特、塔拉吉·P·汉森等主演，于2008年12月25日在美国上映。该片改编自弗朗西斯·斯科特·基·菲茨杰拉德所著同名小说，讲述了一出生便拥有80岁老人形象的本杰明·巴顿，随着岁月的推移逐渐变得年轻，最终回到婴儿形态，并在苍老的恋人黛茜怀中离世的奇异故事。 “Never too late to start over again!\" 《阿凡达》 《阿凡达》是由二十世纪福克斯电影公司 出品，詹姆斯·卡梅隆 执导，萨姆·沃辛顿 、佐伊·索尔达娜 、西格妮·韦弗 领衔主演，史蒂芬·朗 、米歇尔·罗德里格兹 、吉奥瓦尼·瑞比西 、乔·大卫·摩尔 等人主演的科幻电影。 该片讲述了在未来，人类飞到遥远的星球潘多拉 开采资源，受伤后以轮椅代步的前海军杰克，自愿接受实验并以他的阿凡达来到潘多拉。然而，在结识了当地纳美族人公主涅提妮之后，杰克在一场人类与潘多拉军民的战争中陷入两难的故事。 《神探大战》 《神探大战》是由韦家辉 执导，刘青云 、蔡卓妍 、林峯 、李若彤、谭凯、陈家乐、汤怡、何珮瑜等主演的犯罪动作悬疑电影 ，于2022年7月8日在中国大陆上映 。 该片讲述了自封“神探”的凶手与真正的“神探”鬼才之间展开的斗智斗勇的港式七宗罪故事 。 “与怪物战斗，当心自己变成怪物！” 《独行月球》 《独行月球》是由张吃鱼执导，沈腾、马丽主演的科幻喜剧片，改编自韩国漫画家赵石创作的同名漫画。该片讲述了人类为抵御小行星的撞击，拯救地球，在月球部署了月盾计划。陨石提前来袭，全员紧急撤离时，维修工独孤月因为意外，错过了领队马蓝星的撤离通知，一个人落在了月球。不料月盾计划失败，独孤月成为了 “宇宙最后的人类”，开始了他在月球上破罐子破摔的生活。该片于 2022年7月29日在中国大陆上映。 《十二宫》 《十二宫杀手》是由华纳兄弟公司于2007年3月2日推出的悬疑片，由大卫·芬奇执导，杰克·吉伦哈尔、马克·鲁弗洛、小罗伯特·唐尼等领衔主演。影片讲述了旧金山出现了一个自称“十二宫”的杀人狂，杀人后，向媒体寄一封信，留下密码、线索，连环杀人案件引起了《旧金山纪事报》的记者注意，他们在警察的帮助下，开始调查这一系列的连环凶杀案件。 《致命魔术》 You don't really want to work it out. You want to be fooled. ​ 19世纪末，人们对科学文明还不是认识得太过清楚，于是，安吉尔（休•杰克曼Hugh Jackman饰）和伯登（克里斯蒂安•贝尔Christian Bale饰）的魔术，成为了伦敦城内的神奇人物。安吉尔出身贵族，魔术手段华丽丰富，是富人圈子里的表演常客。而伯登即使出身平平，争强好胜的心智和充满创造力的魔术技巧，却也令他有了名气。两人自小本是要好的伙伴，然而，现在魔术界二人各有领地，并且都有野心想成为音乐大厅里的顶级魔术师，一番明争暗斗如箭在弦上。 伯登掌握了精彩的分身术，叫座又叫好。而安吉尔见情势不妙，搬来科学家助阵——他发明的交流电有无穷魔力，保证让观众目瞪口呆。二人出招接招，一来一往，争斗在剧烈升级，友谊和道德都被抛诸脑后，一场血案在悄悄酝酿。 《工作女郎》 在玩具公司上班的行销高手白宝熙，是个不折不扣的工作狂，整日埋首工作的她，不但没时间照顾女儿，更遑论与老公做爱做的事了。然而，在一次关键的提案会议上，宝熙竟然错把邻居南希的情趣用品带到会议上发表。恼羞成怒的宝熙愤而前往南希经营的情趣用品店兴师问罪，却没想到意外开启了宝熙从未体验过的情欲大门。南希让宝熙开始享受到身为女人的美好和快乐，更进一步让宝熙兴起进军情趣用品界的念头。 很好的成人家庭伦理剧 ! 《蜘蛛侠：英雄无归》 《蜘蛛侠：英雄无归》是由哥伦比亚影业公司 、漫威影业 联合出品，乔·沃茨 执导，汤姆·赫兰德 、赞达亚·科尔曼 、本尼迪克特·康伯巴奇 领衔主演，雅各·巴塔伦 、玛丽莎·托梅 、托比·马奎尔 、安德鲁·加菲尔德 、威廉·达福 、阿尔弗雷德·莫里纳 、杰米·福克斯 等主演的动作科幻电影，该片于2021年12月17日在北美上映，于2022年3月22日数字上线，并定于4月12日发行4K、蓝光、DVD。 该片承接《蜘蛛侠：英雄远征 》，讲述了蜘蛛侠的生活因曝光身份而陷入混乱，甚至遭到了全世界民众的愤怒抗议，为改变这一切，蜘蛛侠前往寻求奇异博士的帮助，没料想，多元宇宙打开后更大的危机也随之而来的故事。 三蛛齐聚！ With great power comes great responsibility. 《1942》 《一九四二》是由华谊兄弟传媒集团 和重庆电影集团有限公司 联合出品，冯小刚 执导，张国立 、陈道明 、李雪健 、张涵予 等人主演的灾难、历史、剧情、战争电影 ，于2011年10月19日正式开机，2012年11月29日在中国内地上映。 该片改编自刘震云 的小说《温故一九四二》，以1942年河南大旱，千百万民众离乡背井、外出逃荒的历史事件为背景，分两条线索展开叙述：一条是逃荒路上的民众，主要以老东家范殿元和佃户瞎鹿两个家庭为核心；另一条是国民党政府，他们的冷漠和腐败、他们对人民的蔑视推动和加深了这场灾难。 2013年，该片获得了第32届香港电影金像奖 最佳两岸华语电影、第三届北京国际电影节 天坛奖最佳影片等奖项 令人心情沉重的历史写实电影：日军的狠毒，蒋政府的腐败以及人民身临地狱的痛楚 Dead is good, no more suffering. 愈发珍惜和平的当下，愈发尊重革命烈士们 《环太平洋》 第一次在电影院看的时候果然还是太小了 二刷才发现，男主的脸好像查理·普斯，女主Mako很可爱，Marshall真的帅 2013年，环太平洋地区海底深处出现了一个平行宇宙“突破点”，随后，一个巨大无比的巨兽生物从“海洋”中崛起，第一只巨兽首先摧毁了旧金山以及周围所有的海岸城市，而人类大部分企图阻止巨兽的军事行动全部以失败告终，但巨兽也最终在人类的殊死抵抗下死于核弹中，但旧金山方圆百里变成了荒芜之地。事件结束后，人类为巨兽起了“怪兽”（Kaiju）的名字，但原本认为结束的怪兽袭击却接二连三而出现在各个环太平洋城市。人类在无计可施之下发明了他们自己创造的“怪兽”：“机甲猎人”（Jaeger），利用巨大机械士兵来对抗怪兽大军，由两名脑部神经网络互相串连*（浮动神经元连结）*的操纵者同步操作战斗机械士兵，利用仪器来检测怪兽的级别与代号。自从有了机甲猎人，人类开始获得胜利，甚至将怪兽袭击变成宣传活动与仪式，但人类却并不知道更加恐怖的威胁即将来临，而“怪兽战役”也就此打响。 2020年2月28日，代号“镰刀头”的三级怪兽出现在阿拉斯加海域，贝克特兄弟罗利.贝克特*（**查理·汉纳姆 **饰）*和杨希.贝克特驾驶危险流浪者迎击怪兽，战斗中哥哥杨希不幸牺牲，罗利独自一人驾驶几乎报废的危险流浪者返回。 5年以后，由于怪兽的战斗力不断的增强，PDCC决定召回三代机甲危险流浪者，同时找回失踪5年*（实际上是跟着建防御怪兽围墙的工程队去了）的罗利.贝克特担任驾驶，经过选拔，罗利的新搭档是森麻子，第一次同步试验中森麻子（**菊地凛子 **饰）*神经元连结失败，险些摧毁基地。 紧接着四级怪兽“尾立鼠”和“棱背龟”登陆香港，激战中国机甲“暴风赤红”，俄罗斯机甲”切尔诺阿尔法“不幸牺牲，五代机甲”尤里卡突袭者“受到EMP攻击瘫痪，危险流浪者临时出击，逐一击败了入侵的怪兽。这一战，除了尤里卡突袭者、危险流浪者以外的机甲全部阵亡。 香港一战以后，PPDC决定立马实施缺口摧毁作战计划。在深海中流浪者巧妙击杀四级怪兽”迅龙‘，尤里卡突袭者自爆与四级怪兽“憎恶”同归于尽，同时重伤五级怪兽“毒妇”，最后流浪者穿越缺口，通过自毁成功炸毁虫洞。 《失控玩家》 《自由城》是一款爆款动作冒险类游戏，玩家可以在游戏里烧杀抢掠，体验当法外之徒的快感。游戏中还有不少NPC，他们日复一日配合玩家做出各种反应：被抢劫、被打倒在地、被迫崇拜玩家等等。盖*（**瑞安·雷诺兹 饰）是《自由城》中的一名NPC银行出纳员，拥有一衣柜一模一样的蓝衬衣，过着一成不变的生活。有一天，由于某个契机他突然决定打破这种循环，掌握人生的主动权，出人意料地抢走了玩家的VR眼镜，然后震惊地看到玩家眼中的世界：四处漂浮着的虚拟图标，提示着任务、血量和弹药量等，地上还有装备、医疗包。盖的世界观彻底被震撼了，而他也误打误撞地体验起了当玩家的感觉。然而，善良的他不愿意杀人放火，于是开始了做好事不留名的奇特游戏生涯。他以一己之力影响着游戏的风气，成为传奇人物，现实中的玩家纷纷猜测他的真实身份。但是，好景不长，《自由","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:6:0","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"电视剧 《睡魔》（《The Sandman》) 人类的谎言到底能有几层？人以谎言为食 《睡魔》*（The Sandman）*是由杰米·奇尔兹、安德烈斯·拜斯 、路易丝·胡珀、麦克·巴克 、科拉莉·法尔雅 导演，由尼尔·盖曼 、大卫·S·高耶 编剧，由艾伦·海博格 担任编剧与制作人，由汤姆·斯图里奇 主演的漫画改编真人剧集。 该剧改编自尼尔·盖曼创作的DC同名漫画 ，讲述了威力强大的墨菲斯于他存在的期间，在宇宙和人间铸下种种错误；身为梦境的至尊王者的他，却在挣脱枷锁，重获自由之后，方才明了麻烦才刚要开始的故事 ，于2022年8月5日在Netflix上线 。 ","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:7:0","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"游戏 《杀戮尖塔》 《杀戮尖塔》是Mega Crit Games制作发行的一款卡牌类Roguelike游戏，于2017年11月15日发行测试版，2019年1月23日发行正式版。 该作将传统的卡牌构建及Roguelike玩法结合在一起。玩家需要在游戏中打造出独一无二的牌组，遭遇无数离奇的生物，发现威力强大的遗物，挑战并屠戮高塔。 《王者荣耀》 终于上王者啦！ 8.11 这个游戏真的是让人，又爱又恨吧。每次匹配总是会有orphan，搞人心态，但是终归也算上来了。常说人不可能完全理解其他人的想法，我不懂那些被队友骂一句、被队友批评一句、被抢了一个buff、被蹭了一波线就心态大崩6分投的的人是怎么想的。他们不懂游戏的乐趣在于过程，在于绝境中可以创造奇迹的喜悦。游戏是逃避现实生活用的吗？我看不然，心态失衡者在哪里都找不到归宿，这个世界不会惯着任何人。 ","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:8:0","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"番剧5部 《间谍过家家》 每个人都有不可告人的一面—— 这是一个世界各国均暗地里进行激烈情报战的时代。东国（Ostania）与西国（Westalis）的冷战状态已经持续数十年。 “黄昏”是西国情报局东国对策科“WISE”的一名优秀间谍。为调查威胁东西两国和平的人物——东国国家统一党总裁多诺万·德斯蒙，上级给予了他一个绝密任务。 任务名为：“枭”（Strix）行动。 内容是“一周之内组建家庭，潜入德斯蒙儿子就读的名门学校的联谊会”。 于是“黄昏”扮演成精神科医生劳埃德·福杰，开始组建家庭。 然而，他找来的女儿阿尼亚是个能读心的超能力者，妻子约尔是个杀手。三人利害关系一致，便互相隐瞒身份，开始了共同生活。 世界的和平，就掌握在这意外不断的临时一家人手中 WakuWaku! 《辉夜大小姐》 会长A上去了！ 他AAAA上去了！！！！ 《夏日重现》 《夏日重现》是田中靖规 于2017年10月23日～2021年2月1日在漫画平台“少年 Jump+ ”上连载的漫画，单行本全13卷。 单行本由集英社 出版，繁体中文版由东立出版社 发行，简体中文电子版由哔哩哔哩漫画 发布。 故事的舞台设置在和歌山县 和歌山市 日都岛，讲述了主角网代慎平因青梅竹马小舟潮的死亡而返回故乡参加葬礼，之后所经历的一系列离奇故事。 小舟潮死了。 《异世界叔叔》 敬文的舅舅 17 岁时被卡车撞上，从此陷入昏迷。17 年后他奇迹苏醒，敬文去医院探望舅舅，却只见他独自胡言乱语，宣称自己是从“大巴哈马鲁”异世界归来。 ……舅舅的脑袋显然是坏掉了。 敬文不知所措，但舅舅却施展法术，证明自己的确去过另一个时空。敬文决定将舅舅接回公寓同住，一方面是没有其他亲戚肯帮忙，一方式是他觉得可以利用舅舅的能力来赚钱。 两人住在一起后，敬文一步步得知舅舅奇幻精彩的冒险，以及他对 SEGA 电玩的痴狂。然而，有时听着舅舅孤单又残酷的遭遇，让敬文在开心的同时，也为他感到难过。 。。。叔叔壮烈的异世界生活和侄子敬文的新感觉异世界喜剧拉开了帷幕 《石之海》 空条徐伦yyds！ 怎么感觉普奇神父里的Dio讲的话真的好有哲理 ","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:9:0","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"漫画 《电锯人》 看完感觉结尾有点戛然而止了。真纪真小姐的形象让人很难定义，power和淀治的感觉更配啊… 期待一下不可能在国内上映的动漫版，感觉漫画看过来对夸张战斗场景的渲染冲击力还不够 ","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:9:1","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["日记"],"content":"准备四级","date":"2022-08-06","objectID":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/:10:0","tags":[],"title":"暑期生活","uri":"/posts/%E6%9A%91%E6%9C%9F%E6%97%A5%E5%BF%97/"},{"categories":["自学笔记"],"content":"np.concatenate() import numpy as np x1 = np.random.normal(1,1,(5,4)) x2 = np.random.normal(1,1,(3,4)) print(x1) print(x1.shape) print(x2) print(x2.shape) con = np.concatenate([x1,x2],axis=0) print(con) print(con.shape) 输出结果为： [[ 2.22806658 0.15277615 2.21245262 1.63831116] [ 1.30131232 -1.09226289 -0.65959394 1.16066688] [ 1.52737722 0.84587186 1.53041503 0.4584277 ] [ 1.56096219 1.29506244 3.08048523 2.06008988] [ 1.79964236 0.95087117 1.30845477 -0.2644263 ]] (5, 4) [[0.89383392 1.49502055 2.90571116 1.71943997] [1.44451535 1.87838383 1.4763242 0.82597179] [0.72629108 1.42406398 1.35519112 0.58121617]] (3, 4) [[ 2.22806658 0.15277615 2.21245262 1.63831116] [ 1.30131232 -1.09226289 -0.65959394 1.16066688] [ 1.52737722 0.84587186 1.53041503 0.4584277 ] [ 1.56096219 1.29506244 3.08048523 2.06008988] [ 1.79964236 0.95087117 1.30845477 -0.2644263 ] [ 0.89383392 1.49502055 2.90571116 1.71943997] [ 1.44451535 1.87838383 1.4763242 0.82597179] [ 0.72629108 1.42406398 1.35519112 0.58121617]] (8, 4) axis参数为指定按照哪个维度进行拼接，上述的例子中x1为[5,4] x2为[3,4]，设置axis=0则代表着按照第一维度进行拼接，拼接后的尺寸为[8,4]除了第一维度的尺寸发生变化，其他维度不变，同时也说明，必须保证其他维度的尺寸是能对的上的，如果x1为[5,4],x2为[5,3],在这里如果还设置axis=0的话，则会报错，因为x1和x2的第二维度尺寸不相等，无法拼接。 np.transpose() x = np.arange(12).reshape((2,3,2)) 使用 numpy.transpose ()进行变换，其实就是交换了坐标轴，如：x.transpose(1, 2, 0)，其实就是将x第二维度挪到第一维上，第三维移到第二维上，原本的第一维移动到第三维上，最后的shape为：(3，2，2) ","date":"2022-07-21","objectID":"/posts/numpy/:0:0","tags":["library","python"],"title":"NumPy","uri":"/posts/numpy/"},{"categories":["自学笔记"],"content":"pd.cut() :将某一列按照几个标准分类 housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]) category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on pd.DataFrame.drop(): 删除某一行或某一列 pandas.DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') -- labels 单个标签或者标签列表 -- axis=0 默认 删除index -- axis=1 指定删除列 -- inplace=True 修改原数据, 否则会创造copy -- level 针对多重索引 指定级别 -- index 指定索引 -- columns 指定列名 详见：http://www.manongjc.com/detail/26-eoipnsvfounrfht.html ","date":"2022-07-18","objectID":"/posts/pandas/:0:0","tags":["library","python"],"title":"Pandas","uri":"/posts/pandas/"},{"categories":["自学笔记"],"content":"随机抽样 从数据集中选取随机20%作为测试集 from sklearn.model_selection import train_test_split train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) random_state的作用是设置随机数种子 ","date":"2022-07-18","objectID":"/posts/scikit-learn/:0:1","tags":["library","python"],"title":"Scikit Learn","uri":"/posts/scikit-learn/"},{"categories":["自学笔记"],"content":"分层抽样 from sklearn.model_selection import StratifiedShuffleSplit ## 分类 housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]) ## 分层抽样 split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[\"income_cat\"]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] ","date":"2022-07-18","objectID":"/posts/scikit-learn/:0:2","tags":["library","python"],"title":"Scikit Learn","uri":"/posts/scikit-learn/"},{"categories":["自学笔记"],"content":"End to End project Start– 数据处理部分 ","date":"2022-07-17","objectID":"/posts/ml_note2/:0:0","tags":["Machine Learning"],"title":"ML Learning 3","uri":"/posts/ml_note2/"},{"categories":["自学笔记"],"content":"Performance Measure RSME (Root Mean Square Error) $$ \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(\\bold x^{(i)} - y^{(i)})^2} $$ m : number of instances $x^{(i)}$: a vector of all the features, for example: $$ \\begin{pmatrix} longitude = -118.29\\degree \\newline latitude = 33.91 \\degree \\newline inhabitants = 1416 \\newline median income = 38372 \\newline \\end{pmatrix} $$ h(x)： h is your system’s prediction function, also called a hypothesis. When your system is given an instance’s feature vector x(i), it outputs a predicted value ŷ(i) = $h(x^{(i)})$ for that instance (ŷ is pronounced “y-hat”). MAE(Mean Absolute Error / Average Absolute Deviation) $$ \\frac{1}{m}\\sum_{i =1}^{m}|h(x^{(i)} - y^{(i)})| $$ ​ Computing the root of a sum of squares (RMSE) corresponds to the Euclidean norm: it is the notion of distance you are familiar with. It is also called the $l_{2}$ norm, noted $∥ · ∥_{2}$ (or just $∥ · ∥$). ​ Computing the sum of absolutes (MAE) corresponds to the $l_{1}$ norm, noted $∥ · ∥_{1}$. It is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks. ​ More generally, the $l_{k}$ norm of a vector v containing n elements is defined as $$ ||{v}||_{k}=(|v_{0}|^{k} + |v_{1}|^{k} + \\cdots + |v_{n}|^{k})^{\\frac{1}{k}} $$ ​ $l_{0}$ = the number of elements in the vector ​ $l_{\\infty}$= the maximum absolute value in the vector. Notice for norms ​ RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred. ","date":"2022-07-17","objectID":"/posts/ml_note2/:0:1","tags":["Machine Learning"],"title":"ML Learning 3","uri":"/posts/ml_note2/"},{"categories":["自学笔记"],"content":"Create Test Set 抽样 随机抽样 从数据集中选取随机20%作为测试集 from sklearn.model_selection import train_test_split train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) random_state的作用是设置随机数种子 分层抽样 from sklearn.model_selection import StratifiedShuffleSplit ## 分类 housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]) ## 分层抽样 split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[\"income_cat\"]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] 数据复原 for set_ in (strat_train_set, strat_test_set): set_.drop(\"income_cat\", axis=1, inplace=True) pd.DataFrame.drop(): 删除某一行或某一列 pandas.DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') -- labels 单个标签或者标签列表 -- axis=0 默认 删除index -- axis=1 指定删除列 -- inplace=True 修改原数据 -- level 针对多重索引 指定级别 -- index 指定索引 -- columns 指定列名 ","date":"2022-07-17","objectID":"/posts/ml_note2/:0:2","tags":["Machine Learning"],"title":"ML Learning 3","uri":"/posts/ml_note2/"},{"categories":["自学笔记"],"content":"Discover and Visualize the Data to Gain Insights Visualizing Geographical Data 小技巧：在使用散点图的时候，把alpha值设置为1可以更好地看出密集程度 housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1) 用cmap结合颜色进一步美化 housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, label=\"population\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True, ) plt.legend() ​ The radius of each circle represents the district’s population (option s), and the color represents the price (option c). We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices) Looking for Correlations 1.Standard correlation coefficient (Pearson’s r, 皮尔逊相关系数) warning! ​ The correlation coefficient only measures linear correlations (“if x goes up, then y generally goes up/down”). It may completely miss out on nonlinear relationships (e.g., “if x is close to zero then y generally goes up”). ​ Besides, the correlation coefficient has nothing to do with the slope. For example, your height in inches has a correla‐ tion coefficient of 1 with your height in feet or in nanometers. 2.use Pandas’ scatter_matrix function from pandas.plotting import scatter_matrix attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"] scatter_matrix(housing[attributes], figsize=(12, 8)) ​ This function plots every numerical attribute against every other numerical attribute. If there are now 11 numerical attributes, you would get 112 = 121 plots. ","date":"2022-07-17","objectID":"/posts/ml_note2/:0:3","tags":["Machine Learning"],"title":"ML Learning 3","uri":"/posts/ml_note2/"},{"categories":["自学笔记"],"content":"Experimenting with Attribute Combinations ​ Before feeding the data to a Machine Learning algorithm, and you found interesting correlations between attributes, in particular with the target attribute. You also noticed that some attributes have a tail-heavy distribution, so you may want to trans‐ form them (e.g., by computing their logarithm). ​ For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. Let’s create these new attributes: housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"] housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"] housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"] ","date":"2022-07-17","objectID":"/posts/ml_note2/:0:4","tags":["Machine Learning"],"title":"ML Learning 3","uri":"/posts/ml_note2/"},{"categories":["自学笔记"],"content":"Prepare the Data for Machine Learning Algorithms ​ First let’s revert to a clean training set (by copying strat_train_set once again), and let’s separate the predictors and the labels since we don’t necessarily want to apply the same transformations to the predictors and the target values (note that drop() creates a copy of the data and does not affect strat_train_set): housing = strat_train_set.drop(\"median_house_value\", axis=1) housing_labels = strat_train_set[\"median_house_value\"].copy() Data Cleaning ​ Most Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. You noticed earlier that the total_bedrooms attribute has some missing values, so let’s fix this. You have three options: ​ • Get rid of the corresponding districts. ​ • Get rid of the whole attribute. ​ • Set the values to some value (zero, the mean, the median, etc.). ​ ​ You can accomplish these easily using DataFrame’s dropna(), drop(), and fillna() methods: housing.dropna(subset=[\"total_bedrooms\"]) # option 1 housing.drop(\"total_bedrooms\", axis=1) # option 2 median = housing[\"total_bedrooms\"].median() # option 3 housing[\"total_bedrooms\"].fillna(median, inplace=True) ​ Also, we could use Scikit-Learn. SimpleImputer from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=\"median\") ​ Since the median can only be computed on numerical attributes, we need to create a copy of the data without the text attribute ocean_proximity: housing_num = housing.drop(\"ocean_proximity\", axis=1) ​ Then use the fit() method: imputer.fit(housing_num) ​ The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable. Only the total_bedrooms attribute had missing values, but we cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes: ​ Now you can use this “trained” imputer to transform the training set by replacing missing values by the learned medians: X = imputer.transform(housing_num) ​ The result is a plain NumPy array containing the transformed features. If you want to put it back into a Pandas DataFrame, it’s simple: housing_tr = pd.DataFrame(X, columns=housing_num.columns) ","date":"2022-07-17","objectID":"/posts/ml_note2/:0:5","tags":["Machine Learning"],"title":"ML Learning 3","uri":"/posts/ml_note2/"},{"categories":["自学笔记"],"content":"Handling Text and Categorical Attributes ​ Earlier we left out the categorical attribute ocean_proximity because it is a text attribute so we cannot compute its median. ​ Most Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐ vert these categories from text to numbers. For this, we can use Scikit-Learn’s OrdinalEncoder class:（数字化文字类别） from sklearn.preprocessing import OrdinalEncoder \u003e\u003e\u003e housing_cat = housing[[\"ocean_proximity\"]] \u003e\u003e\u003e ordinal_encoder = OrdinalEncoder() \u003e\u003e\u003e housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) \u003e\u003e\u003e housing_cat_encoded[:10] array([[0.], [0.], [4.], [1.], [0.], [1.], [0.], [1.], [0.], [0.]]) ​ You can get the list of categories using the categories_ instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute): \u003e\u003e\u003e ordinal_encoder.categories_ [array(['\u003c1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)] ​ 一个要解决的问题也随之而来：ML algorithms will assume that two nearby values are more similar than two distant values. 解决办法：one-hot code 具体可参考这个，https://www.cnblogs.com/lianyingteng/p/7755545.html ​ To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “\u003c1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. ​ This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEn coder class to convert categorical values into one-hot vectors: \u003e\u003e\u003e from sklearn.preprocessing import OneHotEncoder \u003e\u003e\u003e cat_encoder = OneHotEncoder() \u003e\u003e\u003e housing_cat_1hot = cat_encoder.fit_transform(housing_cat) \u003e\u003e\u003e housing_cat_1hot \u003c16512x5 sparse matrix of type '\u003cclass 'numpy.float64'\u003e' with 16512 stored elements in Compressed Sparse Row format\u003e Notice ​ 为什么是sparse matrix而不是NumPy Array呢？ ​ This is very useful when you have categorical attributes with thousands of categories. After one- hot encoding we get a matrix with thousands of columns, and the matrix is full of zeros except for a single 1 per row. Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the non‐zero elements. You can use it mostly like a normal 2D array,21 but if you really want to convert it to a (dense) NumPy array, just call the toarray() method: \u003e\u003e\u003e housing_cat_1hot.toarray() array([[1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 0., 1.], ..., [0., 1., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 1., 0.]]) ","date":"2022-07-17","objectID":"/posts/ml_note2/:0:6","tags":["Machine Learning"],"title":"ML Learning 3","uri":"/posts/ml_note2/"},{"categories":["自学笔记"],"content":"Custom Transformers ​ Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes. ​ We should create a class and implement three methods: fit() transform() fit_transform() ​ You can get the last one for free by simply adding TransformerMixin as a base class. Also, if you add BaseEstima tor as a base class (and avoid *args and **kargs in your constructor) you will get two extra methods (get_params() and set_params()) that will be useful for auto‐matic hyperparameter tuning. For example, here is a small transformer class that adds the combined attributes we discussed earlier: ​ from sklearn.base import BaseEstimator, TransformerMixin rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household] attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False) housing_extra_attribs = attr_adder.transform(housing.values) ​ np.c_ 是用来横向连接矩阵的 ​ 这里的hyperparameter是add_bedrooms_per_room ​ This hyperparameter will allow you to easily find out whether adding this attribute helps the Machine Learning algorithms or not. More generally, you can add a hyperparameter to gate any data preparation step that you are not 100% sure about. The more you automate these data preparation steps, the more combinations you can automatically try out, making it much more likely that you will find a great combination (and saving you a lot of time). ","date":"2022-07-17","objectID":"/posts/ml_note2/:0:7","tags":["Machine Learning"],"title":"ML Learning 3","uri":"/posts/ml_note2/"},{"categories":["自学笔记"],"content":"Feature Scaling ​ Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. 调整各数据的scale，以达到合适的比例。 ​ Note that scaling the target values is generally not required. min-max scaling (normalization) ​ values are shifted and rescaled so that they end up ranging from 0 to 1. ​ Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don’t want 0–1 for some reason. standardization ​ first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. ​ Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). ​ However, standardization is much less affected by outliers. For example, suppose a district had a median income equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0–15 down to 0–0.15, whereas standardization would not be much affected. ​ Scikit-Learn provides a transformer called StandardScaler for standardization. Warning ​ As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data). ","date":"2022-07-17","objectID":"/posts/ml_note2/:0:8","tags":["Machine Learning"],"title":"ML Learning 3","uri":"/posts/ml_note2/"},{"categories":["自学笔记"],"content":"Transformation Pipelines ​ As you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations. Here is a small pipeline for the numerical attributes: from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy=\"median\")), ('attribs_adder', CombinedAttributesAdder()), ('std_scaler', StandardScaler()), ]) # name/estimator pairs. names只要unique且不能含有__，estimator必须是transformer(有fit_transform()方法) housing_num_tr = num_pipeline.fit_transform(housing_num) 完整的pipeline如下： from sklearn.compose import ColumnTransformer num_attribs = list(housing_num) cat_attribs = [\"ocean_proximity\"] full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), (\"cat\", OneHotEncoder(), cat_attribs), ]) housing_prepared = full_pipeline.fit_transform(housing) ​ The constructor requires a list of tuples, where each tuple contains a name, a transformer and a list of names (or indices) of columns that the transformer should be applied to. ​ Note that the OneHotEncoder returns a sparse matrix, while the num_pipeline returns a dense matrix. When there is such a mix of sparse and dense matrices, the ColumnTransformerestimates the density of the final matrix (i.e., the ratio of non-zero cells), and it returns a sparse matrix if the density is lower than a given threshold (by default, sparse_threshold=0.3). Tip ​ Instead of a transformer, you can specify the string “drop” if you want the columns to be dropped. Or you can specify “pass through” if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to “passthrough”) if you want these columns to be handled differently. ","date":"2022-07-17","objectID":"/posts/ml_note2/:0:9","tags":["Machine Learning"],"title":"ML Learning 3","uri":"/posts/ml_note2/"},{"categories":["自学笔记"],"content":"模型选择与调整 ","date":"2022-07-17","objectID":"/posts/ml_note3/:0:0","tags":["Machine Learning"],"title":"ML Learning 4","uri":"/posts/ml_note3/"},{"categories":["自学笔记"],"content":"Select and Train a Model Already Done: framed the problem got the data and explored it sampled a training set and a test set wrote transformation pipelines to clean up and prepare your data for Machine Learning algorithms automatically. Training and Evaluating on the Training Set 测试线性回归模型 from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) \u003e\u003e\u003e some_data = housing.iloc[:5] \u003e\u003e\u003e some_labels = housing_labels.iloc[:5] \u003e\u003e\u003e some_data_prepared = full_pipeline.transform(some_data) \u003e\u003e\u003e print(\"Predictions:\", lin_reg.predict(some_data_prepared)) Predictions: [ 210644.6045 317768.8069 210956.4333 59218.9888 189747.5584] \u003e\u003e\u003e print(\"Labels:\", list(some_labels)) Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0] Let’s measure this regression model’s RMSE on the whole training set using Scikit-Learn’s mean_squared_error function: \u003e\u003e\u003e from sklearn.metrics import mean_squared_error \u003e\u003e\u003e housing_predictions = lin_reg.predict(housing_prepared) \u003e\u003e\u003e lin_mse = mean_squared_error(housing_labels, housing_predictions) \u003e\u003e\u003e lin_rmse = np.sqrt(lin_mse) \u003e\u003e\u003e lin_rmse 68628.19819848922 这个偏差显然太大，主要原因是underfitting。主要可以从下面三个角度优化： 选择更强大的模型 给予算法更多特征值 减少加在模型上的束缚 如果用DecisionTreeRegressor from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor() tree_reg.fit(housing_prepared, housing_labels) \u003e\u003e\u003e housing_predictions = tree_reg.predict(housing_prepared) \u003e\u003e\u003e tree_mse = mean_squared_error(housing_labels, housing_predictions) \u003e\u003e\u003e tree_rmse = np.sqrt(tree_mse) \u003e\u003e\u003e tree_rmse 0.0 偏差为0，但很显然有过拟合的嫌疑。As we saw earlier, you don’t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training, and part for model validation. Better Evaluation Using Cross-Validation use Scikit-Learn’s K-fold cross-validation feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores: from sklearn.model_selection import cross_val_score scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10) tree_rmse_scores = np.sqrt(-scores) Warning Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value), which is why the preceding code computes -scores before calculating the square root. \u003e\u003e\u003e def display_scores(scores): ... print(\"Scores:\", scores) ... print(\"Mean:\", scores.mean()) ... print(\"Standard deviation:\", scores.std()) ... \u003e\u003e\u003e display_scores(tree_rmse_scores) Scores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782 71115.88230639 75585.14172901 70262.86139133 70273.6325285 75366.87952553 71231.65726027] Mean: 71407.68766037929 Standard deviation: 2439.4345041191004 Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a score of approximately 71,407, generally ±2,439. You would not have this information if you just used one validation set. But cross-validation comes at the cost of training the model several times, so it is not always possible. 最后一个测试模型：RandomForestRegressor Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algo‐ rithms even further. Tip You should save every model you experiment with, so you can come back easily to any model you want. Make sure you save both the hyperparameters and the trained parameters","date":"2022-07-17","objectID":"/posts/ml_note3/:0:1","tags":["Machine Learning"],"title":"ML Learning 4","uri":"/posts/ml_note3/"},{"categories":["自学笔记"],"content":"Fine-Tune Your Model Let’s assume that you now have a shortlist of promising models. You now need to fine-tune them. Let’s look at a few ways you can do that. Grid Search One way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. 但这样很耗时间，而且枯燥。 Instead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation. from sklearn.model_selection import GridSearchCV param_grid = [ {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, ] forest_reg = RandomForestRegressor() grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(housing_prepared, housing_labels) Tip When you have no idea what value a hyperparameter should have, a simple approach is to try out consecutive powers of 10 (10的连续幂) or a smaller number if you want a more fine-grained search, as shown in this example with the n_estimators hyperparameter. For example: 3, 10, 30, 100… Then, you could get the best combination of parameters like this: \u003e\u003e\u003e grid_search.best_params_ {'max_features': 8, 'n_estimators': 30} You can also get the best estimator directly: \u003e\u003e\u003e grid_search.best_estimator_ RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) Notice If GridSearchCV is initialized with refit=True (which is the default), then once it finds the best estimator using cross- validation, it retrains it on the whole training set. This is usually a good idea since feeding it more data will likely improve its perfor‐ mance. To access evaluation scores: \u003e\u003e\u003e cvres = grid_search.cv_results_ \u003e\u003e\u003e for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]): ... print(np.sqrt(-mean_score), params) ... 63669.05791727153 {'max_features': 2, 'n_estimators': 3} 55627.16171305252 {'max_features': 2, 'n_estimators': 10} 53384.57867637289 {'max_features': 2, 'n_estimators': 30} 60965.99185930139 {'max_features': 4, 'n_estimators': 3} 52740.98248528835 {'max_features': 4, 'n_estimators': 10} 50377.344409590376 {'max_features': 4, 'n_estimators': 30} 58663.84733372485 {'max_features': 6, 'n_estimators': 3} 52006.15355973719 {'max_features': 6, 'n_estimators': 10} 50146.465964159885 {'max_features': 6, 'n_estimators': 30} 57869.25504027614 {'max_features': 8, 'n_estimators': 3} 51711.09443660957 {'max_features': 8, 'n_estimators': 10} 49682.25345942335 {'max_features': 8, 'n_estimators': 30} 62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3} 54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10} 59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3} 52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10} 57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3} 51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10} Randomized Search The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits: 如果跑1000次，就能对每一个h","date":"2022-07-17","objectID":"/posts/ml_note3/:0:2","tags":["Machine Learning"],"title":"ML Learning 4","uri":"/posts/ml_note3/"},{"categories":["自学笔记"],"content":"模型的执行、监督和维护 ","date":"2022-07-17","objectID":"/posts/ml_note4/:0:0","tags":["Machine Learning"],"title":"ML Learning 5","uri":"/posts/ml_note4/"},{"categories":["自学笔记"],"content":"Launch, Monitor, and Maintain Your System get your solution ready for production, in particular by plugging the production input data sources into your system and writing tests. write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops. plug the human evalua‐ tion pipeline into your system. make sure you evaluate the system’s input data quality, or the performance will degrade slightly and gradually. train your models on a regular basis using fresh data. You should automate this process as much as possible. ","date":"2022-07-17","objectID":"/posts/ml_note4/:0:1","tags":["Machine Learning"],"title":"ML Learning 5","uri":"/posts/ml_note4/"},{"categories":["自学笔记"],"content":"处理 MINIST（手写数字图像） 数据集 ","date":"2022-07-17","objectID":"/posts/ml_note5/:0:0","tags":["Machine Learning"],"title":"ML Learning 6","uri":"/posts/ml_note5/"},{"categories":["自学笔记"],"content":"Training a Binary Classifier A good place to start is with a Stochastic Gradient Descent (SGD, 随机梯度下降) classifier, using Scikit-Learn’s SGDClassifier class. This classifier has the advantage of being capable of handling very large datasets efficiently. This is in part because SGD deals with training instances independently, one at a time (which also makes SGD well suited for online learning), as we will see later. Let’s create an SGDClassifier and train it on the whole training set: from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) # 如果想要重复利用结果，随机状态不要忘了设置！ sgd_clf.fit(X_train, y_train_5) ","date":"2022-07-17","objectID":"/posts/ml_note5/:0:1","tags":["Machine Learning"],"title":"ML Learning 6","uri":"/posts/ml_note5/"},{"categories":["自学笔记"],"content":"Performance Measure 对分类器的结果分析，如果单单用准确度(accuracy)有失偏颇。比如一个训练集里面90%都是A类的对象，这时候假若分类标准就是： A类 不是A类 那么我们如果设置分类器将输入的对象全部判断为A类，在同样类别比例的同源测试集里会有90%的准确率！这显然是荒谬的。 Confusion Matrix Its general idea is to count the number of times instances of class A are classified as class B. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. from sklearn.model_selection import cross_val_predict y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3) Just like the cross_val_score() function, cross_val_predict() performs K-fold cross-validation, but instead of returning the evaluation scores, it returns the predictions made on each test fold. This means that you get a clean prediction for each instance in the training set (“clean” meaning that the prediction is made by a model that never saw the data during training). Then use confusion_matrix() function. Just pass it the target classes (y_train_5) and the predicted classes (y_train_pred): \u003e\u003e\u003e from sklearn.metrics import confusion_matrix \u003e\u003e\u003e confusion_matrix(y_train_5, y_train_pred) array([[53057, 1522], [ 1325, 4096]]) $$ \\begin{matrix} true-negative \u0026 false-positive \\\\ false-negative\u0026 true-positive \\\\ \\end{matrix} $$ true negative: 真阴 true positive: 真阳 Precision and Recall Precision of the classifier: $$ precision=\\frac{TP}{TP+FP} $$ recall(sensitivity, or true postitive rate TPR): $$ recall = \\frac{TP}{TP+FN} $$ \u003e\u003e\u003e from sklearn.metrics import precision_score, recall_score \u003e\u003e\u003e precision_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1522) 0.7290850836596654 \u003e\u003e\u003e recall_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1325) 0.7555801512636044 It is often convenient to combine precision and recall into a single metric called the F1 score, in particular if you need a simple way to compare two classifiers. F1 score is the harmonic mean of precision and recall: $$ F_1=\\frac{2}{\\frac{1}{precision}+\\frac{1}{recall}} $$ To compute the F1 score, simply call the f1_score() function: \u003e\u003e\u003e from sklearn.metrics import f1_score \u003e\u003e\u003e f1_score(y_train_5, y_train_pred) 0.7420962043663375 The F1 score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall. For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a classifier that has a much higher recall but lets a few really bad videos show up in your product (in such cases, you may even want to add a human pipeline to check the classifier’s video selection). On the other hand, suppose you train a classifier to detect shoplifters on surveillance images: it is probably fine if your classifier has only 30% precision as long as it has 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught). Unfortunately, you can’t have it both ways: increasing precision reduces recall, and vice versa. This is called the precision/recall tradeoff. Precision/Recall Tradeoff ","date":"2022-07-17","objectID":"/posts/ml_note5/:0:2","tags":["Machine Learning"],"title":"ML Learning 6","uri":"/posts/ml_note5/"},{"categories":["Class Note"],"content":" #include \u003ciostream\u003e #include \u003ccstring\u003e using namespace std; // 写出两个函数的声明 char *\u0026funA(char **\u0026p); char *funB(char b[], char **p); int main() { char a[100], b[100]; for(int i = 0; i \u003c 3; ++i) { char **p; cin \u003e\u003e a; // 1. funA(p)作为左值，需要加引用\u0026 // 2. a是数组名，指向首地址，是一个一级指针：char * // 3. funA(p)对应的返回值也需是一个一级指针：char * // 4. 传入的p是一个二级指针: char ** // 因此函数原型：char * \u0026funA(char **\u0026 ) // 常见错误：char *\u0026funA(char **p); /* P171： 在定义返回引用的函数时，不能返回该函数的局部变量，因为局部变量的生存期仅限于函数内部，当函数返回时，局部变量就消失了。此时引用该变量就会引起一个无效的引用，导致程序运行时出错。返回引用的函数中，返回的值也不能是一个表达式，因为表达式不是左值，而是一个临时值。*/ funA(p) = a; cout \u003c\u003c funB(b, p) \u003c\u003c endl; } return 0; } // 写出两个函数的定义 char* \u0026funA(char **\u0026p) { p = new char*; // 需要赋值二级指针，不然是非法访问内存！ return *p; // 返回一级指针 } char *funB(char b[], char **p) { int i = 0; for(i = 0 ; (*p)[i] != '\\0'; i++) { // *p[i]和(*p)[i]的区别 // *p[i] 是指针数组，p是一个包含指针类型的数组 // (*p)[i] p是一个指针，指向一个数组，取下标为i的元素 // *((*p)+i) if((*p)[i] \u003e= 'a' \u0026\u0026 (*p)[i] \u003c= 'z') b[i] = (*p)[i] - 32; else b[i] = (*p)[i]; } b[i] = '\\0'; delete p; return b; } new返回的是地址！ ","date":"2022-07-06","objectID":"/posts/cpp%E5%AD%97%E7%AC%A6%E6%8C%87%E9%92%88/:0:0","tags":["C++","Char","Pointer"],"title":"C++ 字符指针","uri":"/posts/cpp%E5%AD%97%E7%AC%A6%E6%8C%87%E9%92%88/"},{"categories":["Class Note"],"content":"c++字符/字符串输入函数： ​ 注：程序运行后键盘输入会首先存储到输入缓冲区中，遇到结束符时终止输入（一般为回车）。 cin \u003e\u003e s cin从输入缓冲区中读入非空白字符到字符串s中（若第一个字符为空白字符，直至遇到第一个非空白字符开始读取），读取时遇到空白字符（空格符 space、制表符 tab、换行符 return等）结束读取。 ch = getchar() 从缓冲区中读入任何一个字符（包括空格符 space、制表符 tab 与换行符 return）。 in.get(s, 10, ‘#’); 从缓冲区中接收字符到字符串s中，遇到结束符’#’（默认为换行符） 或9个字符后结束读取。 cin.getline(s, 10, '#') 从缓冲区中接收字符到字符串s中，遇到结束符'#'（默认为换行符） 符后结束读取。 注：cin、cin.get()在读取缓冲区时不会处理结束符，cin.getline()会从缓冲区中移除结束符， 可以利用cin.ignore(1024, ‘\\n’)函数来清除输入缓冲区中存留的换行符。 ","date":"2022-07-06","objectID":"/posts/cpp%E5%AD%97%E7%AC%A6%E4%B8%B2/:1:0","tags":["C++","Char"],"title":"C++ 字符研究","uri":"/posts/cpp%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"categories":["Class Note"],"content":"C++ cout字符指针会直接输出字符串(重载) 其他指针例如double*，float*，int*等，输出的是地址值. #include \u003ciostream\u003e #include \u003ccstring\u003e using namespace std; char *oddstr(char str[]); int main() { char str[200]; cin \u003e\u003e str; char *odd = oddstr(str); cout \u003c\u003c odd \u003c\u003c endl; delete []odd; return 0; } char *oddstr(char str[]) { char *p = new char[200]; int j = 0; for(int i = 0; str[i] != '\\0'; i++) // for(int i = 0; i \u003c strlen(str); i++) { if(i % 2) p[j++] = str[i]; } // char * 不会自动在末尾补0，需手动添加 p[j] = '\\0'; return p; } ","date":"2022-07-06","objectID":"/posts/cpp%E5%AD%97%E7%AC%A6%E4%B8%B2/:2:0","tags":["C++","Char"],"title":"C++ 字符研究","uri":"/posts/cpp%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"categories":[],"content":"git init: 在当前目录下生成一个 git 仓库 git init dir_name: 使用指定目录作为Git仓库 设置提交代码时的用户信息： $ git config --global user.name \"runoob\" $ git config --global user.email test@runoob.com 基本操作 git add 添加文件到暂存区 git status 查看仓库当前的状态，显示有变更的文件。 git diff 比较文件的不同，即暂存区和工作区的差异。 git commit 提交暂存区到本地仓库。 git reset 回退版本。 git rm 将文件从暂存区和工作区中删除。 git mv 移动或重命名工作区文件。 git pull 拉取 cat file: 查看这个文件的内容 vim file:对该文件进行修改 ; :wq 表示结束 新建分支：git branch + 分支名 切换分支：git checkout + 分支名 推送分支：git push origin + 分支名 把远程服务器上所有的更新都拉取下来：git fetch 查看远程分支：git branch -a 查看本地分支：git branch 拉取远端分支代码：git pull origin 分支名 添加目录等：git add . 提交代码：git commit -m “备注” 推送提交的代码：git push origin 分支名 权限问题 给文件最高权限：sudo chmod -R 777 htg 切换主干：git checkout master 合并分支：git marge –no–ff 分支名 （提交）推送 删除远程分支：git push origin –delete 分支名 删除本地分支：git branch -d 分支名 更新qa分支代码 sudo su qa 输入密码 cd cd wwwroot/education/ 进入项目文件夹 git status git branch 查看当前分支 git pull origin v2.0 –\u003egit checkout ***文件 –\u003egit pull origin v2.0 服务器命令： 项目文件路径：/home/hantianguang/wwwroot/education 日志文件路径：/data/logs/htg 给文件夹权限：sudo chmod -R 777 htg 添加软链接：ln -s [源文件或目录] [目标文件或目录] ln -s env.php.htg env.php —\u003e效果—\u003eenv.php -\u003e env.php.htg 定时执行脚本 1 0 * * * 文件 （每天零点零一分执行脚本） 利用pscp软件，从linux服务器下载文件或文件夹至win本地 pscp -r root@ip_url:/data/images/coach/avatar/2020-04-20 d:\\avatar 切换用户：sudo su root 给文件更改拥有者、组：chown www.www -R excel/ git branch 命令操作 1、查看本地分支 ： git branch 前面带有*号的是当前分支 2 、删除本地已合并的分支： git branch -d [branchname] 某些情况下可以用 git branch -D [branchName] (使用时应注意是否已合并) 提示删除了一个名为list的本地分支 3、删除远程分支: git push origin –delete [branchname] 提示删除了一个名为 201804019-test-files 的分支, 注意: 在删除远程分支时，同名的本地分支并不会被删除，所以还需要单独删除本地同名分支 如果发生以下错误: error: unable to delete ‘origin/xxxxxxxx-fixbug’: remote ref does not exist error: failed to push some refs to ‘git@github.com:xxxxxxxx/xxxxxxxxxx.git’ 解决办法： git checkout xxxxx-fixbug 切换到当前分支上， 然后再 进行 git push –delete origin origin/xxxxx-fixbug 此时将不会再发生错误 。 4、创建分支 ：git branch [branchname] 需要注意，创建分支时，不会有什么提示。 5、 查看全部分支(包含本地和远程) ：git branch -a 6、根据指定版本号创建分支: git checkout -b branchName commitId 7、清理本地无效分支(远程已删除本地没删除的分支): git fetch -p 8、如果分支太多，还可以用此命令进行分支模糊查找: git branch | grep ‘branchName’ ","date":"2022-07-06","objectID":"/posts/git/:0:0","tags":["git"],"title":"Git","uri":"/posts/git/"},{"categories":[],"content":"echo + text pwd: 当前路径 cd .:当前目录 cd ..:上级目录 cd /：根目录 ls -l:显示文件的详细信息 rm -rf:用强制权限删除，避免烦人的提醒信息 mv: mv命令将文件重命名 或将其移至一个新的目录中。 man: man + 命令 , 展示手册 cat \u003c hello.txt \u003e\u003e hello2.txt:把hello.txt输入到cat里，然后再追加到hello2.txt中 ls -l / | tail head ? root su: super user 变量的使用 cat=\"Tom\" //no blankspace echo $cat 输出: Tom echo \"Cat is $cat\" foo = $(pwd) echo foo 输出：当前路径 ?: 用问号替代一个字符 *: 用星号替代所有自负，通配符 mkdir touch {foo,bar}/{a..h} #!: Shebang，找到python的路径 find: ​ find . -mtime -1: 找到在当前路径中所有在前一天修改的文件 grep history: 打印出所有你打过的命令 ~\\ ","date":"2022-07-06","objectID":"/posts/shell/:0:0","tags":["Shell","MacOs"],"title":"Shell","uri":"/posts/shell/"},{"categories":[],"content":"“有用的快捷键” cmd + k + 0: 折叠所有函数 ","date":"2022-07-06","objectID":"/posts/vscode%E5%BF%AB%E6%8D%B7%E9%94%AE/:0:0","tags":["git"],"title":"Vscode快捷键","uri":"/posts/vscode%E5%BF%AB%E6%8D%B7%E9%94%AE/"},{"categories":["Class Note"],"content":"性能分析 ","date":"2022-07-06","objectID":"/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/:0:0","tags":["Tools"],"title":"性能分析","uri":"/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["Class Note"],"content":"Timing usr sys: 在cpu里跑的时间 wall-Clock：所有进程花费的时间， 包括用户输入 time 指令 + 文件地址 ","date":"2022-07-06","objectID":"/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/:1:0","tags":["Tools"],"title":"性能分析","uri":"/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["Class Note"],"content":"Memory htop Glances ","date":"2022-07-06","objectID":"/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/:2:0","tags":["Tools"],"title":"性能分析","uri":"/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["Class Note"],"content":"网络参数 shell 中输入 ifconfig ","date":"2022-07-06","objectID":"/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/:3:0","tags":["Tools"],"title":"性能分析","uri":"/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["Class Note"],"content":"比较命令行指令用时 hyperfine 例子：find比fd慢很多！ ","date":"2022-07-06","objectID":"/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/:4:0","tags":["Tools"],"title":"性能分析","uri":"/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":null,"content":"English","date":"2022-07-06","objectID":"/posts/english/","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"怎么翻译？ ","date":"2022-07-06","objectID":"/posts/english/:0:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"潜在的恶果 insidious 潜在的，潜伏的但暗中为害的 the insidious effects of polluted water supplies 供水系统污染的潜在恶果 ","date":"2022-07-06","objectID":"/posts/english/:1:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"沉思 mull over 反复思考 I need some time to mull it over before making a decision.在作出决定之前我需要一些时间来认真琢磨一下。 musing 沉思 We had to sit and listen to his musings on life.我们只好坐着听他谈论人生。 ","date":"2022-07-06","objectID":"/posts/english/:2:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"思绪起伏 turmoil 动乱；骚动；混乱；焦虑 Ashley gazed at him, her thoughts in turmoil.阿什利注视着他﹐思绪起伏。 ","date":"2022-07-06","objectID":"/posts/english/:3:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"熙熙攘攘 swarm 成群地来回移动 Tourists were swarming all over the island.岛上到处是旅游者熙来攘往。 ","date":"2022-07-06","objectID":"/posts/english/:4:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"阻止，使不能 preclude 使行不通；阻止；妨碍；排除 （preclude somebody from doing something） My lack of interest in the subject precluded me from gaining much enjoyment out of it.由于对这个科目缺乏兴趣，我没有从中获得多少乐趣。 stem 阻止；封堵；遏止 They discussed ways of stemming the flow of smuggled drugs.他们讨论了遏制走私毒品流通的办法。 ","date":"2022-07-06","objectID":"/posts/english/:5:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"让人呆滞 stultify 使思维迟钝;（因为很无聊）使厌烦;使呆滞 Cults stultify their members’ critical thinking abilities. 邪教让信徒丧失思辨能力。 ","date":"2022-07-06","objectID":"/posts/english/:6:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"胡说八道 drivel 蠢话；傻话；废话 Don’t talk such drivel! 别胡说八道 ","date":"2022-07-06","objectID":"/posts/english/:7:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"绥靖，平息 appease 安抚；抚慰 They attempted to appease international opposition by promising to hold talks.他们答应举行会谈﹐试图以此平息国际上的反对声音。 Chamberlain’s policy of appeasement towards Hitler in the 30s. 30年代张伯伦对希特勒的绥靖政策 ","date":"2022-07-06","objectID":"/posts/english/:8:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"拿人出气 take out on Jane’s always annoying her and she takes it out on me sometimes. [VERB noun PREP. PREP. noun] 简老是惹她生气，她有时就拿我出气。 ","date":"2022-07-06","objectID":"/posts/english/:9:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"胜过，超过，比..更好 surpass He had surpassed all our expectations. 他超越了我们所有的的预期。 The number of multiple births has surpassed 100,000 for the first time. 多胎产首次突破了10万例。 surpass yourself (=do something better than you have ever done before) 超越自己 ","date":"2022-07-06","objectID":"/posts/english/:10:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"连珠炮似的问题／批评／抱怨 barrage 弹幕 a barrage of questions/criticisms/complaints连珠炮似的问题／批评／抱怨 the media’s barrage of attacks on the President’s wife ","date":"2022-07-06","objectID":"/posts/english/:11:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"巅峰之作 acme 顶峰；顶点；典范 His work is considered the acme of cinematic art. [+ of] 他的作品被认为是电影艺术的巅峰之作。 ","date":"2022-07-06","objectID":"/posts/english/:12:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"显著的 pronounced 显著的；很明显的；表达明确的 “Mankind has not yet come out of the shadow of the pandemic, the world economic recovery is still fragile and weak, while geopolitical and security tensions are becoming more pronounced. “人类尚未走出疫情阴霾，世界经济复苏仍然脆弱乏力，地缘政治和安全矛盾愈发突出。 ","date":"2022-07-06","objectID":"/posts/english/:13:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"普惠的 inclusive inclusive elderly care 普惠性养老专项 ","date":"2022-07-06","objectID":"/posts/english/:14:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"隔离 quarantine 隔离，（为防传染的）检疫，隔离期 The close contacts of the cases in Shunyi are now under quarantine. 目前，顺义地区区病例的密切接触者正在接受隔离。 ","date":"2022-07-06","objectID":"/posts/english/:15:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"形容风雨肆虐 batter 连续猛击；殴打 China has launched a Level-IV emergency response for flood control as heavy rains are forecast to batter southern parts of the country. 据预报，中国南部地区将有强降雨，国家已启动四级防洪应急响应。 ","date":"2022-07-06","objectID":"/posts/english/:16:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"应该充分发挥… full play should be given to… China’s central bank says full play should be given to the guiding roles of structural monetary policy tools. 中国央行表示，应充分发挥结构性货币政策工具的牵引带动作用。 ","date":"2022-07-06","objectID":"/posts/english/:17:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"奋勇向前 forge ahead President Xi also encouraged young people to forge ahead on the new journey to realize the Chinese Dream of national rejuvenation. 习主席还鼓励年轻人在实现民族复兴中国梦的新征程上奋勇前进。 ","date":"2022-07-06","objectID":"/posts/english/:18:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"开端 onset 开端，发生，肇始（尤指不快的事件） Over 13 million children in the United States have tested positive for COVID-19 since the onset of the pandemic. 自疫情爆发以来，美国有 1300 多万儿童新冠检测呈阳性。 ","date":"2022-07-06","objectID":"/posts/english/:19:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"如释重负地舒了口气 heave a sigh, etc. to make a sound slowly and often with effort（常指吃力地）缓慢发出（声音） We all heaved a sigh of relief. 我们都如释重负地舒了一口气。 ","date":"2022-07-06","objectID":"/posts/english/:20:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"多样化的 diversified In the final analysis, there will remain diversified ways for physical exercise in the near years, and it offers us a promising vision of a fitness-for-all society. 总之，在近些年，体育锻炼的方式仍将呈现多样化的特点，我们有望看到一个全民健身的社会。 ","date":"2022-07-06","objectID":"/posts/english/:21:0","tags":["English"],"title":"English","uri":"/posts/english/"},{"categories":null,"content":"My first blog","date":"2022-07-06","objectID":"/posts/hello/","tags":null,"title":"Hello","uri":"/posts/hello/"},{"categories":null,"content":"“My First Blog” 我在配置hugo网站时遇到的种种问题 去露营啦! ⛺ 很快就回来. 真开心! 😂 ","date":"2022-07-06","objectID":"/posts/hello/:0:0","tags":null,"title":"Hello","uri":"/posts/hello/"},{"categories":null,"content":"github提交问题 ","date":"2022-07-06","objectID":"/posts/hello/:1:0","tags":null,"title":"Hello","uri":"/posts/hello/"},{"categories":null,"content":"1. 更新被拒绝 具体信息：更新被拒绝，因为远程仓库包含您本地尚不存在的提交。这通常是因为另外，一个仓库已向该引用进行了推送。再次推送前，您可能需要先整合远程变更，如 ‘git pull … 解决办法： 强行提交：git push -u origin +master，但是容易导致文件细节出现问题！ git pull 之后，git add . -\u003e git commit -m “提交信息” -\u003e git push origin ","date":"2022-07-06","objectID":"/posts/hello/:1:1","tags":null,"title":"Hello","uri":"/posts/hello/"},{"categories":null,"content":"2. 更新位置没有相应的分支： 原因：git在本地进行git add .后，生成的分支默认名是“master”；而github现在生成的分支名是“main” 解决方法： 重新命名分支：git branch -m main master, 把main改为了master(但是这是github上的吗？) 直接在github上重新命名分支main为master ","date":"2022-07-06","objectID":"/posts/hello/:1:2","tags":null,"title":"Hello","uri":"/posts/hello/"},{"categories":null,"content":"github push操作卡顿问题 挂了vpn无法解决，得辅以修改端口号的手段： 在git bash中输入:git config --global http.proxy http://localhost:7890,这是能连上全球网的端口号 ","date":"2022-07-06","objectID":"/posts/hello/:2:0","tags":null,"title":"Hello","uri":"/posts/hello/"},{"categories":null,"content":"macos终端操作 删除文件夹：rm -rf 文件夹 ","date":"2022-07-06","objectID":"/posts/hello/:3:0","tags":null,"title":"Hello","uri":"/posts/hello/"},{"categories":null,"content":"macos查看隐藏文件夹 打开Finder，然后点击主文件夹。 您可以访问左栏中“设备”下的主文件夹。 按键盘键Cmd + Shift +。 （点）。 只要按下此组合键，就会看到所有隐藏的文件夹和文件。 如果您需要再次隐藏这些文件，您只需再次按下该组合即可。 参考的网站： https://zhuanlan.zhihu.com/p/57361697 hugo安装中文教程： https://www.gohugo.org/ 巨细的教程：https://jellyzhang.github.io/%E4%BD%BF%E7%94%A8hugo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/ ","date":"2022-07-06","objectID":"/posts/hello/:4:0","tags":null,"title":"Hello","uri":"/posts/hello/"},{"categories":["自学笔记"],"content":"Types of Machine Learning Systems Whether or not they are trained with human supervision supervised unsupervised semisupervised （半监督） Reinforcement Learning Whether or not they can learn incrementally on the fly online learning batch learning Whether they work by simply comparing new data points to known data points, or instead detect patterns in the training data and build a predictive model, much like scientists do instance-based model-based learning ","date":"2022-07-06","objectID":"/posts/ml_note/:1:0","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Supervised/Unsupervised Learning ","date":"2022-07-06","objectID":"/posts/ml_note/:2:0","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Supervised Learning ​ data: instance Desired solutions, called labels Algorithms k-Nearest Neighbors Linear Regression Logistic Regression Support Vector Machines (SVMs) Decision Trees and Random Forests Neural networks ","date":"2022-07-06","objectID":"/posts/ml_note/:2:1","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Unsupervised Learning Algorithms • Clustering — K-Means — DBSCAN — Hierarchical Cluster Analysis (HCA) • Anomaly detection and novelty detection — One-class SVM — Isolation Forest • Visualization and dimensionality reduction — Principal Component Analysis (PCA) — Kernel PCA — Locally-Linear Embedding (LLE) — t-distributed Stochastic Neighbor Embedding (t-SNE) • Association rule learning — Apriori — Eclat Related task: dimensionality reduction It is often a good idea to try to reduce the dimension of your training data using a dimensionality reduction algorithm before you feed it to another Machine Learning algorithm (such as a super‐ vised learning algorithm). It will run much faster, the data will take up less disk and memory space, and in some cases it may also per‐ form better. ","date":"2022-07-06","objectID":"/posts/ml_note/:2:2","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Semisupervised learning ​ Some algorithms can deal with partially labeled training data, usually a lot of unla‐ beled data and a little bit of labeled data. This is called semisupervised learning. ​ Some photo-hosting services, such as Google Photos, are good examples of this. Once you upload all your family photos to the service, it automatically recognizes that the same person A shows up in photos 1, 5, and 11, while another person B shows up in photos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all the system needs is for you to tell it who these people are. Just one label per person,4 and it is able to name everyone in every photo, which is useful for searching photos. ​ 自动找出不同图片中一样的人，但需要人为输入人的名字。 ","date":"2022-07-06","objectID":"/posts/ml_note/:2:3","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Reinforcement Learning ​ Reinforcement Learning is a very different beast. ​ The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return or penalties in the form of negative rewards. It must then learn by itself what is the best strategy, called a policy, to get the most reward over time. A policy defines what action the agent should choose when it is in a given situation. ","date":"2022-07-06","objectID":"/posts/ml_note/:2:4","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Batch and Online Learning ","date":"2022-07-06","objectID":"/posts/ml_note/:3:0","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Batch learning ​ In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing resources, so it is typically done offline. ​ First the system is trained, and then it is launched into production and runs without learning anymore; it just applies what it has learned. This is called offline learning. ​ If you want a batch learning system to know about new data (such as a new type of spam), you need to train a new version of the system from scratch on the full dataset (not just the new data, but also the old data), then stop the old system and replace it with the new one. Deficiency: More time More compute resources ","date":"2022-07-06","objectID":"/posts/ml_note/:3:1","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Online learning ​ In online learning, you train the system incrementally by feeding it data instances sequentially, either individually or by small groups called mini-batches. Advantages: Huge data set Space saving Challenge: bad data will lead to bad performance, so monitor the performance is necessary. ​ Online learning algorithms can also be used to train systems on huge datasets that cannot fit in one machine’s main memory (this is called out-of-core learning). The algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14). ​ Out-of-core learning is usually done offline (i.e., not on the live system), so online learning can be a confusing name. Think of it as incremental learning. ​ Learning rate: High rate: easy to forget old data, rapidly adapt to new data Low rate : learn slowly, less sensitive to noise in the new data o r to sequences of non representative data points(outliers) ","date":"2022-07-06","objectID":"/posts/ml_note/:3:2","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Instance-Based Versus Model-Based Learning ","date":"2022-07-06","objectID":"/posts/ml_note/:4:0","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Instance-Based learning ​ Instead of just flagging emails that are identical to known spam emails, your spam filter could be programmed to also flag emails that are very similar to known spam emails. This requires a measure of similarity between two emails. A (very basic) similarity measure between two emails could be to count the number of words they have in common. The system would flag an email as spam if it has many words in com‐ mon with a known spam email. ​ This is called instance-based learning: the system learns the examples by heart, then generalizes to new cases by comparing them to the learned examples (or a subset of them), using a similarity measure. ","date":"2022-07-06","objectID":"/posts/ml_note/:4:1","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Model-based learning ​ Another way to generalize from a set of examples is to build a model of these exam‐ ples, then use that model to make predictions. This is called model-based learning. A simple linear model life_satisfaction = $\\theta_{0} + \\theta_{1}\\times GDP _ per_capita$ Two ways to measure performances: Utility function: how good Cost function: how bad ","date":"2022-07-06","objectID":"/posts/ml_note/:4:2","tags":["Machine Learning"],"title":"ML Learning 1","uri":"/posts/ml_note/"},{"categories":["自学笔记"],"content":"Main Challenges ","date":"2022-07-06","objectID":"/posts/ml_note1/:1:0","tags":["Machine Learning"],"title":"ML Learning 2","uri":"/posts/ml_note1/"},{"categories":["自学笔记"],"content":"Insufficient Quantity of Training Data ​ The idea that data matters more than algorithms for complex problems was further popularized by Peter Norvig et al. in a paper titled “The Unreasonable Effectiveness of Data” published in 2009.10 It should be noted, however, that small- and medium- sized datasets are still very common, and it is not always easy or cheap to get extra training data, so don’t abandon algorithms just yet. ","date":"2022-07-06","objectID":"/posts/ml_note1/:1:1","tags":["Machine Learning"],"title":"ML Learning 2","uri":"/posts/ml_note1/"},{"categories":["自学笔记"],"content":"Nonrepresentative Training Data ​ In order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to. This is true whether you use instance-based learning or model-based learning. ","date":"2022-07-06","objectID":"/posts/ml_note1/:1:2","tags":["Machine Learning"],"title":"ML Learning 2","uri":"/posts/ml_note1/"},{"categories":["自学笔记"],"content":"Poor-Quality Data ​ Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor- quality measurements), it will make it harder for the system to detect the underlying patterns, so your system is less likely to perform well. It is often well worth the effort to spend time cleaning up your training data. The truth is, most data scientists spend a significant part of their time doing just that. For example: • If some instances are clearly outliers, it may help to simply discard them or try to fix the errors manually. • If some instances are missing a few features (e.g., 5% of your customers did not specify their age), you must decide whether you want to ignore this attribute alto‐ gether, ignore these instances, fill in the missing values (e.g., with the median age), or train one model with the feature and one model without it, and so on. ","date":"2022-07-06","objectID":"/posts/ml_note1/:1:3","tags":["Machine Learning"],"title":"ML Learning 2","uri":"/posts/ml_note1/"},{"categories":["自学笔记"],"content":"Irrelevant Features ​ As the saying goes: garbage in, garbage out. Your system will only be capable of learn‐ ing if the training data contains enough relevant features and not too many irrelevant ones. A critical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called feature engineering, involves: • Feature selection: selecting the most useful features to train on among existing features. • Feature extraction: combining existing features to produce a more useful one (as we saw earlier, dimensionality reduction algorithms can help). • Creating new features by gathering new data. ","date":"2022-07-06","objectID":"/posts/ml_note1/:1:4","tags":["Machine Learning"],"title":"ML Learning 2","uri":"/posts/ml_note1/"},{"categories":["自学笔记"],"content":"Overfitting the Training Data ​ Say you are visiting a foreign country and the taxi driver rips you off. You might be tempted to say that all taxi drivers in that country are thieves. Overgeneralizing is something that we humans do all too often, and unfortunately machines can fall into the same trap if we are not careful. In Machine Learning this is called overfitting: it means that the model performs well on the training data, but it does not generalize well. 过拟合 ​ Constraining a model to make it simpler and reduce the risk of overfitting is called regularization. （减少模型的参数变量，减少过拟合的风险。 ​ The amount of regularization to apply during learning can be controlled by a hyperparameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. ​ ","date":"2022-07-06","objectID":"/posts/ml_note1/:1:5","tags":["Machine Learning"],"title":"ML Learning 2","uri":"/posts/ml_note1/"},{"categories":["自学笔记"],"content":"Underfitting the Training Data The main options to fix this problem are: • Selecting a more powerful model, with more parameters • Feeding better features to the learning algorithm (feature engineering) • Reducing the constraints on the model (e.g., reducing the regularization hyper‐ parameter) ","date":"2022-07-06","objectID":"/posts/ml_note1/:1:6","tags":["Machine Learning"],"title":"ML Learning 2","uri":"/posts/ml_note1/"},{"categories":["自学笔记"],"content":"Testing and Validating ​ Split your data into two sets: the training set and the test set. ​ As these names imply, you train your model using the training set, and you test it using the test set. The error rate on new cases is called the generalization error (or out-of- sample error), and by evaluating your model on the test set, you get an estimate of this error. This value tells you how well your model will perform on instances it has never seen before. ​ If the training error is low (i.e., your model makes few mistakes on the training set) but the generalization error is high, it means that your model is overfitting the training data. This is a tip ​ It is common to use 80% of the data for training and hold out 20% for testing. However, this depends on the size of the dataset: if it contains 10 million instances, then holding out 1% means your test set will contain 100,000 instances: that’s probably more than enough to get a good estimate of the generalization error. ","date":"2022-07-06","objectID":"/posts/ml_note1/:2:0","tags":["Machine Learning"],"title":"ML Learning 2","uri":"/posts/ml_note1/"},{"categories":["自学笔记"],"content":"Hyperparameter Tuning and Model Selection 怎么选取 Hyperparameter regularization的值？ ​ A common solution to this problem is called holdout validation: you simply hold out part of the training set to evaluate several candidate models and select the best one. The new heldout set is called the validation set (or sometimes the development set, or dev set). More specifically, you train multiple models with various hyperparameters on the reduced training set (i.e., the full training set minus the validation set), and you select the model that performs best on the validation set. After this holdout vali‐ dation process, you train the best model on the full training set (including the valida‐ tion set), and this gives you the final model. Lastly, you evaluate this final model on the test set to get an estimate of the generalization error. ","date":"2022-07-06","objectID":"/posts/ml_note1/:2:1","tags":["Machine Learning"],"title":"ML Learning 2","uri":"/posts/ml_note1/"},{"categories":["自学笔记"],"content":"Data Dismatch ​ In some cases, it is easy to get a large amount of data for training, but it is not per‐ fectly representative of the data that will be used in production. ​ For example, suppose you want to create a mobile app to take pictures of flowers and automatically determine their species. You can easily download millions of pictures of flowers on the web, but they won’t be perfectly representative of the pictures that will actually be taken using the app on a mobile device. Perhaps you only have 10,000 representative pictures (i.e., actually taken with the app). ​ In this case, the most important rule to remember is that the validation set and the test must be as representative as possible of the data you expect to use in production, so they should be composed exclusively of representative pictures: you can shuffle them and put half in the validation set, and half in the test set (making sure that no duplicates or near-duplicates end up in both sets). ​ After training your model on the web pictures, if you observe that the performance of your model on the validation set is disappointing, you will not know whether this is because your model has overfit the training set, or whether this is just due to the mismatch between the web pictures and the mobile app pictures. ​ One solution is to hold out part of the training pictures (from the web) in yet another set that Andrew Ng calls the train-dev set. After the model is trained (on the training set, not on the train-dev set), you can evaluate it on the train-dev set: if it performs well, then the model is not overfitting the training set, so if performs poorly on the validation set, the problem must come from the data mismatch. You can try to tackle this problem by preprocessing the web images to make them look more like the pictures that will be taken by the mobile app, and then retraining the model. ​ Conversely, if the model performs poorly on the train-dev set, then the model must have overfit the training set, so you should try to simplify or regularize the model, get more training data and clean up the training data, as discussed earlier. No Free Lunch Theorem ​ A model is a simplified version of the observations. The simplifications are meant to discard the superfluous details that are unlikely to generalize to new instances. However, to decide what data to discard and what data to keep, you must make assumptions. For example, a linear model makes the assumption that the data is fundamentally linear and that the distance between the instances and the straight line is just noise, which can safely be ignored. ​ In a famous 1996 paper,11 David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. For some datasets the best model is a linear model, while for other datasets it is a neural network. There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and you evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks. ","date":"2022-07-06","objectID":"/posts/ml_note1/:2:2","tags":["Machine Learning"],"title":"ML Learning 2","uri":"/posts/ml_note1/"},{"categories":null,"content":"汉字文化","date":"2022-07-06","objectID":"/posts/%E9%80%A0%E5%AD%97/","tags":null,"title":"造字","uri":"/posts/%E9%80%A0%E5%AD%97/"},{"categories":null,"content":"“汉字文化” 武则天 武曌[zhào]，即武则天 。唐朝 至武周 时期政治家，武周开国君主（690年－705年在位），也是中国历史 上唯一的正统女皇帝 、即位年龄最大（67岁）及寿命最长的皇帝之一（82岁）。 十四岁时进入后宫 ，为唐太宗 才人，获赐号“武媚” 。唐高宗 时封昭仪 ，永徽六年（655年）在“废王立武”事件后成为皇后 。 [4] 上元元年（674年）加号“天后 ”，与高宗并称“二圣 ”，参预朝政。高宗驾崩后，作为唐中宗 、唐睿宗 的皇太后临朝称制 。 [5] 天授 元年（690年），武则天称帝，改国号为周，定都洛阳，称“神都 ”，建立武周 。 在位前后，“明察善断”，多权略，知人善任，重视人材的选拔，开创殿试 、武举 及试官制度 。又奖励农桑，改革吏治。同时大肆杀害唐朝宗室，兴起“酷吏 政治”。军事上收复并稳定安西四镇 ，一度使后突厥 归降。晚年逐渐豪奢专断，渐生弊政。 神龙 元年（705年），武则天病笃，宰相张柬之 等发动“神龙革命 ”，拥立唐中宗 复辟，迫使其退位。中宗恢复唐朝后，为其上尊号“则天大圣皇帝 ”。同年十一月，武则天于上阳宫 崩逝，年八十二。中宗遵其遗命，改称“则天大圣皇后”，以皇后身份入葬乾陵 。其后累谥为“则天顺圣皇后”。 武则天智略过人，兼涉文史，颇有诗才。有《垂拱集 》及《金轮集》，今已佚。《全唐诗 》存其诗。 文化： 武则天曾召集文学侍臣周茂思、范履冰 等人编纂《要览》《字海》及《乐书要录》等书。 [100] [101] 其中，《字海》收录有所有武则天首创的则天文字 ，全书已散佚 [102] 。《乐书要录》为唐代一部具有很高的史料价值和学术价值的乐律文献，除少量内容因袭就说外，多富实践意义，在中国古代音乐史上占有重要地位。全书共十卷，今仅存第五、六、七卷。 [103] 。武则天还改制音乐机构，将内教坊 改为云韶府 ，习雅乐，促进宫廷音乐的发展。武则天执政期间是唐代音乐过渡阶段，上承唐初的雅乐，下为唐中期燕乐 、俗乐 奠定基础。 沈既济 ：太后颇涉文史，好雕虫之艺……太后君临天下二十余年，当时公卿百辟，无不以文章达，因循日久，浸已成风。 [132] （《通典 》引） 武则天除精通史籍诗文之外，还精于书法。尤其精于飞白书 和行、草书 。所谓“飞白”就是在笔画中具有丝丝露白特点的书法，难度极大，但看上去却极为高雅。武则天当年曾以飞白书把大臣姓名写出来赐给他们，有大臣就上表说：“蒙恩作飞白书，题臣等名字垂赐，跪呈宝贶，仰戴琼文，如批七曜之图，似发五神之检。冠六文而首出，掩八体而孤骞……钟繇竭力而难比，伯英绝筋而不逮。则知乃神乃圣，包众智而同归；多才多艺，总群芳而兼善。”武则天的书法造诣于此可见一斑。 武周圣历二年（699年）二月初四，武则天由洛阳赴嵩山 封禅，返回时留宿于缑山升仙太子 庙，一时触景生情而撰写碑文，并亲为书丹。碑文表面记述周灵王 时太子晋 升仙故事，实则歌颂武周盛世。笔法婉约流畅，意态纵横。碑额“升仙太子之碑”六字，以“飞白体 ”书就，笔划中丝丝露白。碑文33行，每行66字，行书和草书相间，接近章草书体。碑文上下款和碑阴的《游仙篇》杂言诗、题名等，分别出自唐代书法名家薛稷 、钟绍京 之手。历代书法爱好者都视《升仙太子碑 》为书法艺术珍品。此碑书体极草，多用侧锋，尚存一定古雅之气。北宋《宣和书谱 》称：“武后本喜作字，初得晋王导 十世孙王方庆 者家藏祖父二十八人书迹，摹拓把玩，自此笔力益进，其行骎骎稍能，有丈夫胜气。”传世书迹有《荐福寺题额》《崇福寺题额》及《周升仙太子碑》等。《宣和书谱 》又称宋时御府藏有武则天的《夜宴诗》。但这些墨迹至今大都散失。 [86] [148] 武则天倾力打造了一支宫廷乐队，名为“十万宫廷乐”，后世传为“武皇十万宫廷乐舞 ”，其规模之雄伟是历史上罕见的。唐玄宗 钦点的三百梨园 子弟备受赞誉，与之相比，长寿二年（693年），武则天自制的《神宫大乐 》，舞用九百人，演出于万象神宫 之庭， [54] 规模宏大，堪称帝王建制宫廷乐队之最。 爱美： 据记载，晚年的武则天“善自粉饰，虽子孙在侧，不觉其衰老”。等到退位迁入上阳宫 后，却不再梳妆打扮，面容憔悴。一次，李显入见武则天时，为此而大惊。武则天哭泣道：“我从房陵 把你接回神都，固然是要把天下托付于你，而五贼（参与神龙政变的五王）却贪求事功，把我惊动到这里。”李显听后，悲泣不已，跪地“拜谢死罪”。有观点认为，正是此事，使得武三思等武氏族人仍能够参与朝政（由是三思等得入其谋）。 [5] 狠戾： 据《新唐书》和《资治通鉴 》记载，在永徽五年（654年）武则天产下长女安定思公主 。在公主出生后一月之际，王皇后来看望，怜爱并逗弄公主。离开后，武则天趁着没人，竟将公主掐死，又盖上被子掩饰。正好李治来到，武则天假装欢笑，打开被子一同看孩子，发现女儿已死，啼哭不已，并且惊问侍从，侍从都说：“皇后刚来过。”李治勃然大怒，说道：“皇后杀了我的女儿！”武则天于是哭泣着数落王皇后的罪过，王皇后无法解释清楚。李治从此有了”废王立武“的打算（但此事有争议，成书于五代的《旧唐书 》和《唐会要 》只记载了公主的暴卒，并未言明其死因）。时间一久，李治便想把武则天晋封为一品宸妃 [19] ，由于受到宰相韩瑗 和来济 的反对，最后不能成事。 武则天的统治稳定之后，开始启用酷吏 。为巩固统治，武则天使用严酷手段。为掌握国家统治大权，她毒死了已立为太子的亲生儿子。称帝第二年，武则天便用两大酷吏之一的来俊臣 杀了另一个酷吏周兴；至万岁通天二年（697年），杀死来俊臣，结束了酷吏政治。 [99] 崔瑞德 ：对于这位敢于推翻李唐皇室并像男人一样泼辣地实行统治的女人，尽管儒家历史学家都进行恶毒攻击和抱敌对态度，但是武曌显然具有特殊的才能，对政治具有天赋，并且非常善于操纵宫廷的权力结构。她之所以能非凡地攫取到权力，是由于她的杰出的才能、坚毅的决心和识别人的能力，再加上她的冷酷、肆无忌惮和政治上的机会主义。她对敌人和对手表现出的残忍和报复心，这在中国历史上很少有人能与之相比。 [145] （《剑桥中国隋唐史 》） 心机： 武则天最初能“屈身忍辱，奉顺上意”，故而李治力排众议，坚持立她为后。待到武则天得志后，“专作威福，上欲有所为，动为后所制”，李治不胜其忿。麟德 元年（664年），宰相上官仪 请求废后，李治亦以为然，即命上官仪起草废后诏书。左右侍从及时奔告于武则天，武则天立即到李治面前自诉，使其“羞缩不忍，复待之如初”。李治又担心武则天怨怒，因此哄骗她说：“我初无此心，皆上官仪教我。”从此每当李治理政，武则天便“垂帘于后，政无大小皆与闻之。天下大权，悉归中宫，黜陟、生杀，决于其口”。 [23-24] 上元二年（675年 ），李治所患的风眩 症愈加严重，欲禅位给皇后， [27] 便与大臣们商议，准备让武则天摄政，因宰相郝处俊 劝谏，这才暂时停议。武则天得知后，就召集了大批文人学士，大量修书，先后撰成《玄览》《古今内范》《青宫纪要》《少阳正范》《维城典训》《紫枢要录》《凤楼新诫》《孝子传》《列女传》《内范要略》《乐书要录》《百僚新诫》《兆人本业》《臣轨》等书；且密令这批学者参决朝廷奏议，以分割宰相的权力，被当时的人称作“北门学士 ”。 [28] 忍耐 赵翼 ：①古来无道之君好杀者，有石虎、符生、齐明帝、北齐文宣帝、金海陵炀王；其英主好杀者，有明太祖。然皆未有如唐武后之忍者也……真千古未有之忍人也哉！ [133] （《廿二史札记》）②人主富有四海，妃嫔动千百，后既为女王，而所宠幸不过数人，固亦未足深怪，故后初不以为讳，而且不必讳也……然则区区帷薄不修，固其末节，而知人善任，权不下移，不可谓非女中英主也。 [133] （《廿二史札记》） 贤能： 上元元年（674年）八月，李治称天皇 ，武则天称天后 。十二月，武则天上建言十二事 ，被李治悉数采纳，下诏颁布施行。武则天重视农业生产，规定各州县境内，“田畴垦辟，家有余粮”者予以升奖；“为政苛滥，户口流移”者必加惩罚。她所编农书 《兆人本业》，颁行天下，影响很大。而武则天执政期间，其宗教政策以尊崇佛教 为主。 垂拱 元年（685年）五月，武则天下诏允许内外九品以上官员和百姓向朝廷自荐，以求被任用。 [34] 三月，武则天下令制造铜匦 （铜制的匣子，类似于检举箱、报冤盒），置于洛阳宫城前，分为延恩 （献赋颂 、谋求仕途者投之）、招谏（言朝政得失者投之）、伸冤（有冤抑者投之）、通玄（言天象灾变及军机秘计者投之）四匦，随时接纳天下表疏 [35] 。此举广开言路，对稳定当时的朝政起了重大作用。 长寿 元年（692年）九月，派大将王孝杰 与阿史那忠节 [183] 率军出征西北。十月，王孝杰大破吐蕃，收复龟兹 、疏勒 、于阗 、碎叶 等安西四镇 ， [52] 设安西都护府 于龟兹。在群臣一致反对的情况下，武则天毅然对安西四镇增兵三万。这一措施使安西四镇从此安定，直到唐玄宗 时再无反复。 [53] 神化： 武则天的侄子武承嗣 命人凿白石为文曰：“圣母临人，永昌帝业。”号称在洛水中发现，献给武则天，武则天大喜，为石取名为“宝图”。垂拱四年（688年）五月，武则天加尊号为“圣母神皇”。十二月，武则天亲自拜洛受图 ，史称此时“皇帝、皇太子皆从，文武百官、蛮夷酋长，各依方位而立。珍禽奇兽，并列于坛前。文物、仪仗”，被评价为“自有唐已来，未有如此之盛者也”。 [39-41] 载初元年（690年 ）七月，东魏国寺僧人法明 等撰《大云经》四卷，称武则天是弥勒佛 化身下凡，应作为天下主人，武则天下令颁行天下。命两京诸州各置大云寺一所，藏《大云经》，命僧人讲解，并将佛教的地位提高在道","date":"2022-07-06","objectID":"/posts/%E9%80%A0%E5%AD%97/:0:0","tags":null,"title":"造字","uri":"/posts/%E9%80%A0%E5%AD%97/"},{"categories":null,"content":"无字碑 关于无字碑 的由来，众说纷纭，主要有几种说法：其一，武则天用以夸耀自己，表示功高德大非文字所能表达；其二，武则天自知罪孽重大，感到还是不写碑文为好；其三，武则天立“无字碑”，功过是非任凭后人评说，是聪明之举。 [168] 此外，还有说法认为唐廷为武则天立碑时已经拟好了碑文，但因各种原因碑文没有铭刻到墓碑上，而有可能被埋藏在乾陵地宫里。 [169-170] 值得一提的是，武则天的无字碑虽无原始碑文，却刻有后人题词。据媒体2008年报道，无字碑题词共四十二段，阳面三十二段，始于北宋，终于明。明代以后无字碑碑身仆倒，直到1957年维修扶直，再未有人题词。 [170] 这些题词中，还包括了用契丹文写成的《大金皇帝都统经略郎君行记》（简称《郎君行记》），且在旁边配有汉字译文；保存下珍贵的文字史料，这也不失为武则天的无字碑的一大贡献。 [168] ","date":"2022-07-06","objectID":"/posts/%E9%80%A0%E5%AD%97/:0:1","tags":null,"title":"造字","uri":"/posts/%E9%80%A0%E5%AD%97/"},{"categories":null,"content":"指事 抽象符號標誌事物部位、特徵或性質，或完全以抽象符號組合 的構字方法。 与象形和会意的区分： 有無形象可象：指事之別於象形者，形謂一物，事晐眾物，專 博斯分。上下所晐之物多，日月只一物。（《說文》段註） 是否獨體合體：指事不可以會意，殽合兩文為會意，獨體為指事。（《說文》段註） ","date":"2022-07-06","objectID":"/posts/%E9%80%A0%E5%AD%97/:1:0","tags":null,"title":"造字","uri":"/posts/%E9%80%A0%E5%AD%97/"},{"categories":null,"content":"会意 用兩個或兩個以上的獨體字根據意義之間的關係合成一個字，綜合表示這些構字成分合成的意義，是為會意。 1.种类 （1）構字方式 1. 以形会意 1. 以义会意 （2）构件异同 2.特點 合體字各部分有無主次 表達聯合意義？ 還是某一部分的意義？ 能否表抽象概念 ","date":"2022-07-06","objectID":"/posts/%E9%80%A0%E5%AD%97/:2:0","tags":null,"title":"造字","uri":"/posts/%E9%80%A0%E5%AD%97/"},{"categories":null,"content":"假借 語言中的某些詞，原本並無專門所造之字對應，後來借用同 音(音近)字來記錄它。 ​ ","date":"2022-07-06","objectID":"/posts/%E9%80%A0%E5%AD%97/:3:0","tags":null,"title":"造字","uri":"/posts/%E9%80%A0%E5%AD%97/"},{"categories":null,"content":"CS Kr-Panghu \"浪漫艺术家\" Huskydoge \"Dr. Woof\" ","date":"0001-01-01","objectID":"/friends/:1:0","tags":null,"title":"友链墙","uri":"/friends/"},{"categories":null,"content":"人工智能 Nobody-Know \"AI Expert\" ","date":"0001-01-01","objectID":"/friends/:2:0","tags":null,"title":"友链墙","uri":"/friends/"},{"categories":null,"content":"数学 huangyh123892 \"Cute Pig\" ","date":"0001-01-01","objectID":"/friends/:2:1","tags":null,"title":"友链墙","uri":"/friends/"},{"categories":null,"content":"ACM 相关 链接 描述 各大 OJ 题目分类 各种算法分类，对应 OJ 题号超链接 ACM 题目分类 codeforces 俄国 CF 在线编程，一般比赛在晚上 9:30 和 11：00 牛客网 国内求职学习网，有很多程序设计比赛 vjudge 这个不用说了吧 hihocoder 打得少，感觉好多数据结构的题 百练 OpenJudge 的一个小组，很多题目来自 POJ；也是我 ACM 启蒙地 HDUOJ 杭州电子科技大学 OJ POJ 北京大学 OJ 洛谷 lintcode 领扣（国外） leedcode 力扣 codewar 国外 更多 OJ，在线刷题网站 … ","date":"0001-01-01","objectID":"/tools/:1:0","tags":null,"title":"网站工具墙","uri":"/tools/"},{"categories":null,"content":"编程，学习 链接 描述 visualgo 算法学习，数据结构和算法动态可视化 c,c++ 学习 很详细的知识讲解 Tomcat 下载 Tomcat 各版本下载，Tomcat 解压即安装；\\bin 下可以启动和关闭服务器；可手动在 \\webapps 里创建 web 应用，也可以配合 Eclisp IDE for Java EE Developers 等 Java IDE 工具创建 w3school 可在线测试，Web 技术教程，HTML, 浏览器脚本，服务器脚本，xml 教程等 w3cschool 和上面不同哦，是不是和上面的很像哈哈哈！我估计是上一个网站的新版，功能更强大，内容更丰富，还有微信小程序教程等 菜鸟教程 和上面两个差不多，还有一些数据库、安卓的东西，git 学习 python 学习 Python 库安装包下载，python,Django,HTML,ACM 学习 python123 Python123 是专注于为中国高等院校教学 Python 语言的而开发的一款学习工具网站 [python 学习](https://python123.io/index/notebooks/python_programming_basic_v2/ 源代码/2-1 基本的温度转换程序?from=Python语言程序设计基础(第2版)) pypi Find, install and publish Python packages with the Python Package Index 优矿 python 在线测试，笔记 阮一峰的网络日志 开发者手册，JavaScript 等等 廖雪峰 git,python,javascript 学习 新晴网 PS 学习，摄影教程等 实验楼 在线做实验，高效学编程 慕课网 在线学习，免费，付费视频 wxpy 一个 python 关于微信的库 So1n python 学习 ","date":"0001-01-01","objectID":"/tools/:2:0","tags":null,"title":"网站工具墙","uri":"/tools/"},{"categories":null,"content":"云服务及站长工具 链接 描述 腾讯云 dnspod 解析新版入口，腾讯云控制台 dnspod 解析 域名解析，其他：阿里云等 cloudflare 解析 域名解析，https 解析，[介绍](https://oliverqueen.cn/2018/01/25/ 可能是最全的使用 HEXO 搭建个人博客教程) cloud studio 这是腾讯云和 coding 合作后的一个东西，简单来说就是云端开发环境。试了一下完全可以把 hexo 博客源码挂上面编辑。这也就不用只局限于一台电脑发布博客了。 腾讯云大学开放实验室 可以在线进行一些云服务应用搭建等的实验。与之对应的还有阿里云开放实验室 宝塔面板 宝塔面板是一款使用方便、功能强大且终身免费的服务器管理软件，支持 Linux 与 Windows 系统。一键配置：LAMP/LNMP、网站、数据库、FTP、SSL，通过 Web 端轻松管理服务器。 leancloud 一站式后端云服务 daovoice 网站在线客服 百度网站收录 网站 seo Bing 站长 Bing 网站管理员工具 站长工具 网站信息查询，权重，seo，网站速度查询，ping 等 站长工具大全 各种 seo 优化，工具等 ","date":"0001-01-01","objectID":"/tools/:3:0","tags":null,"title":"网站工具墙","uri":"/tools/"},{"categories":null,"content":"前端工具 链接 描述 全栈开发者 顾名思义，有很多 web 开发的教程，也可以在线执行代码 JSFuck JSFuck 是一种基于 JavaScript 原子部分的深奥教育编程风格。它只使用六个不同的字符来编写和执行代码。（代码极丑化） BootCDN 稳定、快速、免费的前端开源项目 CDN 加速服务，共收录了 3441 个前端开源项目。比如 vue,fancybox 等 bootstrap 中文网 简洁、直观、强悍的前端开发框架，让 web 开发更迅速、简单。还有一些基于 bootstrap 开发或扩充的项目的 cdn，BootCDN 也是在该网站旗下 Font Awesome 图标库 hexo 指定图标库 iconfont 阿里巴巴矢量图标库 easyicon 图标库 图标下载，格式转换，可外链 semantic-ui.com 图标库 semantic-ui.com 还有很多前端样式对应代码 colorhunt 配色方案推荐，调试 grabient 渐变色 渐变色调色 gradient 渐变色调色 渐变色调色，获取代码 encycolorpedia encycolorpedia 取色器 优酷视频上传 获取视频外链，其他：腾讯视频 sm.ms 非常好用的图床 imgURL 一个开源的图床，很不错，有兴趣的可以自己尝试搭建，比如 img.lruihao.cn 现实君外链 支持多种文件外链，唯一不好的是 http PEXELS 美图 文章配图，无版权美图网，避免配图侵权 unsplash 美图 API 还可以获取 随机图 图标工厂 移动应用图标生成工具，一键生成所有尺寸的应用图标 emoji 在线复制 Simple emoji copy and paste HTML 字符实体 网页特殊符号大全 特效字转换工具集 各种特效字转换工具，彩虹字生成器，RGB 转 16 进制颜色等 压缩图 在线 ps，图片去底，证件照换底等操作很方便的 web 图片处理工具 改图宝 在线改图压缩，加水印，生成二维码，印章制作等 GIF 之家 一个压缩效果很好且免费的 gif 压缩工具 二维码解码器 草料二维码 https://cli.im 联图二维码 第九工厂 死磕艺术二维码 模板码 动态二维码等 ","date":"0001-01-01","objectID":"/tools/:4:0","tags":null,"title":"网站工具墙","uri":"/tools/"},{"categories":null,"content":"实用工具 链接 描述 Gearn Git Branching 通过游戏闯关的方式学习 git！ PicGo 一个很好的开源的图床管理桌面程序，图床神器支持微博图床、七牛图床、腾讯云 COS、又拍云、GitHub 图床、阿里云 OSS、Imgur 图床等等 Proxyee Down 百度云下载解决方案，满速下载，但不限于百度云的下载，感觉比迅雷还好用！ PanDownload 这是 pandownload 网页版，把需要下载的百度网盘链接 baidu 改成 baiduwp 即可加速，也有桌面版。 ubuntu pastebin 代码展示托管，生成分享网址，防止代码直接分享缩进消失 图片转字符工具 Img–\u003eString everyfont 中文字库在线压缩 字蛛 font-spider 中文字库压缩 mdtr2pdf markdown 转 pdf AD’s API 包括动态签名，网易音乐等 在线工具 w3cschool 的在线工具集合 在线工具 开源中国社区 菜鸟工具 菜鸟教程 在线工具，编译，加密，压缩代码等 msdn windows 系统上 office 等软件下载（备用） 老殁科技 Adobe 等各种绿色破解软件 百度接口 百度搜索关键词接口 在线短信 不想泄露个人的电话号码，注册一些一次性网站可以用到 配音阁 文字转换语音 / 语音合成广告_叫卖录音_在线配音网络软件_促销宣传片配音 - 配音阁 - 配音阁，国内专业的广告配音平台 hacknical 在线个性简历，个人 Github 总结分析 ","date":"0001-01-01","objectID":"/tools/:5:0","tags":null,"title":"网站工具墙","uri":"/tools/"},{"categories":null,"content":"其他 链接 描述 俄罗斯方块 这是一个开源的游戏，小白的我看来简直牛逼爆了，好逼真，刷新都不会打断游戏进度！！ 网址迷宫 nazo_game 一个程序员的网页游戏，我只玩到 12 关。 无损音乐下载 手写体制作 北京大学计算机科学技术研究所的一个项目 59.108.48.27 （我的字体至今还未完成） 大象代理 收费，口碑不错 蒲公英 应用内测发布平台 ","date":"0001-01-01","objectID":"/tools/:6:0","tags":null,"title":"网站工具墙","uri":"/tools/"},{"categories":null,"content":"博客文章 文章 概括 Web 笔记 记录一下自己 web 相关学习的笔记 Git 常用指令汇总 可能用到按需自查 ","date":"0001-01-01","objectID":"/tools/:7:0","tags":null,"title":"网站工具墙","uri":"/tools/"}]